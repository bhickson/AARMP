{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging, math\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import fiona\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import gdal, osr\n",
    "from pyproj import transform, Proj\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import Utilities as utils\n",
    "import Create_Classification_Points\n",
    "from RasterCalculations import *\n",
    "from StackGeneration import generateStack\n",
    "\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "import rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global utm12, utm11\n",
    "utm11 = Proj(init=\"epsg:26911\")\n",
    "utm12 = Proj(init=\"epsg:26912\")\n",
    "predicted_column = \"CLASS_PREDICT\"\n",
    "\n",
    "\n",
    "dataDir = os.path.abspath(\"../Data\")\n",
    "utils.initializeDirectoryStructure(dataDir)\n",
    "classifier_file = os.path.join(utils.inputs_dir, \"RandomForestClassifier.joblib.pkl\")\n",
    "rf_model = joblib.load(classifier_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_class_polygons = os.path.join(utils.inputs_dir, 'classificationTrainingPolygons.shp')\n",
    "loc_usgs_qquads = os.path.join(utils.inputs_dir, \"USGS_QQuads_AZ.shp\")\n",
    "\n",
    "\n",
    "training_points, rf_rasters = createTrainingData(loc_class_polygons, loc_usgs_qquads, dataDir)\n",
    "\n",
    "# INITIALIZE COLUMN FOR PREDICTED CLASSIFICATION VALUES\n",
    "\n",
    "\n",
    "#testing_data[predicted_column] = \"Null\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627285     8\n",
       "54997      9\n",
       "922365     8\n",
       "1894921    8\n",
       "1506899    4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data[\"Class\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-d2e856868c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Split the points data frame into train and test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Split the points data frame into train and test\n",
    "training_data, testing_data = train_test_split(training_points, training_points[\"Class\"], test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   22.1s finished\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy score: 0.9913892150950583\n",
      "Kappa Score: 0.9884792025306961\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Classes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46527</td>\n",
       "      <td>295</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>534</td>\n",
       "      <td>11470</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>381</td>\n",
       "      <td>28483</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>46497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>63</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>11748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>49703</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>298896</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21941</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>9158</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>13</td>\n",
       "      <td>15542</td>\n",
       "      <td>77</td>\n",
       "      <td>153</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "      <td>92</td>\n",
       "      <td>31427</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>157</td>\n",
       "      <td>17499</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>19667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Classes     1      2      3      4      5      6      7       8   \\\n",
       "Actual Classes                                                               \n",
       "1                  46527    295     41      0      1      2      0      10   \n",
       "2                    534  11470    468      0     83     14     27       3   \n",
       "3                     81    381  28483      0    237     11      0       0   \n",
       "4                     20      0      0  16156      0      0      0       0   \n",
       "5                      1     31    137      0  46497      1      0       0   \n",
       "6                     19     63     93      0     75  11748      0       0   \n",
       "7                      9     17      8      0      2      0  49703      57   \n",
       "8                     17      0      2      0      1      1      5  298896   \n",
       "9                      0      2      5      0     22      0      0       0   \n",
       "10                    12     42     30      0    117      5      0       5   \n",
       "11                    17      9     26      0     47      0     13       2   \n",
       "12                     0      8     45      0    161      4      0       0   \n",
       "13                    13     12     12      0      4      0      1       5   \n",
       "14                    44      1      0      0      0      0      7      11   \n",
       "\n",
       "Predicted Classes     9     10     11     12     13     14  \n",
       "Actual Classes                                              \n",
       "1                      3     0      0      3     30     22  \n",
       "2                      0    10      3     28     48      1  \n",
       "3                      0    14      6     48     11      0  \n",
       "4                      0     0      0      0      0      0  \n",
       "5                      9    25     15     89      0      0  \n",
       "6                      0    72      0     29     31      9  \n",
       "7                      0     3     60      8     39      2  \n",
       "8                      0     3     26      2     22     11  \n",
       "9                  21941     0     81     24      1      0  \n",
       "10                    30  9158     38     56     60      4  \n",
       "11                   132    13  15542     77    153      6  \n",
       "12                    75    29     92  31427    133      0  \n",
       "13                     0     7     21    157  17499     10  \n",
       "14                     0     1     13      3     99  19667  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from IPython.display import display\n",
    "\n",
    "test_pred = rf_model.predict(testing_data[rf_rasters])\n",
    "accu = accuracy_score(testing_data[\"Class\"], test_pred)\n",
    "kappa = cohen_kappa_score(testing_data[\"Class\"], test_pred)\n",
    "print(\"Mean accuracy score: {}\".format(accu))\n",
    "print(\"Kappa Score: {}\".format(kappa))\n",
    "pd.crosstab(testing_data[\"Class\"], test_pred, rownames=['Actual Classes'], colnames=['Predicted Classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only perform ops with scalar values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-965bffc3f8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"VegD\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mindex_arithmetic_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_numeric_binop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[1;31m# handle time-based others\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_validate_for_numeric_binop\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4704\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4705\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mis_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4706\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"can only perform ops with scalar values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   4708\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only perform ops with scalar values"
     ]
    }
   ],
   "source": [
    "confusion_matrix.columns = ([\"VegD\"] + confusion_matrix.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#confusion_matrix = pd.crosstab(testing_data[\"Class\"], test_pred, rownames=['Actual Classes'], colnames=['Predicted Classes'])\n",
    "#from IPython.display import display\n",
    "#display(confusion_matrix)\n",
    "\n",
    "def render_mpl_table(data, col_width=2.5, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, rowLabels=data[] **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in  six.iteritems(mpl_table._cells):\n",
    "        #print(k, cell)\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "a = render_mpl_table(confusion_matrix.reset_index())\n",
    "a.get_figure().savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (149, 5)\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: species, dtype: int64\n",
      "Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n",
      "The independent features set: \n",
      "[[4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]]\n",
      "The dependent variable: \n",
      "[0 0 0 0 0]\n",
      "Predicted Species  Iris-setosa  Iris-versicolor  Iris-virginica\n",
      "Actual Species                                                 \n",
      "Iris-setosa                 13                0               0\n",
      "Iris-versicolor              0               15               0\n",
      "Iris-virginica               0                2               8\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\BenJames\\Downloads\\iris_data.csv\")\n",
    "dataset.columns = ['sepal length in cm', 'sepal width in cm','petal length in cm','petal width in cm','species']\n",
    "print('Shape of the dataset: ' + str(dataset.shape))\n",
    "#print(dataset.head())\n",
    "#Creating the dependent variable class\n",
    "factor = pd.factorize(dataset['species'])\n",
    "dataset.species = factor[0]\n",
    "definitions = factor[1]\n",
    "print(dataset.species.head())\n",
    "print(definitions)\n",
    "#Splitting the data into independent and dependent variables\n",
    "X = dataset.iloc[:,0:4].values\n",
    "y = dataset.iloc[:,4].values\n",
    "print('The independent features set: ')\n",
    "print(X[:5,:])\n",
    "print('The dependent variable: ')\n",
    "print(y[:5])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 21)\n",
    "# Feature Scaling\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#Reverse factorize (converting y_pred from 0s,1s and 2s to Iris-setosa, Iris-versicolor and Iris-virginica\n",
    "reversefactor = dict(zip(range(3),definitions))\n",
    "y_test = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)\n",
    "# Making the Confusion Matrix\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['Actual Species'], colnames=['Predicted Species']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RED</th>\n",
       "      <th>GREEN</th>\n",
       "      <th>BLUE</th>\n",
       "      <th>NIR</th>\n",
       "      <th>area</th>\n",
       "      <th>perim</th>\n",
       "      <th>perc_area</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>OSAVI</th>\n",
       "      <th>...</th>\n",
       "      <th>L8_2</th>\n",
       "      <th>L8_3</th>\n",
       "      <th>L8_4</th>\n",
       "      <th>L8_5</th>\n",
       "      <th>L8_6</th>\n",
       "      <th>L8_7</th>\n",
       "      <th>L8_8</th>\n",
       "      <th>L8_NDSI</th>\n",
       "      <th>L8_NDWI</th>\n",
       "      <th>Slope_Degr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1367094</th>\n",
       "      <td>42.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2407.0</td>\n",
       "      <td>5749.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-377.0</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>2438.0</td>\n",
       "      <td>1691.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-273.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019394</th>\n",
       "      <td>165.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-191.0</td>\n",
       "      <td>-196.0</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>905.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-295.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374849</th>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23425.0</td>\n",
       "      <td>2293.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>-344.0</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-241.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481191</th>\n",
       "      <td>210.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>8220.0</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>3816.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>5098.0</td>\n",
       "      <td>4047.0</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>-147.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285169</th>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>2684.0</td>\n",
       "      <td>2665.0</td>\n",
       "      <td>2558.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-319.0</td>\n",
       "      <td>436.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RED  GREEN   BLUE    NIR     area   perim  perc_area   NDVI   SAVI  \\\n",
       "1367094   42.0   47.0   51.0   19.0   2407.0  5749.0       22.0 -377.0 -183.0   \n",
       "2019394  165.0  169.0  166.0  112.0      3.0     3.0        0.0 -191.0 -196.0   \n",
       "1374849   39.0   41.0   54.0   19.0  23425.0  2293.0       59.0 -344.0 -161.0   \n",
       "481191   210.0  211.0  209.0  202.0   8220.0  1533.0       76.0  -19.0  -22.0   \n",
       "1285169   76.0   76.0   72.0  119.0     64.0    47.0        0.0  220.0  200.0   \n",
       "\n",
       "         OSAVI     ...        L8_2    L8_3    L8_4    L8_5    L8_6    L8_7  \\\n",
       "1367094 -225.0     ...      1656.0  1707.0  2064.0  2986.0  2438.0  1691.0   \n",
       "2019394 -166.0     ...       905.0   602.0   373.0   240.0   131.0    84.0   \n",
       "1374849 -202.0     ...      1148.0   821.0   732.0   777.0   463.0   581.0   \n",
       "481191   -17.0     ...      3620.0  3816.0  4451.0  5098.0  4047.0  2302.0   \n",
       "1285169  182.0     ...      1421.0  1385.0  1575.0  2684.0  2665.0  2558.0   \n",
       "\n",
       "         L8_8  L8_NDSI  L8_NDWI  Slope_Degr  \n",
       "1367094  14.0   -106.0   -273.0         0.0  \n",
       "2019394  21.0   -295.0    429.0         0.0  \n",
       "1374849  13.0   -241.0     44.0         0.0  \n",
       "481191   12.0   -113.0   -147.0       164.0  \n",
       "1285169  14.0     -3.0   -319.0       436.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[rf_rasters].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64),\n",
       " Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#new_forest = RandomForestClassifier()\n",
    "#new_forest.fit(X_train, y_train)\n",
    "\n",
    "y_predict = rf_model.predict(X_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateGeom(row):\n",
    "    geom = row[\"geometry\"]\n",
    "    if row['PROJ'] == \"NAD83 / UTM zone 11N\":\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "\n",
    "        point = Point(transform(utm12, utm11, x, y))\n",
    "        if point.x <= 0 or point.y <= 0:\n",
    "            print(\"Bad\", point, \"Original: \", geom)\n",
    "        return point\n",
    "    else:\n",
    "        return geom\n",
    "\n",
    "\n",
    "def extractToPoints(training_points, out_file, data_dir, overwrite=False):\n",
    "    def get_values(geom):\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "\n",
    "        values = []\n",
    "        # print(\"Starting Raster Extract for %s at x:%s y:%s\" % (os.path.basename(raster), str(x), str(y)))\n",
    "\n",
    "        for val in ras_stack.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "\n",
    "        return pd.Series(values, index=band_order)\n",
    "\n",
    "    ext_start = datetime.now()\n",
    "\n",
    "    # IF VECTOR FILE OF POINTS WITH RASTER EXTRACTS DOESN'T EXIST, BUILD IT\n",
    "    if not os.path.exists(out_file) or overwrite:\n",
    "        # if \"class_points\" not in locals():\n",
    "        #    logging.debug(\"READING IN %s as training_points\" % training_points)\n",
    "        training_data_df = gpd.read_file(training_points, crs={'init': 'epsg:26912'})\n",
    "        # print(\"Columns: \", training_data_df.columns)\n",
    "\n",
    "        # DEFINE THE PROJECTION USED OVER ARIZONA. USED FOR TRANSLATING CORRECT POINT GEOMETRY\n",
    "        global utm12, utm11\n",
    "        utm11 = Proj(init=\"epsg:26911\")\n",
    "        utm12 = Proj(init=\"epsg:26912\")\n",
    "\n",
    "        if \"utm_geom\" not in training_data_df:\n",
    "            logging.debug(\"ADDING COLUMN 'utm_geom' WITH CORRECT UTM COORDINATES FOR EACH QUARTER QUAD\")\n",
    "            # CREATE TRUE RASTER GEOMETRY COLUMN (BASED ON UTM)\n",
    "            training_data_df[\"utm_geom\"] = training_data_df.apply(calculateGeom, axis=1)\n",
    "\n",
    "        # NDSI is only used because its the last raster column\n",
    "        if \"L8_NDWI\" not in training_data_df:\n",
    "            logging.debug(\"CREATING COLUMNS...\")\n",
    "            # CREATE EMPTY COLUMNS IN DATA FRAME FOR EACH RASTER VARIABLE\n",
    "\n",
    "            sample_naip = training_data_df.iloc[0]['NAIP_FILE']\n",
    "            raster_loc = generateStack(sample_naip)\n",
    "            band_columns = []\n",
    "            with rio.open(raster_loc) as ras_stack:\n",
    "                for i in range(1, ras_stack.count + 1):\n",
    "                    column = ras_stack.tags(i)['NAME']\n",
    "                    band_columns.append(column)\n",
    "                    training_data_df[column] = np.NaN\n",
    "\n",
    "        # print(\"COLUMNS\", training_data_df.columns)\n",
    "\n",
    "        complete = 0\n",
    "        all = len(training_data_df)\n",
    "        # ITERATE THROUGH DATAFRAME IN GROUPS BY NAIP_FILE. KEEPS US FROM OPENING/CLOSING RASTERS FOR EACH POINT - INSTEAD FOR EACH GROUP\n",
    "        for loc_NAIPFile, group in training_data_df.groupby(\"NAIP_FILE\"):\n",
    "            logging.debug(\"\\nStarting raster value extraction for points in qquad %s\" % loc_NAIPFile)\n",
    "            print(\"\\nStarting raster value extraction for points in qquad %s\" % loc_NAIPFile)\n",
    "            loc_NAIPFile.replace(\"\\\\\", \"/\")  # normalize for windows paths\n",
    "\n",
    "            # LOOK FOR RASTERS FROM WHICH VALUES WILL BE EXTRACTED\n",
    "            # file = os.path.basename(loc_NAIPFile)\n",
    "\n",
    "            if group[\"L8_NDWI\"].isnull().values.any():\n",
    "                training_raster = generateStack(loc_NAIPFile)\n",
    "\n",
    "                band_order = []\n",
    "                with rio.open(training_raster) as ras_stack:\n",
    "                    for i in range(1, ras_stack.count + 1):\n",
    "                        band_order.append(ras_stack.tags(i)['NAME'])\n",
    "\n",
    "                    training_data_df.loc[\n",
    "                        training_data_df.NAIP_FILE == loc_NAIPFile, band_columns] = \\\n",
    "                        training_data_df.loc[training_data_df.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_values)\n",
    "\n",
    "            complete += len(group)\n",
    "            percent_done = (complete/all) * 100\n",
    "            logging.debug(\"{}% Done - Finished with group %s at %s\" % (percent_done, loc_NAIPFile, str(datetime.now())))\n",
    "\n",
    "        logging.info(\"Finished raster value extraction of %s points in %s\" % (\n",
    "            str(len(training_data_df)), str(datetime.now() - ext_start)))\n",
    "\n",
    "        # GEOPANDAS WON\"T ALLOW MORE THAN ONE COLUMN WITH GEOMETRY TYPE. REMOVE THE utm_geom COLUMN CREATED PREVIOUSLY\n",
    "        del training_data_df['utm_geom']\n",
    "        # print(\"COLUMNS:\\n\\t{}\".format(training_data_df.columns))\n",
    "        print(\"WRITING DATAFRAME TO OUTPUT...\")\n",
    "        logging.debug(\"WRITING DATAFRAME TO OUTPUT...\")\n",
    "        training_data_df.to_file(out_file)\n",
    "\n",
    "    else:\n",
    "        print(\"Reading in point file %s\" % out_file)\n",
    "        logging.info(\"Reading in point file %s\" % out_file)\n",
    "\n",
    "        # BEN - UPDATE RASTERS NAMES READ\n",
    "        training_data_df = gpd.read_file(out_file)\n",
    "        # Had to delete utm_geom when writing file (can't have two geometry columns). Recreate...\n",
    "        print(\"COLUMNS:\\n\\t{}\".format(training_data_df.columns))\n",
    "        band_columns = training_data_df.columns.tolist()[4:-1]\n",
    "\n",
    "    return {\"training_points\": training_data_df, \"band_names\": band_columns}\n",
    "\n",
    "\n",
    "\n",
    "def createTrainingData(polygons, qquad_file, datadir):\n",
    "\n",
    "    # for each training polygon, identify the utm zone that it falls in\n",
    "    loc_out_join = Create_Classification_Points.joinProjectionAndNAIP(polygons, utils.naip_dir, qquad_file,\n",
    "                                                                      overwrite=False)\n",
    "    # create points inside polygons at pixel center locations\n",
    "    loc_points_in_poly = Create_Classification_Points.createShapefilePoints(loc_out_join, overwrite=False)\n",
    "\n",
    "    # LOCATION OF FILE CONTAINING CLASSIFICATION POINTS POST EXTRACT\n",
    "    loc_points_wRaster_extracts = loc_points_in_poly[:-4] + \"_extracts.shp\"\n",
    "\n",
    "    # EXTRACT RASTER VALUES TO POINTS\n",
    "    training_info = extractToPoints(loc_points_in_poly, loc_points_wRaster_extracts, datadir, overwrite=False)\n",
    "\n",
    "    class_points = training_info[\"training_points\"]\n",
    "    band_names = training_info[\"band_names\"]\n",
    "    logging.debug(\"Available raster variables: \\n\\t%s\" % band_names)\n",
    "\n",
    "    class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "\n",
    "    return class_points, band_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "riolog = rio.logging.getLogger()\n",
    "riolog.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getFullNAIPPath(naip_file, naipdir):\n",
    "    for root,dirs,files in os.walk(naipdir):\n",
    "        for file in files:\n",
    "            if naip_file in file:\n",
    "                return os.path.join(root,file)\n",
    "    \n",
    "    logging.error(\"Unable to find naip file %s in %s. Exiting\" % (naip_file, naipdir))\n",
    "    raise Exception\n",
    "\n",
    "\n",
    "def findSTDDevFile(dir, naip_file, band_num, windowsize):\n",
    "    #findFile(os.path.join(std3px_dir, bandnum), ffile)\n",
    "    \n",
    "    window_dir = os.path.join(dir, \"StdDev_\" + str(windowsize) + \"px\")\n",
    "    utilities.useDirectory(window_dir)\n",
    "    band_dir = os.path.join(window_dir, \"band\" + band_num)\n",
    "    utilities.useDirectory(band_dir)\n",
    "\n",
    "    fname = os.path.basename(naip_file)\n",
    "\n",
    "    for root, dirs, files in os.walk(band_dir):\n",
    "        for file in files:\n",
    "            if fname in file:\n",
    "                fpath = os.path.join(root, file)\n",
    "                return fpath\n",
    "                \n",
    "    if \"fpath\" not in locals():\n",
    "        standardDeviation(naip_file, dir, window_size=windowsize, overwrite=False)\n",
    "\n",
    "    logging.error(\"Unable to find standard deviation file for %s\" % naip_file)\n",
    "    raise Exception\n",
    "\n",
    "\n",
    "def findVIFile(type, dir, f):\n",
    "    fname = os.path.basename(f)\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if fname in file:\n",
    "                fpath = os.path.join(root, file)\n",
    "                return fpath\n",
    "\n",
    "    if \"fpath\" not in locals():\n",
    "        vegIndexCalc(f, dir, [type])\n",
    "    return None\n",
    "\n",
    "\n",
    "def createSubSetLandsat(naip_path, landsat_file, opath, overwrite=False):\n",
    "    ssl_start = datetime.now()\n",
    "    ofile = \"Landsat8_\" + os.path.basename(naip_path)\n",
    "\n",
    "    landsat_opath = os.path.join(opath, ofile)\n",
    "    \n",
    "    if not os.path.exists(landsat_opath) or overwrite:\n",
    "        start = datetime.now()\n",
    "        reference_f = gdal.Open(naip_path)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        proj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        # build landsat tile from naip extent\n",
    "\n",
    "        if \"ndsi\" in opath.lower() or \"ndwi\" in opath.lower():\n",
    "            resampletype = \"bilinear\"\n",
    "        else:\n",
    "            resampletype = \"bilinear\"\n",
    "            #resampletype = \"near\"\n",
    "\n",
    "        gdal_warp = \"gdalwarp -overwrite -tap -r %s -t_srs %s -tr %s %s -te_srs %s -te %s %s %s %s %s %s\" % (\n",
    "            resampletype, proj, resx, resy, proj, str(minx), str(miny), str(maxx), str(maxy), landsat_file, landsat_opath)\n",
    "        logging.debug(\"Executing gdal_warp operation on %s for footprint of naip file %s\" % (landsat_file, naip_path))\n",
    "        os.system(gdal_warp)\n",
    "\n",
    "        logging.debug(\"\\tFinished qquad for %s landsat in %s\" % (landsat_file, str(datetime.now() - ssl_start)))\n",
    "    \n",
    "    return landsat_opath\n",
    "\n",
    "# FUNCTION TO WRITE OUT CLASSIFIED RASTER\n",
    "def write_geotiff(fname, data, geo_transform, projection, classes, COLORS, data_type=gdal.GDT_Byte):\n",
    "    \"\"\"\n",
    "    Create a GeoTIFF file with the given data.\n",
    "    :param fname: Path to a directory with shapefiles\n",
    "    :param data: Number of rows of the result\n",
    "    :param geo_transform: Returned value of gdal.Dataset.GetGeoTransform (coefficients for\n",
    "                          transforming between pixel/line (P,L) raster space, and projection\n",
    "                          coordinates (Xp,Yp) space.\n",
    "    :param projection: Projection definition string (Returned by gdal.Dataset.GetProjectionRef)\n",
    "    \"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, data_type)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "\n",
    "    ct = gdal.ColorTable()\n",
    "    for pixel_value in range(len(classes)+1):\n",
    "        color_hex = COLORS[pixel_value]\n",
    "        r = int(color_hex[1:3], 16)\n",
    "        g = int(color_hex[3:5], 16)\n",
    "        b = int(color_hex[5:7], 16)\n",
    "        ct.SetColorEntry(pixel_value, (r, g, b, 255))\n",
    "    band.SetColorTable(ct)\n",
    "\n",
    "    metadata = {\n",
    "        'TIFFTAG_COPYRIGHT': 'CC BY 4.0, AND BEN HICKSON',\n",
    "        'TIFFTAG_DOCUMENTNAME': 'Land Cover Classification',\n",
    "        'TIFFTAG_IMAGEDESCRIPTION': 'Random Forests Supervised classification.',\n",
    "        'TIFFTAG_MAXSAMPLEVALUE': str(len(classes)),\n",
    "        'TIFFTAG_MINSAMPLEVALUE': '0',\n",
    "        'TIFFTAG_SOFTWARE': 'Python, GDAL, scikit-learn'\n",
    "    }\n",
    "    dataset.SetMetadata(metadata)\n",
    "\n",
    "    dataset = None  # Close the file\n",
    "    return\n",
    "\n",
    "\n",
    "def report_and_exit(txt, *args, **kwargs):\n",
    "    logger.error(txt, *args, **kwargs)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def getQQuadFromNAIP(f):\n",
    "    fname = os.path.basename(f)\n",
    "    qquad = fname.split(\"_\")[1] + \"_\" + fname.split(\"_\")[2]\n",
    "    return qquad\n",
    "\n",
    "\"\"\"\n",
    "def get_STDDev_VRT(naip_file, qquad_vrt_dir):\n",
    "    naip_path = getFullNAIPPath(naip_file, naip_dir)\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    rasters_stddev = []\n",
    "    \n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=3, overwrite=False)\n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=5, overwrite=False)\n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=10, overwrite=False)\n",
    "\n",
    "    #for bandnum in range(1, 5):\n",
    "    #    bandnum = \"band\" + str(bandnum)\n",
    "    #    ffile = \"stddev_\" + os.path.splitext(naip_file)[0] + bandnum + \".tif\"\n",
    "        \n",
    "    #   rasters_stddev.append(os.path.abspath(findFile(os.path.join(std3px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "    #    rasters_stddev.append(os.path.abspath(findFile(os.path.join(std5px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "    #    rasters_stddev.append(os.path.abspath(findFile(os.path.join(std10px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    stddev_vrt_dir = os.path.join(qquad_vrt_dir, \"stddev\")\n",
    "    vrt_stddev = os.path.join(stddev_vrt_dir, qquad + \"_stddev.vrt\")\n",
    "    #print(vrt_stddev)\n",
    "\n",
    "    if not os.path.exists(vrt_stddev):\n",
    "        build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_stddev, \" \".join(rasters_stddev))\n",
    "        logging.debug(\"BUILDING VRT WITH: \\n\\t%s\" % build_vrt)\n",
    "        os.system(build_vrt)\n",
    "\n",
    "    return vrt_stddev\n",
    "\n",
    "def get_GaussianFile(naip_file):\n",
    "    #naip_path = getFullNAIPPath(naip_file, naip_dir)\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    \n",
    "    gaussfile = gaussianCalc(naip_file, base_datadir, sigma=1, overwrite=False)\n",
    "    \n",
    "    return gaussfile\n",
    "\"\"\"\n",
    "    \n",
    "def get_VegIndicies_VRT(naip_file, qquad_vrt_dir):\n",
    "    print(naip_file)\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    rasters_float = []\n",
    "\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"NDVI\", ndvi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"SAVI\", savi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"OSAVI\", osavi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"MSAVI2\", msavi2_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"EVI2\", evi2_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    naipvis_vrt_dir = os.path.join(qquad_vrt_dir, \"naipvis\")\n",
    "    vrt_naipvis = os.path.join(naipvis_vrt_dir, qquad + \"_naipvis.vrt\")\n",
    "    \n",
    "    if not os.path.exists(vrt_naipvis):\n",
    "        build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_naipvis, \" \".join(rasters_float))\n",
    "        os.system(build_vrt)\n",
    "\n",
    "    return vrt_naipvis\n",
    "\n",
    "\n",
    "def createClassifiedFile(loc_NAIPFile,  rf_classifier, rf_args, mltype=\"RF\", overwrite=False):\n",
    "    beg = datetime.now()\n",
    "    print(\"Starting on qquad: %s\" % loc_NAIPFile)\n",
    "    logger.debug(\"Starting on qquad: %s\" % loc_NAIPFile)\n",
    "\n",
    "    file = os.path.basename(loc_NAIPFile)\n",
    "    \n",
    "    qquad = getQQuadFromNAIP(file)\n",
    "    \n",
    "    if mltype == \"RF\":\n",
    "        output_fname = \"{}_D{}E{}MPL{}_{}.tif\".\\\n",
    "            format(mltype, rf_args[\"maxdepth\"], rf_args[\"n_est\"], rf_args[\"min_per_leaf\"], qquad)\n",
    "    else:\n",
    "        print(\"Unknown classifier type '{}' specified. Exiting...\")\n",
    "        raise(ValueError)\n",
    "        \n",
    "    loc_classified_file = os.path.join(loc_classifiedQuarterQuads, output_fname)\n",
    "    print(\"loc_classified_file; \", loc_classified_file)\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        cl_start = datetime.now()\n",
    "        logging.info(\"\\tClassifying landcover file at %s...\" % (loc_classified_file))\n",
    "        # loc_NAIPFile = os.path.join(root, file)\n",
    "\n",
    "        \"\"\"\n",
    "        vrt_naipvis = get_VegIndicies_VRT(loc_NAIPFile, qquad_vrt_dir) #  All float32\n",
    "        #vrt_stddev = get_STDDev_VRT(file)  # All 8 bit (\"byte\")\n",
    "        gaussf_path = get_GaussianFile(loc_NAIPFile)\n",
    "\n",
    "        # BEN!!!!!! YOU REMOVED LANDSAT VARIABLE FOR QUICK RUN\n",
    "        landsat_path = createSubSetLandsat(loc_NAIPFile, landsat_file, landsat_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        landsat_ndsi_path = createSubSetLandsat(loc_NAIPFile, ndsi_file, ndsi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "        landsat_ndwi_path = createSubSetLandsat(loc_NAIPFile, ndwi_file, ndwi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        bands_data = []\n",
    "        #for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "        for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "            print(\"\\tIN RAS: {}\".format(inras))\n",
    "            try:\n",
    "                raster_dataset = gdal.Open(inras, gdal.GA_ReadOnly)\n",
    "            except RuntimeError as e:\n",
    "                report_and_exit(str(e))\n",
    "\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "\n",
    "            for b in range(1, raster_dataset.RasterCount + 1):\n",
    "                band = raster_dataset.GetRasterBand(b)\n",
    "                bands_data.append(band.ReadAsArray())\n",
    "        \"\"\"\n",
    "        # GET PROJECTION INFO FOR GDAL WRITE\n",
    "        try:\n",
    "            raster_dataset = gdal.Open(loc_NAIPFile, gdal.GA_ReadOnly)\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "        except RuntimeError as e:\n",
    "            report_and_exit(str(e))\n",
    "\n",
    "\n",
    "        training_raster = generateStack(loc_NAIPFile)\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        #bands_data = np.dstack(bands_data)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        with rio.open(training_raster) as tras:\n",
    "            print(tras)\n",
    "            bands_data = tras.read()\n",
    "\n",
    "        scikit_array = np.moveaxis(bands_data, 0, -1)\n",
    "        print(scikit_array.shape)\n",
    "        rows, cols, n_bands = scikit_array.shape\n",
    "        n_samples = rows * cols\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        flat_pixels = scikit_array.reshape((n_samples, n_bands))\n",
    "\n",
    "        classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
    "\n",
    "        # A list of colors for each class\n",
    "        COLORS = [\n",
    "            \"#000000\",  # 0 EMPTY\n",
    "            \"#00af11\",  # 1 - Vegetation - Thick\n",
    "            \"#00e513\",  # 2 - Vegetation - Thin\n",
    "            \"#e9ff5a\",  # 3 - Herbaceous\n",
    "            \"#f1ac34\",  # 4 - Barren - Light\n",
    "            \"#a9852e\",  # 5 - Barren - Dark\n",
    "            \"#2759ff\",  # 6 - Water\n",
    "            \"#efefef\",  # 7 - Roof - White\n",
    "            \"#d65133\",  # 8 - Roof - Red\n",
    "            \"#cecece\",  # 9 - Roof - Grey\n",
    "            \"#a0a0a0\",  # 10 - Impervious - Light\n",
    "            \"#555555\",  # 11 - Impervious - Dark\n",
    "            \"#000000\",  # 12 - Shadows\n",
    "            \"#00734C\"   # 13 - Vegetation - Irrigated\n",
    "        ]\n",
    "\n",
    "        print(\"Classifying...\")\n",
    "        \"\"\"if not np.all(np.isfinite(flat_pixels)):\n",
    "            print(\"Not all value finite. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "        if np.any(np.isnan(flat_pixels)):\n",
    "            print(\"Some values are NaN. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels == np.NaN, np.finfo(np.float32).max, flat_pixels)\n",
    "        \"\"\"\n",
    "        #try:\n",
    "        result = rf_classifier.predict(flat_pixels)\n",
    "        # Reshape the result: split the labeled pixels into rows to create an image\n",
    "        classification = result.reshape((rows, cols))\n",
    "\n",
    "        # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "        # TODO - Rewrite this to use rasterio for consistency\n",
    "        write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "        logging.info(\"\\tCreated classified file in %s\" % (str(datetime.now() - cl_start)))\n",
    "        #except (ValueError) as e:\n",
    "        #logging.info(\"-----------BAD VALUES FOR PREDICTORS. SKIPPING FILE %s\\n%s\" % (file, str(e)))\n",
    "        #return None\n",
    "        \n",
    "        del bands_data\n",
    "        del flat_pixels\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"LandCover file %s exists and no overwrite.\" % loc_classified_file)\n",
    "\n",
    "    \"\"\"\n",
    "    o_veg_loc = r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\RiparianClass_VBs\"\n",
    "    utilities.useDirectory(o_veg_loc)\n",
    "        \n",
    "    quadrant_loc = os.path.join(o_veg_loc, qquad[:5])\n",
    "    utilities.useDirectory(quadrant_loc)\n",
    "    \n",
    "    # not writing out density calculations\n",
    "    #dq_path = os.path.join(degress_quadrant_loc, denseVeg_file)\n",
    "    #sq_path = os.path.join(degress_quadrant_loc, sparseVeg_file)\n",
    "    \n",
    "    riparian_class_qquad = os.path.join(quadrant_loc, \"RiparianClassification_\" + qquad + \".tif\")\n",
    "\n",
    "    if not os.path.exists(riparian_class_qquad) or overwrite:\n",
    "        createRiparianClass(loc_classified_file, riparian_class_qquad, qquad)\n",
    "\n",
    "    shutil.copy(riparian_class_qquad, r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\Pinal_AOI\")\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"COMPLETED riparian classification. Finished in %s\" % (str(datetime.now() - beg)))\n",
    "    logger.debug(\"_____________________________________________________________________________________\")\n",
    "\n",
    "    return loc_classified_file\n",
    "\n",
    "\n",
    "def createRiparianClass(lc_raster, o_file, qquad):\n",
    "    rc_start = datetime.now()\n",
    "    logging.info(\"\\tClassifying riparian zones for %s\" % lc_raster)\n",
    "    with rio.open(lc_raster) as class_file:\n",
    "        class_array = class_file.read(1)#_band(1)\n",
    "        kwargs = class_file.profile\n",
    "    \n",
    "    # Get average densities of each class across the whole raster.\n",
    "    # TO DO - Updatet this to be evaulate on on something more specific than the qquad area\n",
    "    dense_veg_array = np.where(class_array == 1, 1, 0)\n",
    "    dense_file_avg = np.mean(dense_veg_array)\n",
    "    sparse_veg_array = np.where(class_array == 2, 1, 0)\n",
    "    sparse_file_avg = np.mean(sparse_veg_array)\n",
    "    \n",
    "    sparse_veg_array_localmean = ndimage.uniform_filter(sparse_veg_array.astype(np.float32), size=vaa_diameter, mode='constant')\n",
    "    dense_veg_array_localmean = ndimage.uniform_filter(dense_veg_array.astype(np.float32), size=vaa_diameter, mode='constant')\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # CRITICAL : Identify the splits where xero, meso, and hydro will be identified \n",
    "    # based on density ofsparse and thick vegetation\n",
    "    sparse_xero_lowlimit = sparse_file_avg + np.std(sparse_veg_array_localmean)\n",
    "    sparse_meso_lowlimit = sparse_file_avg + (1-sparse_file_avg)*0.7\n",
    "    sparse_hydro_lowlimit = sparse_file_avg + (1-sparse_file_avg)*0.9\n",
    "    \n",
    "    dense_xero_lowlimit = dense_file_avg + np.std(dense_veg_array_localmean)\n",
    "    dense_meso_lowlimit = dense_file_avg + (1-dense_file_avg)*0.7\n",
    "    dense_hydro_lowlimit = dense_file_avg + (1-dense_file_avg)*0.9\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Reassign pixel values based on density assessment\n",
    "    sparse_local_xero  = np.where(sparse_veg_array_localmean > sparse_xero_lowlimit,  1, 0) # xero (1) if true, upland (0) if false\n",
    "    sparse_local_meso  = np.where(sparse_veg_array_localmean > sparse_meso_lowlimit,  2, 0) # meso (2) if true, upland (0) if false\n",
    "    sparse_local_hydro = np.where(sparse_veg_array_localmean > sparse_hydro_lowlimit, 3, 0) # hydro (3) if true, upland (0) if false\n",
    "    # For some reason can't take numpy.maximum from more than two arrays at once\n",
    "    sparse_combine = np.maximum(sparse_local_xero, sparse_local_meso)#, sparse_local_hydro)\n",
    "    sparse_combine = np.maximum(sparse_combine, sparse_local_hydro)\n",
    "        \n",
    "    dense_local_xero  = np.where(dense_veg_array_localmean > dense_xero_lowlimit,  1, 0) # xero (1) if true, upland (0) if false\n",
    "    dense_local_meso  = np.where(dense_veg_array_localmean > dense_meso_lowlimit,  2, 0) # meso (2) if true, upland (0) if false\n",
    "    dense_local_hydro = np.where(dense_veg_array_localmean > dense_hydro_lowlimit, 3, 0) # hydro (3) if true, upland (0) if false\n",
    "    # For some reason can't take numpy.maximum from more than two arrays at once\n",
    "    dense_combine = np.maximum(dense_local_xero, dense_local_meso)#, sparse_local_hydro)\n",
    "    dense_combine = np.maximum(dense_combine, dense_local_hydro)\n",
    "    \n",
    "    # COMPARISON OF DENSITY VALUES OF BOTH RASTERS AT EACH PIXEL FOR \n",
    "    # DETERMINATION. ESSENTAILLY A DECISION TREE\n",
    "    p = np.where(dense_combine == 0, np.where(sparse_combine == 0, 0, 0), 0)\n",
    "    o = np.where(dense_combine == 0, np.where(sparse_combine == 1, 1, p), p)\n",
    "    n = np.where(dense_combine == 0, np.where(sparse_combine == 2, 2, o), o)\n",
    "    m = np.where(dense_combine == 0, np.where(sparse_combine == 3, 3, n), n)\n",
    "    l = np.where(dense_combine == 1, np.where(sparse_combine == 0, 1, m), m)\n",
    "    k = np.where(dense_combine == 1, np.where(sparse_combine == 1, 1, l), l)\n",
    "    j = np.where(dense_combine == 1, np.where(sparse_combine == 2, 2, k), k)\n",
    "    i = np.where(dense_combine == 1, np.where(sparse_combine == 3, 3, j), j)\n",
    "    h = np.where(dense_combine == 2, np.where(sparse_combine == 0, 1, i), i)\n",
    "    g = np.where(dense_combine == 2, np.where(sparse_combine == 1, 2, h), h)\n",
    "    f = np.where(dense_combine == 2, np.where(sparse_combine == 2, 2, g), g)\n",
    "    e = np.where(dense_combine == 2, np.where(sparse_combine == 3, 3, f), f)\n",
    "    d = np.where(dense_combine == 3, np.where(sparse_combine == 0, 2, e), e)\n",
    "    c = np.where(dense_combine == 3, np.where(sparse_combine == 1, 2, d), d)\n",
    "    b = np.where(dense_combine == 3, np.where(sparse_combine == 2, 3, c), c)\n",
    "    riparian = np.where(dense_combine == 3, np.where(sparse_combine == 3, 3, b), b)\n",
    "    \n",
    "    kwargs.update(\n",
    "        dtype=np.uint8,\n",
    "        nodata=0,\n",
    "        compress='lzw'\n",
    "    )\n",
    "\n",
    "    valleybottom_ras = findVBRaster(qquad)\n",
    "    \n",
    "    with rio.open(valleybottom_ras) as vb_raster:\n",
    "        vb_array = vb_raster.read(1).astype(np.float32)\n",
    "    \n",
    "    #print(\"Clipping to Valley Bottoms\")\n",
    "    clipped_riparian = np.where(vb_array > 1, riparian, 0)\n",
    "\n",
    "    with rio.open(o_file, 'w', **kwargs) as dst:\n",
    "        dst.write_band(1, clipped_riparian.astype(np.uint8))\n",
    "\n",
    "        dst.write_colormap(\n",
    "            1, {\n",
    "                0: (255, 255, 255),\n",
    "                1: (186,228,179),\n",
    "                2: (116,196,118),\n",
    "                3: (35,139,69)})\n",
    "        cmap = dst.colormap(1)\n",
    "        \n",
    "    logging.info(\"\\tFinished riparian classification in %s\" % (str(datetime.now()-rc_start)))\n",
    "\n",
    "\n",
    "def findVBRaster(qquad, overwrite=False):\n",
    "    vb_start = datetime.now()\n",
    "    logging.debug(\"Starting creation of subset of valley bottom...\")\n",
    "    naip_path = getFullNAIPPath(qquad, naip_dir)\n",
    "    ofile = \"ValleyBottom_\" + qquad + \".tif\"\n",
    "\n",
    "    o_path = os.path.join(loc_valleybottoms, ofile)\n",
    "    \n",
    "    reference_f = gdal.Open(VBET_VB_loc)\n",
    "    geo_transform = reference_f.GetGeoTransform()\n",
    "    sproj = reference_f.GetProjectionRef()\n",
    "    \n",
    "    # TODO - Duplicative scripting. Exisits twice in this file and also in the VBET classification\n",
    "    if not os.path.exists(o_path) or overwrite:\n",
    "        reference_f = gdal.Open(naip_path)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        tproj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        resampletype = \"bilinear\"\n",
    "        \n",
    "        gdal_warp = \"gdalwarp -overwrite -tap -r %s -s_srs %s -t_srs %s -tr %s %s -te_srs %s -te %s %s %s %s %s %s\" % (\n",
    "            resampletype, sproj, tproj, resx, resy, tproj, str(minx), str(miny), str(maxx), str(maxy), VBET_VB_loc, o_path)\n",
    "        #print(\"Executing gdal_warp operation on %s for footprint of naip file %s\" % (o_path, naip_path))\n",
    "        os.system(gdal_warp)\n",
    "\n",
    "        logging.debug(\"\\tFinished VB subset in %s\" % (str(datetime.now() - vb_start)))\n",
    "    \n",
    "    return o_path\n",
    "\n",
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n",
    "\n",
    "\n",
    "def calculateGeom(row):\n",
    "    #print(row)\n",
    "    geom = row[\"geometry\"]\n",
    "    if row['PROJ'] == \"NAD83 / UTM zone 11N\":\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "\n",
    "        point = Point(transform(utm12, utm11, x, y))\n",
    "        if point.x <= 0 or point.y <= 0:\n",
    "            print(\"Bad\", point, \"Original: \", geom)\n",
    "        return point\n",
    "    else:\n",
    "        return geom\n",
    "\n",
    "\n",
    "def extractToPoints(training_points, out_file, data_dir, landsatdir, overwrite=False):\n",
    "\n",
    "    def get_values(geom):\n",
    "        # print(row)\n",
    "        # geom = row['geometry']\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "\n",
    "        values = []\n",
    "        # for raster in raster_objects:\n",
    "        # print(\"Starting Raster Extract for %s at x:%s y:%s\" % (os.path.basename(raster), str(x), str(y)))\n",
    "        # with rio.open(raster) as ras:\n",
    "        \"\"\"for val in rasnaip.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in rasnaipvis.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in rasgauss.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in raslandsat.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in rasNDSI.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in rasNDWI.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        \"\"\"\n",
    "        #print(geom)\n",
    "        for val in ras_stack.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "\n",
    "        return pd.Series(values, index=band_order)\n",
    "\n",
    "    ext_start = datetime.now()\n",
    "    landsat_qquad_dir = os.path.join(landsatdir, \"byNAIPDOY_QQuads\")\n",
    "    ndwi_qquad_dir = os.path.join(data_dir, \"NDWI\")\n",
    "    ndsi_qquad_dir = os.path.join(data_dir, \"NDSI\")\n",
    "    # LOCATION OF THE NDSI FILE\n",
    "    ndsi_file = os.path.join(ndsi_qquad_dir, \"LandsatOLI_NDSI_30m.tif\")\n",
    "    # LOCATION OF THE NDWI FILE\n",
    "    ndwi_file = os.path.join(ndwi_qquad_dir, \"LandsatOLI_NDWI_30m.tif\")\n",
    "\n",
    "    utils.useDirectory(landsat_qquad_dir)\n",
    "    utils.useDirectory(ndwi_qquad_dir)\n",
    "    utils.useDirectory(ndsi_qquad_dir)\n",
    "\n",
    "    # IF VECTOR FILE OF POINTS WITH RASTER EXTRACTS DOESN'T EXIST, BUILD IT\n",
    "    if not os.path.exists(out_file) or overwrite:\n",
    "        # if \"class_points\" not in locals():\n",
    "        #    logging.debug(\"READING IN %s as training_points\" % training_points)\n",
    "        training_data_df = gpd.read_file(training_points, crs={'init': 'epsg:26912'})\n",
    "        #print(\"Columns: \", training_data_df.columns)\n",
    "\n",
    "        if \"utm_geom\" not in training_data_df:\n",
    "            logging.debug(\"ADDING COLUMN 'utm_geom' WITH CORRECT UTM COORDINATES FOR EACH QUARTER QUAD\")\n",
    "            # CREATE TRUE RASTER GEOMETRY COLUMN (BASED ON UTM)\n",
    "            training_data_df[\"utm_geom\"] = training_data_df.apply(calculateGeom, axis=1)\n",
    "\n",
    "        # NDSI is only used because its the last raster column\n",
    "        if \"L8_NDWI\" not in training_data_df:\n",
    "            logging.debug(\"CREATING COLUMNS...\")\n",
    "            # CREATE EMPTY COLUMNS IN DATA FRAME FOR EACH RASTER VARIABLE\n",
    "\n",
    "            sample_naip = training_data_df.iloc[0]['NAIP_FILE']\n",
    "            raster_loc = generateStack(sample_naip)\n",
    "            band_columns = []\n",
    "            with rio.open(raster_loc) as ras_stack:\n",
    "                for i in range(1, ras_stack.count + 1):\n",
    "                    column = ras_stack.tags(i)['NAME']\n",
    "                    band_columns.append(column)\n",
    "                    training_data_df[column] = np.NaN\n",
    "        #for column in band_columns:\n",
    "\n",
    "\n",
    "        #print(\"COLUMNS\", training_data_df.columns)\n",
    "\n",
    "        net_percentage = 0.0\n",
    "        # ITERATE THROUGH DATAFRAME IN GROUPS BY NAIP_FILE. KEEPS US FROM OPENING/CLOSING RASTERS FOR EACH POINT - INSTEAD FOR EACH GROUP\n",
    "        for loc_NAIPFile, group in training_data_df.groupby(\"NAIP_FILE\"):\n",
    "            logger.debug(\"\\nStarting raster value extraction for points in qquad %s\" % loc_NAIPFile)\n",
    "            print(\"\\nStarting raster value extraction for points in qquad %s\" % loc_NAIPFile)\n",
    "            loc_NAIPFile.replace(\"\\\\\", \"/\")  # normalize for windows paths\n",
    "\n",
    "            # LOOK FOR RASTERS FROM WHICH VALUES WILL BE EXTRACTED\n",
    "            #file = os.path.basename(loc_NAIPFile)\n",
    "\n",
    "            \"\"\"\n",
    "            vrt_naipvis = get_VegIndicies_VRT(loc_NAIPFile, qquad_vrt_dir)\n",
    "            # vrt_stddev = get_STDDev_VRT(file)\n",
    "            gaussf_path = get_GaussianFile(loc_NAIPFile)\n",
    "\n",
    "            landsat_path = createSubSetLandsat(loc_NAIPFile, landsat_file, landsat_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            landsat_ndsi_path = createSubSetLandsat(loc_NAIPFile, ndsi_file, ndsi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "            landsat_ndwi_path = createSubSetLandsat(loc_NAIPFile, ndwi_file, ndwi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            net_percentage += 100 * len(training_data_df.loc[training_data_df[\"NAIP_FILE\"] == loc_NAIPFile]) / len(\n",
    "                training_data_df)\n",
    "            logger.debug(\"Percentage of total: %d\" % net_percentage)\n",
    "            # SELECT POINTS WHICH HAVE NAIP PATH VALUE\n",
    "\n",
    "            # Only if group hasn't had values assigned (Jupyter and Rodeo iterations)\n",
    "            \"\"\"\n",
    "\n",
    "            if group[\"L8_NDWI\"].isnull().values.any():\n",
    "                training_raster = generateStack(loc_NAIPFile)\n",
    "\n",
    "                band_order = []\n",
    "                with rio.open(training_raster) as ras_stack:\n",
    "                    for i in range(1, ras_stack.count + 1):\n",
    "                        band_order.append(ras_stack.tags(i)['NAME'])\n",
    "\n",
    "                    training_data_df.loc[\n",
    "                        training_data_df.NAIP_FILE == loc_NAIPFile, band_columns] = \\\n",
    "                        training_data_df.loc[training_data_df.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_values)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            if group[\"NDSI\"].isnull().values.any():\n",
    "                with rio.open(loc_NAIPFile) as rasnaip:\n",
    "                    with rio.open(vrt_naipvis) as rasnaipvis:\n",
    "                        with rio.open(gaussf_path) as rasgauss:\n",
    "                            with rio.open(landsat_path) as raslandsat:\n",
    "                                with rio.open(landsat_ndsi_path) as rasNDSI:\n",
    "                                    with rio.open(landsat_ndwi_path) as rasNDWI:\n",
    "                                        count = 0\n",
    "                                        training_data_df.loc[\n",
    "                                            training_data_df.NAIP_FILE == loc_NAIPFile, rasters_names] = \\\n",
    "                                            training_data_df.loc[\n",
    "                                                training_data_df.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(\n",
    "                                                get_values)\n",
    "            \"\"\"\n",
    "            logger.debug(\"Finished with group %s at %s\" % (loc_NAIPFile, str(datetime.now())))\n",
    "\n",
    "        # REMOVE ALL ROWS WHICH EXTRACTED NO DATA VALUES FROM LANDSAT\n",
    "        # for column in landsat:\n",
    "        #    class_points = class_points[class_points.loc[column] != 32766]\n",
    "\n",
    "        logger.info(\"Finished raster value extraction of %s points in %s\" % (\n",
    "            str(len(training_data_df)), str(datetime.now() - ext_start)))\n",
    "\n",
    "        # GEOPANDAS WON\"T ALLOW MORE THAN ONE COLUMN WITH GEOMETRY TYPE. REMOVE THE utm_geom COLUMN CREATED PREVIOUSLY\n",
    "        del training_data_df['utm_geom']\n",
    "        # print(\"COLUMNS:\\n\", training_data_df.columns)\n",
    "        print(\"WRITING DATAFRAME TO OUTPUT...\")\n",
    "        logger.debug(\"WRITING DATAFRAME TO OUTPUT...\")\n",
    "        training_data_df.to_file(out_file)\n",
    "\n",
    "    else:\n",
    "        # if \"training_data_df\" not in \"locals\":\n",
    "        print(\"Reading in point file %s\" % out_file)\n",
    "        logger.info(\"Reading in point file %s\" % out_file)\n",
    "\n",
    "        # BEN - UPDATE RASTERS NAMES READ\n",
    "        training_data_df = gpd.read_file(out_file)\n",
    "        # Had to delete utm_geom when writing file (can't have two geometry columns). Recreate...\n",
    "        print(training_data_df.columns)\n",
    "        band_columns = training_data_df.columns.tolist()[4:-1]\n",
    "\n",
    "    return {\"training_points\": training_data_df, \"band_names\": band_columns}\n",
    "\n",
    "\n",
    "def getClassifier(classifier_file, train_data, rf_rasters, args, createClassifier=False):\n",
    "    \"\"\" Either returns classifier from file or creates a new one \"\"\"\n",
    "    if not os.path.exists(classifier_file) or createClassifier:\n",
    "        # save classifier to file\n",
    "        # TRAIN RANDOM FORESTS\n",
    "        rf_start = datetime.now()\n",
    "        logger.info(\"Beginning Random Forest Train\")\n",
    "        #maxdepth = 400\n",
    "        #n_est = 40\n",
    "        #n_job = 1\n",
    "        #min_per_leaf = 50\n",
    "        #crit = \"entropy\"  # gini or entropy\n",
    "\n",
    "        rf_model = RandomForestClassifier(verbose=1, max_depth=args[\"maxdepth\"], n_estimators=args[\"n_est\"],\n",
    "                                          n_jobs=args[\"n_job\"], min_samples_leaf=args[\"min_per_leaf\"],\n",
    "                                          criterion=args[\"crit\"])\n",
    "\n",
    "        #X = Imputer().fit_transform(train_data[rf_rasters].dropna())\n",
    "\n",
    "        rf_model.fit(train_data[rf_rasters].dropna(),\n",
    "                     train_data[rf_rasters + [\"Class\"]].dropna()[\"Class\"])\n",
    "\n",
    "        logger.info(\"Finished Fitting in\", datetime.now() - rf_start)\n",
    "        #_ = joblib.dump(rf_model, classifier_file, compress=9)\n",
    "    else:\n",
    "        # load classifier from file\n",
    "        rf_model = joblib.load(classifier_file)\n",
    "\n",
    "    return rf_model\n",
    "\n",
    "\n",
    "def createClassification(base_datadir, naip_dir=False):\n",
    "    if not naip_dir:\n",
    "        naip_dir = os.path.join(base_datadir, \"NAIP\")\n",
    "\n",
    "    base_landsatdir = os.path.join(base_datadir, \"Landsat8\")\n",
    "\n",
    "    training_data_dir = os.path.join(base_datadir, \"inital_model_inputs\")\n",
    "\n",
    "    #ndvi_dir = utils.useDirectory(os.path.join(base_datadir, \"NDVI\"))\n",
    "    #savi_dir = utils.useDirectory(os.path.join(base_datadir, \"SAVI\"))\n",
    "    #osavi_dir = utils.useDirectory(os.path.join(base_datadir, \"OSAVI\"))\n",
    "    #msavi2_dir = utils.useDirectory(os.path.join(base_datadir, \"MSAVI2\"))\n",
    "    #evi2_dir = utils.useDirectory(os.path.join(base_datadir, \"EVI2\"))\n",
    "    ndwi_qquad_dir = utils.useDirectory(os.path.join(base_datadir, \"NDWI\"))\n",
    "    ndsi_qquad_dir = utils.useDirectory(os.path.join(base_datadir, \"NDSI\"))\n",
    "\n",
    "    # std3px_dir = os.path.join(base_datadir, \"StdDev_3px\")\n",
    "    # std5px_dir = os.path.join(base_datadir, \"StdDev_5px\")\n",
    "    # std10px_dir = os.path.join(base_datadir, \"StdDev_10px\")\n",
    "\n",
    "    base_landsatdir = utils.useDirectory(os.path.join(base_datadir, \"Landsat8\"))\n",
    "    #landsat_qquad_dir = utils.useDirectory(os.path.join(base_landsatdir, \"byNAIPDOY_QQuads\"))\n",
    "    valleybottoms_dir = utils.useDirectory(os.path.join(base_datadir, \"ValleyBottoms\"))\n",
    "    loc_valleybottoms = utils.useDirectory(os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms\"))\n",
    "\n",
    "    # LOCATION OF THE LANDSAT FILE\n",
    "    landsat_file = os.path.os.path.join(base_landsatdir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")\n",
    "    # LOCATION OF THE NDSI FILE\n",
    "    ndsi_file = os.path.join(ndsi_qquad_dir, \"LandsatOLI_NDSI_30m.tif\")\n",
    "    # LOCATION OF THE NDWI FILE\n",
    "    ndwi_file = os.path.join(ndwi_qquad_dir, \"LandsatOLI_NDWI_30m.tif\")\n",
    "\n",
    "    # LOCATION OF FILE CONTAINING CLASSIFICATION POLYGONS\n",
    "    loc_class_polygons = os.path.join(training_data_dir, 'classificationTrainingPolygons.shp')\n",
    "\n",
    "    # LOCATION OF USGS QUARTER QUADs in ARIZONA\n",
    "    loc_usgs_qquads = os.path.join(training_data_dir, \"USGS_QQuads_AZ.shp\")\n",
    "\n",
    "    # DIRECTORY HOLDING VRTS BY QUARTER QUAD FOR VEGETATION INDICIES (FLOAT32) AND STD_DEV (UINT8))\n",
    "    qquad_vrt_dir = os.path.join(base_datadir, \"QQuad_VRTs\")\n",
    "\n",
    "    day = datetime.today().strftime('%Y%m%d')\n",
    "    global loc_classifiedQuarterQuads\n",
    "    loc_classifiedQuarterQuads = utils.useDirectory(os.path.join(base_datadir, \"classifiedQuarterQuads_\" + day))\n",
    "    print(\"loc_classifiedQuarterQuads: \", loc_classifiedQuarterQuads)\n",
    "\n",
    "    loc_classifier = os.path.join(training_data_dir, \"RandomForestClassifier.joblib.pkl\")\n",
    "\n",
    "    aoi = gpd.read_file(os.path.join(training_data_dir, \"Ecoregions_AOI.gpkg\"))\n",
    "\n",
    "    valleybottoms_dir = utils.useDirectory(os.path.join(base_datadir, \"ValleyBottoms\"))\n",
    "    loc_valleybottoms = utils.useDirectory(os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms\"))\n",
    "    VBET_VB_loc = os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms_20180624.tif\")\n",
    "\n",
    "    # DEFINE THE PROJECTION USED OVER ARIZONA. USED FOR TRANSLATING POINT GEOMETRY LATER ON\n",
    "    utm11 = Proj(init=\"epsg:26911\")\n",
    "    utm12 = Proj(init=\"epsg:26912\")\n",
    "\n",
    "    \"\"\"\n",
    "    # IDENTIFY RASTER VARIABLES\n",
    "    # THESE ORDER OF THESE RASTER VARIABLES MUST COINCIDE WITH THE CONSTRUCTED ARRAY OF EXTRACTS in the get_values FUNCTION\n",
    "    naip = [\"NAIP1\", \"NAIP2\", \"NAIP3\", \"NAIP4\"]\n",
    "    landsat = [\"Landsat1\", \"Landsat2\", \"Landsat3\", \"Landsat4\", \"Landsat5\", \"Landsat6\", \"Landsat7\", \"Landsat8\"]\n",
    "    landsat_vis = [\"NDSI\", \"NDWI\"]\n",
    "    naip_vis = [\"NDVI\", \"EVI2\", \"SAVI\", \"OSAVI\", 'MSAVI2']\n",
    "\n",
    "    textures = [\"StdDev_3px_band1\", \"StdDev_3px_band2\", \"StdDev_3px_band3\", \"StdDev_3px_band4\",\n",
    "                \"StdDev_5px_band1\", \"StdDev_5px_band2\", \"StdDev_5px_band3\", \"StdDev_5px_band4\",\n",
    "                \"StdDev_10px_band1\", \"StdDev_10px_band2\", \"StdDev_10px_band3\", \"StdDev_10px_band4\"]\n",
    "    filters = [\"Gauss1_band1\", \"Gauss1_band2\", \"Gauss1_band3\", \"Gauss1_band4\",]\n",
    "    #rasters_names = naip + naip_vis + filters + landsat + landsat_vis\n",
    "    #print(rasters_names)\n",
    "    \"\"\"\n",
    "\n",
    "    classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
    "\n",
    "    loc_out_join = Create_Classification_Points.joinProjectionAndNAIP(loc_class_polygons, naip_dir, loc_usgs_qquads,\n",
    "                                                                      overwrite=False)\n",
    "\n",
    "    loc_points_in_poly = Create_Classification_Points.createShapefilePoints(loc_out_join, overwrite=False)\n",
    "\n",
    "    # LOCATION OF FILE CONTAINING CLASSIFICATION POINTS POST EXTRACT\n",
    "    loc_points_wRaster_extracts = loc_points_in_poly[:-4] + \"_extracts.shp\"\n",
    "\n",
    "    # EXTRACT RASTER VALUES TO POINTS\n",
    "    training_info = extractToPoints(loc_points_in_poly, loc_points_wRaster_extracts, base_datadir, base_landsatdir, overwrite=False)\n",
    "\n",
    "    class_points = training_info[\"training_points\"]\n",
    "    band_names = training_info[\"band_names\"]\n",
    "    logger.debug(\"Available raster variables: \\n\\t%s\" % band_names)\n",
    "\n",
    "    class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "\n",
    "    # Split the points data frame into train and test\n",
    "    training_data, testing_data = train_test_split(class_points, test_size=0.3)\n",
    "\n",
    "    # INITIALIZE COLUMN FOR PREDICTED CLASSIFICATION VALUES\n",
    "    predicted_column = \"CLASS_PREDICT\"\n",
    "    testing_data[predicted_column] = \"Null\"\n",
    "\n",
    "    # remove_landsat = True\n",
    "\n",
    "    # rasters values used in random forest\n",
    "    # Allows removal of some rasters\n",
    "    temp_rasters = band_names[:]\n",
    "    rf_rasters = band_names[:]\n",
    "    print(\"rf_raster: \", rf_rasters)\n",
    "\n",
    "    \"\"\"\n",
    "    if remove_landsat:\n",
    "        for r in temp_rasters:\n",
    "            if \"Landsat\" in r:\n",
    "                rf_rasters.remove(r)\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Using raster variables: \\n%s\" % rf_rasters)\n",
    "\n",
    "    rf_args = {\"maxdepth\": 400,\n",
    "               \"n_est\": 40,\n",
    "               \"n_job\": 1,\n",
    "               \"min_per_leaf\": 50,\n",
    "               \"crit\": \"entropy\"}  # gini or entropy}\n",
    "\n",
    "    # print('Out-of-bag score estimate:', {rf.oob_score_:.3})\n",
    "    rf = getClassifier(loc_classifier, training_data, rf_rasters, rf_args, createClassifier=True)\n",
    "\n",
    "    irods_sess, irods_files = utils.getFilesonDE()\n",
    "\n",
    "\n",
    "    print(\"------------------- STARTING CREATION FOR TRAINING QUADS -----------------------------\")\n",
    "    files_list = []\n",
    "    for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "        files_list.append(loc_NAIPFile)\n",
    "\n",
    "    Parallel(n_jobs=2)(delayed(createClassifiedFile)(naip_file, rf, rf_args) for naip_file in files_list)\n",
    "    #    classified_file_rf = createClassifiedFile(loc_NAIPFile, rf, rf_args, overwrite=False)\n",
    "\n",
    "    print(\"-------------------STARTING CREATION FOR SUB AREA-----------------------------\")\n",
    "    single_comp_subset = []\n",
    "    with open(os.path.join(base_datadir, \"NaipDone1.txt\")) as txt:\n",
    "        for l in txt:\n",
    "            fullname = getFullNAIPPath(l[:-1], naip_dir)\n",
    "            # print(l)\n",
    "            single_comp_subset.append(fullname)\n",
    "            #if l[:-1] not in irods_files.keys():\n",
    "            #    single_comp_subset.append(fullname)\n",
    "\n",
    "            #createClassifiedFile(fullname, rf, rf_args)\n",
    "\n",
    "    Parallel(n_jobs=2)(delayed(createClassifiedFile)(naip_file, rf, rf_args) for naip_file in single_comp_subset)\n",
    "    # for qquad_name in single_comp_subset:\n",
    "    # fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "\n",
    "    # createClassifiedFile(fpath, \"RF\", rf, rf_args, overwrite=False)\n",
    "\n",
    "    print(\"-------------------FINISHED CREATION FOR SUB AREA-----------------------------\")\n",
    "\n",
    "    print(\"-------------------STARTING CREATION FOR AOI-----------------------------\")\n",
    "    aoi.crs = fiona.crs.from_epsg(2163)\n",
    "    print(aoi.crs)\n",
    "\n",
    "    footprints = gpd.read_file(loc_usgs_qquads)\n",
    "    aoi.to_crs(footprints.crs, inplace=True)\n",
    "\n",
    "    aoi_qquads = []\n",
    "    for i, row in footprints.iterrows():\n",
    "        for j, arow in aoi.iterrows():\n",
    "            if row.geometry.within(arow.geometry):\n",
    "                aoi_qquads.append(row.Name)\n",
    "    print(len(aoi_qquads))\n",
    "    print(\"AOI QQUADS\\n{}\\n\".format(aoi_qquads))\n",
    "\n",
    "    for qquad_name in aoi_qquads:\n",
    "        fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "\n",
    "        beg = datetime.now()\n",
    "        logger.debug(\"Starting on qquad: %s\" % qquad_name)\n",
    "        createClassifiedFile(fpath, rf, rf_args, overwrite=False)\n",
    "        logger.debug(\"COMPLETED riparian classification. Finished in %s\" % (str(datetime.now() - beg)))\n",
    "        logger.debug(\"_____________________________________________________________________________________\")\n",
    "\n",
    "    print(\"-------------------FINISHED CREATING FOR AOI-----------------------------\")\n",
    "\n",
    "    exit()\n",
    "\n",
    "    def get_class_value(geom):\n",
    "        \"\"\" TAKES A VARIABLE OF GEOMETRY TYPE AND RETURNS THE VALUE AT X,Y\n",
    "        AS A PANDAS SERIES FOR FOR LOCAL RASTER 'CLASSRAS' \"\"\"\n",
    "        # print(row)\n",
    "        # geom = row.geometry\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "        for val in classras.sample([(x, y)]):\n",
    "            # print(np.ndarray.tolist(val))\n",
    "            return pd.Series(val, index=[predicted_column])\n",
    "\n",
    "    # CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "    for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "        classified_file_rf = createClassifiedFile(loc_NAIPFile, rf, rf_args, overwrite=False)\n",
    "\n",
    "        # EXTRACT PREDICTED PIXEL CLASSIFICATION TO TESTING DATAFRAME\n",
    "        print(\"Extracting predicted classified values...\")\n",
    "        with rio.open(classified_file_rf) as classras:\n",
    "            # print(classras.indexes)\n",
    "            testing_data.loc[testing_data.NAIP_FILE == loc_NAIPFile, [predicted_column]] = \\\n",
    "                testing_data.loc[testing_data.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_class_value)\n",
    "\n",
    "    \"\"\"THIS CODE BLOCK USES A SHAPEFILE OF THE NAIP FOOTPRINTS AND THE AOI ECOREGIONS FEATURE\n",
    "    CLASS TO FIND ONLY THE NAMES OF THE QQUADS WHICH WE WAND TO CLASSIFY. THEN IDENTIFIES THE\n",
    "    ACTUAL PATH OF THE NAIP FILE AND PASSES IT TO THE CLASSIFIER\"\"\"\n",
    "\n",
    "    # THEN CREATE CLASSIFIED RASTER FROM ALL OTHER QUARTER QUADS\n",
    "    for root, dirs, files in os.walk(naip_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\"):\n",
    "                fpath = os.path.join(root, file)\n",
    "                # createClassifiedFile(fpath, rf, rf_args, overwrite=False)\n",
    "\n",
    "    logger.info(\"\\n\\t\\t-- DONE WITH ALL FILES --\\n\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # # TESTING SECTION\n",
    "\n",
    "\n",
    "    \"\"\"flat_pixels = np.array([1,10,50,500, np.NaN])\n",
    "\n",
    "    if np.isfinite(flat_pixels.any()) and not np.isnan(flat_pixels.any()):\n",
    "        flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "        #result = rf_classifier.predict(flat_pixels)\n",
    "        #np.where(x.values >= np.finfo(np.float32).max,)\n",
    "        print(\"MODING\")\n",
    "    else:\n",
    "        print(\"TURRIBLE\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utm11 = Proj(init=\"epsg:26911\")\n",
    "utm12 = Proj(init=\"epsg:26912\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "global qquad, naip_dir, loc_valleybottoms, VBET_VB_loc, \\\n",
    "    ndvi_dir, savi_dir, osavi_dir, msavi2_dir, evi2_dir, loc_classifiedQuarterQuads\n",
    "\n",
    "veg_assessment_area = 0.1 # in acres\n",
    "vaa_meters = veg_assessment_area * 4046.86\n",
    "vaa_radius = math.sqrt(vaa_meters/math.pi)\n",
    "vaa_diameter = vaa_radius*2\n",
    "\n",
    "# LOCATIONS OF FOLDERS HOLDING ALL INPUT RASTER DATA\n",
    "naip_data_dir = os.path.abspath(r\"Q:\\Arid Riparian Project\\Data\\NAIP_2015_Compressed\")\n",
    "\n",
    "data_dir = os.path.abspath(r\"M:\\Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#createClassification(data_dir, naip_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_datadir = data_dir\n",
    "naip_dir = naip_data_dir\n",
    "\n",
    "base_landsatdir = os.path.join(base_datadir, \"Landsat8\")\n",
    "\n",
    "training_data_dir = os.path.join(base_datadir, \"inital_model_inputs\")\n",
    "\n",
    "#ndvi_dir = utils.useDirectory(os.path.join(base_datadir, \"NDVI\"))\n",
    "#savi_dir = utils.useDirectory(os.path.join(base_datadir, \"SAVI\"))\n",
    "#osavi_dir = utils.useDirectory(os.path.join(base_datadir, \"OSAVI\"))\n",
    "#msavi2_dir = utils.useDirectory(os.path.join(base_datadir, \"MSAVI2\"))\n",
    "#evi2_dir = utils.useDirectory(os.path.join(base_datadir, \"EVI2\"))\n",
    "ndwi_qquad_dir = utils.useDirectory(os.path.join(base_datadir, \"NDWI\"))\n",
    "ndsi_qquad_dir = utils.useDirectory(os.path.join(base_datadir, \"NDSI\"))\n",
    "\n",
    "# std3px_dir = os.path.join(base_datadir, \"StdDev_3px\")\n",
    "# std5px_dir = os.path.join(base_datadir, \"StdDev_5px\")\n",
    "# std10px_dir = os.path.join(base_datadir, \"StdDev_10px\")\n",
    "\n",
    "base_landsatdir = utils.useDirectory(os.path.join(base_datadir, \"Landsat8\"))\n",
    "#landsat_qquad_dir = utils.useDirectory(os.path.join(base_landsatdir, \"byNAIPDOY_QQuads\"))\n",
    "valleybottoms_dir = utils.useDirectory(os.path.join(base_datadir, \"ValleyBottoms\"))\n",
    "loc_valleybottoms = utils.useDirectory(os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms\"))\n",
    "\n",
    "# LOCATION OF THE LANDSAT FILE\n",
    "landsat_file = os.path.os.path.join(base_landsatdir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")\n",
    "# LOCATION OF THE NDSI FILE\n",
    "ndsi_file = os.path.join(ndsi_qquad_dir, \"LandsatOLI_NDSI_30m.tif\")\n",
    "# LOCATION OF THE NDWI FILE\n",
    "ndwi_file = os.path.join(ndwi_qquad_dir, \"LandsatOLI_NDWI_30m.tif\")\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POLYGONS\n",
    "loc_class_polygons = os.path.join(training_data_dir, 'classificationTrainingPolygons.shp')\n",
    "\n",
    "# LOCATION OF USGS QUARTER QUADs in ARIZONA\n",
    "loc_usgs_qquads = os.path.join(training_data_dir, \"USGS_QQuads_AZ.shp\")\n",
    "\n",
    "# DIRECTORY HOLDING VRTS BY QUARTER QUAD FOR VEGETATION INDICIES (FLOAT32) AND STD_DEV (UINT8))\n",
    "qquad_vrt_dir = os.path.join(base_datadir, \"QQuad_VRTs\")\n",
    "\n",
    "day = datetime.today().strftime('%Y%m%d')\n",
    "global loc_classifiedQuarterQuads\n",
    "loc_classifiedQuarterQuads = utils.useDirectory(os.path.join(base_datadir, \"classifiedQuarterQuads_\" + day))\n",
    "print(\"loc_classifiedQuarterQuads: \", loc_classifiedQuarterQuads)\n",
    "\n",
    "loc_classifier = os.path.join(training_data_dir, \"RandomForestClassifier.joblib.pkl\")\n",
    "\n",
    "aoi = gpd.read_file(os.path.join(training_data_dir, \"Ecoregions_AOI.gpkg\"))\n",
    "\n",
    "valleybottoms_dir = utils.useDirectory(os.path.join(base_datadir, \"ValleyBottoms\"))\n",
    "loc_valleybottoms = utils.useDirectory(os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms\"))\n",
    "VBET_VB_loc = os.path.join(valleybottoms_dir, \"VBET_ValleyBottoms_20180624.tif\")\n",
    "\n",
    "# DEFINE THE PROJECTION USED OVER ARIZONA. USED FOR TRANSLATING POINT GEOMETRY LATER ON\n",
    "utm11 = Proj(init=\"epsg:26911\")\n",
    "utm12 = Proj(init=\"epsg:26912\")\n",
    "\n",
    "\"\"\"\n",
    "# IDENTIFY RASTER VARIABLES\n",
    "# THESE ORDER OF THESE RASTER VARIABLES MUST COINCIDE WITH THE CONSTRUCTED ARRAY OF EXTRACTS in the get_values FUNCTION\n",
    "naip = [\"NAIP1\", \"NAIP2\", \"NAIP3\", \"NAIP4\"]\n",
    "landsat = [\"Landsat1\", \"Landsat2\", \"Landsat3\", \"Landsat4\", \"Landsat5\", \"Landsat6\", \"Landsat7\", \"Landsat8\"]\n",
    "landsat_vis = [\"NDSI\", \"NDWI\"]\n",
    "naip_vis = [\"NDVI\", \"EVI2\", \"SAVI\", \"OSAVI\", 'MSAVI2']\n",
    "\n",
    "textures = [\"StdDev_3px_band1\", \"StdDev_3px_band2\", \"StdDev_3px_band3\", \"StdDev_3px_band4\",\n",
    "            \"StdDev_5px_band1\", \"StdDev_5px_band2\", \"StdDev_5px_band3\", \"StdDev_5px_band4\",\n",
    "            \"StdDev_10px_band1\", \"StdDev_10px_band2\", \"StdDev_10px_band3\", \"StdDev_10px_band4\"]\n",
    "filters = [\"Gauss1_band1\", \"Gauss1_band2\", \"Gauss1_band3\", \"Gauss1_band4\",]\n",
    "#rasters_names = naip + naip_vis + filters + landsat + landsat_vis\n",
    "#print(rasters_names)\n",
    "\"\"\"\n",
    "\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
    "\n",
    "loc_out_join = Create_Classification_Points.joinProjectionAndNAIP(loc_class_polygons, naip_dir, loc_usgs_qquads,\n",
    "                                                                  overwrite=False)\n",
    "\n",
    "loc_points_in_poly = Create_Classification_Points.createShapefilePoints(loc_out_join, overwrite=False)\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POINTS POST EXTRACT\n",
    "loc_points_wRaster_extracts = loc_points_in_poly[:-4] + \"_extracts.shp\"\n",
    "\n",
    "# EXTRACT RASTER VALUES TO POINTS\n",
    "training_info = extractToPoints(loc_points_in_poly, loc_points_wRaster_extracts, base_datadir, base_landsatdir, overwrite=False)\n",
    "\n",
    "class_points = training_info[\"training_points\"]\n",
    "band_names = training_info[\"band_names\"]\n",
    "logger.debug(\"Available raster variables: \\n\\t%s\" % band_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the points data frame into train and test\n",
    "train, test = train_test_split(class_points, test_size=0.3)\n",
    "\n",
    "rf_rasters = band_names[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gridsearch_forest = RandomForestClassifier()\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [40, 100, 300],\n",
    "    \"max_depth\": [80,150,300],\n",
    "    \"min_samples_leaf\" : [20,50,200]\n",
    "}\n",
    "params = {\n",
    "    \"n_estimators\": [40, 100]}\n",
    "clf = GridSearchCV(gridsearch_forest, param_grid=params)\n",
    "\n",
    "clf.fit(train[rf_rasters].dropna(),\n",
    "       train[rf_rasters+[\"Class\"]].dropna()[\"Class\"])\n",
    "\n",
    "clf.best_params_\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_dir = r\"M:\\Data\\inital_model_inputs\"\n",
    "\n",
    "loc_usgs_qquads = os.path.join(training_data_dir, \"USGS_QQuads_AZ.shp\")\n",
    "aoi = gpd.read_file(os.path.join(training_data_dir, \"PinalCounty_AOI.gpkg\"))\n",
    "\n",
    "#aoi.crs = fiona.crs.from_epsg(2163)\n",
    "#print(aoi.crs)\n",
    "\n",
    "footprints = gpd.read_file(loc_usgs_qquads)\n",
    "aoi.to_crs(footprints.crs, inplace=True)\n",
    "\n",
    "aoi_qquads = []\n",
    "for j, arow in aoi.iterrows():\n",
    "    for i, row in footprints.iterrows():\n",
    "        if row.geometry.intersects(arow.geometry):\n",
    "            fpath = getFullNAIPPath(row.QUADID, naip_dir)\n",
    "            aoi_qquads.append(fpath)\n",
    "print(len(aoi_qquads))\n",
    "#print(\"AOI QQUADS\\n{}\\n\".format(aoi_qquads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for quad in aoi_qquads:\n",
    "    print(\"Copying %s\" % quad)\n",
    "    shutil.copy (quad, r\"D:\\Data\\NAIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "\n",
    "# Split the points data frame into train and test\n",
    "training_data, testing_data = train_test_split(class_points, test_size=0.3)\n",
    "\n",
    "# INITIALIZE COLUMN FOR PREDICTED CLASSIFICATION VALUES\n",
    "predicted_column = \"CLASS_PREDICT\"\n",
    "testing_data[predicted_column] = \"Null\"\n",
    "\n",
    "# remove_landsat = True\n",
    "\n",
    "# rasters values used in random forest\n",
    "# Allows removal of some rasters\n",
    "temp_rasters = band_names[:]\n",
    "rf_rasters = band_names[:]\n",
    "print(\"rf_raster: \", rf_rasters)\n",
    "\n",
    "\"\"\"\n",
    "if remove_landsat:\n",
    "    for r in temp_rasters:\n",
    "        if \"Landsat\" in r:\n",
    "            rf_rasters.remove(r)\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Using raster variables: \\n%s\" % rf_rasters)\n",
    "\n",
    "rf_args = {\"maxdepth\": 400,\n",
    "           \"n_est\": 40,\n",
    "           \"n_job\": 1,\n",
    "           \"min_per_leaf\": 50,\n",
    "           \"crit\": \"entropy\"}  # gini or entropy}\n",
    "\n",
    "# print('Out-of-bag score estimate:', {rf.oob_score_:.3})\n",
    "rf = getClassifier(loc_classifier, training_data, rf_rasters, rf_args, createClassifier=True)\n",
    "\n",
    "irods_sess, irods_files = utils.getFilesonDE()\n",
    "\n",
    "\n",
    "print(\"------------------- STARTING CREATION FOR TRAINING QUADS -----------------------------\")\n",
    "files_list = []\n",
    "for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "    files_list.append(loc_NAIPFile)\n",
    "\n",
    "Parallel(n_jobs=2)(delayed(createClassifiedFile)(naip_file, rf, rf_args) for naip_file in files_list)\n",
    "#    classified_file_rf = createClassifiedFile(loc_NAIPFile, rf, rf_args, overwrite=False)\n",
    "\n",
    "print(\"-------------------STARTING CREATION FOR SUB AREA-----------------------------\")\n",
    "single_comp_subset = []\n",
    "with open(os.path.join(base_datadir, \"NaipDone1.txt\")) as txt:\n",
    "    for l in txt:\n",
    "        fullname = getFullNAIPPath(l[:-1], naip_dir)\n",
    "        # print(l)\n",
    "        single_comp_subset.append(fullname)\n",
    "        #if l[:-1] not in irods_files.keys():\n",
    "        #    single_comp_subset.append(fullname)\n",
    "\n",
    "        #createClassifiedFile(fullname, rf, rf_args)\n",
    "\n",
    "Parallel(n_jobs=2)(delayed(createClassifiedFile)(naip_file, rf, rf_args) for naip_file in single_comp_subset)\n",
    "# for qquad_name in single_comp_subset:\n",
    "# fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "\n",
    "# createClassifiedFile(fpath, \"RF\", rf, rf_args, overwrite=False)\n",
    "\n",
    "print(\"-------------------FINISHED CREATION FOR SUB AREA-----------------------------\")\n",
    "\n",
    "print(\"-------------------STARTING CREATION FOR AOI-----------------------------\")\n",
    "aoi.crs = fiona.crs.from_epsg(2163)\n",
    "print(aoi.crs)\n",
    "\n",
    "footprints = gpd.read_file(loc_usgs_qquads)\n",
    "aoi.to_crs(footprints.crs, inplace=True)\n",
    "\n",
    "aoi_qquads = []\n",
    "for i, row in footprints.iterrows():\n",
    "    for j, arow in aoi.iterrows():\n",
    "        if row.geometry.within(arow.geometry):\n",
    "            aoi_qquads.append(row.Name)\n",
    "print(len(aoi_qquads))\n",
    "print(\"AOI QQUADS\\n{}\\n\".format(aoi_qquads))\n",
    "\n",
    "for qquad_name in aoi_qquads:\n",
    "    fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "\n",
    "    beg = datetime.now()\n",
    "    logger.debug(\"Starting on qquad: %s\" % qquad_name)\n",
    "    createClassifiedFile(fpath, rf, rf_args, overwrite=False)\n",
    "    logger.debug(\"COMPLETED riparian classification. Finished in %s\" % (str(datetime.now() - beg)))\n",
    "    logger.debug(\"_____________________________________________________________________________________\")\n",
    "\n",
    "print(\"-------------------FINISHED CREATING FOR AOI-----------------------------\")\n",
    "\n",
    "exit()\n",
    "\n",
    "def get_class_value(geom):\n",
    "    \"\"\" TAKES A VARIABLE OF GEOMETRY TYPE AND RETURNS THE VALUE AT X,Y\n",
    "    AS A PANDAS SERIES FOR FOR LOCAL RASTER 'CLASSRAS' \"\"\"\n",
    "    # print(row)\n",
    "    # geom = row.geometry\n",
    "    x = geom.centroid.x\n",
    "    y = geom.centroid.y\n",
    "    for val in classras.sample([(x, y)]):\n",
    "        # print(np.ndarray.tolist(val))\n",
    "        return pd.Series(val, index=[predicted_column])\n",
    "\n",
    "# CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "    classified_file_rf = createClassifiedFile(loc_NAIPFile, rf, rf_args, overwrite=False)\n",
    "\n",
    "    # EXTRACT PREDICTED PIXEL CLASSIFICATION TO TESTING DATAFRAME\n",
    "    print(\"Extracting predicted classified values...\")\n",
    "    with rio.open(classified_file_rf) as classras:\n",
    "        # print(classras.indexes)\n",
    "        testing_data.loc[testing_data.NAIP_FILE == loc_NAIPFile, [predicted_column]] = \\\n",
    "            testing_data.loc[testing_data.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_class_value)\n",
    "\n",
    "\"\"\"THIS CODE BLOCK USES A SHAPEFILE OF THE NAIP FOOTPRINTS AND THE AOI ECOREGIONS FEATURE\n",
    "CLASS TO FIND ONLY THE NAMES OF THE QQUADS WHICH WE WAND TO CLASSIFY. THEN IDENTIFIES THE\n",
    "ACTUAL PATH OF THE NAIP FILE AND PASSES IT TO THE CLASSIFIER\"\"\"\n",
    "\n",
    "# THEN CREATE CLASSIFIED RASTER FROM ALL OTHER QUARTER QUADS\n",
    "for root, dirs, files in os.walk(naip_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\"):\n",
    "            fpath = os.path.join(root, file)\n",
    "            # createClassifiedFile(fpath, rf, rf_args, overwrite=False)\n",
    "\n",
    "logger.info(\"\\n\\t\\t-- DONE WITH ALL FILES --\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with rio.open(r\"M:\\Data\\TrainingImageStack\\m_3511043_ne_12_1_20150607_TrainingStack.tif\") as tras:\n",
    "    print(tras.shape)\n",
    "    bands_data = tras.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bands_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ras_array = np.moveaxis(bands_data, 0, -1) \n",
    "print(ras_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with rio.open(r\"Q:\\Arid Riparian Project\\Data\\NAIP_2015_Compressed\\m_3511043_ne_12_1_20150607.tif\") as tras:\n",
    "    print(tras.read().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_df = gpd.read_file(loc_points_wRaster_extracts)\n",
    "band_names = training_data_df.columns.tolist()[4:-1]\n",
    "band_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "veg_assessment_area = 0.1 # in acres\n",
    "vaa_meters = veg_assessment_area * 4046.86\n",
    "vaa_radius = math.sqrt(vaa_meters/math.pi)\n",
    "vaa_diameter = vaa_radius*2\n",
    "\n",
    "# LOCATIONS OF FOLDERS HOLDING ALL INPUT RASTER DATA\n",
    "naip_dir = os.path.abspath(r\"Q:\\Arid Riparian Project\\Data\\NAIP_2015_Compressed\")\n",
    "\n",
    "base_datadir = os.path.abspath(r\"M:\\Data\")\n",
    "\n",
    "base_landsatdir = os.path.join(base_datadir, \"Landsat8\")\n",
    "\n",
    "training_data_dir = os.path.join(base_datadir, \"inital_model_inputs\")\n",
    "\n",
    "ndvi_dir = os.path.join(base_datadir, \"NDVI\")\n",
    "savi_dir = os.path.join(base_datadir, \"SAVI\")\n",
    "osavi_dir = os.path.join(base_datadir, \"OSAVI\")\n",
    "msavi2_dir = os.path.join(base_datadir, \"MSAVI2\")\n",
    "evi2_dir = os.path.join(base_datadir, \"EVI2\")\n",
    "ndwi_qquad_dir = os.path.join(base_datadir, \"NDWI\")\n",
    "ndsi_qquad_dir = os.path.join(base_datadir, \"NDSI\")\n",
    "\n",
    "std3px_dir = os.path.join(base_datadir, \"StdDev_3px\")\n",
    "std5px_dir = os.path.join(base_datadir, \"StdDev_5px\")\n",
    "std10px_dir = os.path.join(base_datadir, \"StdDev_10px\")\n",
    "\n",
    "base_landsatdir = os.path.join(base_datadir, \"Landsat8\")\n",
    "landsat_qquad_dir = os.path.join(base_landsatdir, \"byNAIPDOY_QQuads\")\n",
    "\n",
    "\n",
    "landsat_file = os.path.os.path.join(base_landsatdir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")\n",
    "\n",
    "# LOCATION OF THE NDSI FILE\n",
    "ndsi_file = os.path.join(ndsi_qquad_dir, \"LandsatOLI_NDSI_30m.tif\")\n",
    "\n",
    "# LOCATION OF THE NDWI FILE\n",
    "ndwi_file = os.path.join(ndwi_qquad_dir, \"LandsatOLI_NDWI_30m.tif\")\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POLYGONS\n",
    "loc_class_polygons = os.path.join(training_data_dir, 'classificationTrainingPolygons.shp')\n",
    "\n",
    "# LOCATION OF USGS QUARTER QUADs in ARIZONA\n",
    "loc_usgs_qquads = os.path.join(training_data_dir, \"USGS_QQuads_AZ.shp\")\n",
    "\n",
    "# DIRECTORY HOLDING VRTS BY QUARTER QUAD FOR VEGETATION INDICIES (FLOAT32) AND STD_DEV (UINT8))\n",
    "qquad_vrt_dir = os.path.join(base_datadir, \"QQuad_VRTs\")\n",
    "\n",
    "day = datetime.today().strftime('%Y%m%d')\n",
    "loc_classifiedQuarterQuads = os.path.join(base_datadir, \"classifiedQuarterQuads_\" + day)\n",
    "utils.useDirectory(loc_classifiedQuarterQuads)\n",
    "\n",
    "aoi = gpd.GeoDataFrame.from_file(r\"Q:\\Arid Riparian Project\\AridRiparianProject\\AridRiparianProject.gdb\", layer='TargetEcoregions')\n",
    "\n",
    "nhd_dir = os.path.join(base_datadir, \"ValleyBottoms\")\n",
    "loc_valleybottoms = os.path.join(nhd_dir, \"VBET_ValleyBottoms\")\n",
    "utilities.useDirectory(loc_valleybottoms)\n",
    "VBET_VB_loc = os.path.abspath(r\"M:\\Data\\ValleyBottoms\\VBET_ValleyBottoms_20180624.tif\")\n",
    "\n",
    "# DEFINE THE PROJECTION USED OVER ARIZONA. USED FOR TRANSLATING POINT GEOMETRY LATER ON\n",
    "utm11 = Proj(init=\"epsg:26911\")\n",
    "utm12 = Proj(init=\"epsg:26912\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# IDENTIFY RASTER VARIABLES\n",
    "# THESE ORDER OF THESE RASTER VARIABLES MUST COINCIDE WITH THE CONSTRUCTED ARRAY OF EXTRACTS in the get_values FUNCTION\n",
    "naip = [\"NAIP1\", \"NAIP2\", \"NAIP3\", \"NAIP4\"]\n",
    "landsat = [\"Landsat1\", \"Landsat2\", \"Landsat3\", \"Landsat4\", \"Landsat5\", \"Landsat6\", \"Landsat7\", \"Landsat8\"]\n",
    "landsat_vis = [\"NDSI\", \"NDWI\"]\n",
    "naip_vis = [\"NDVI\", \"EVI2\", \"SAVI\", \"OSAVI\", 'MSAVI2']\n",
    "# NOT using texture in this iteration\n",
    "textures = [\"StdDev_3px_band1\", \"StdDev_3px_band2\", \"StdDev_3px_band3\", \"StdDev_3px_band4\",\n",
    "            \"StdDev_5px_band1\", \"StdDev_5px_band2\", \"StdDev_5px_band3\", \"StdDev_5px_band4\",\n",
    "            \"StdDev_10px_band1\", \"StdDev_10px_band2\", \"StdDev_10px_band3\", \"StdDev_10px_band4\"]\n",
    "filters = [\"Gauss1_band1\", \"Gauss1_band2\", \"Gauss1_band3\", \"Gauss1_band4\",]\n",
    "            \n",
    "#rasters_names = naip + naip_vis + filters + landsat + landsat_vis\n",
    "#print(rasters_names)\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "loc_out_join = Create_Classification_Points.joinProjectionAndNAIP(loc_class_polygons, naip_dir, loc_usgs_qquads, overwrite=False)\n",
    "\n",
    "loc_points_in_poly = Create_Classification_Points.createShapefilePoints(loc_out_join, overwrite=False)\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POINTS POST EXTRACT\n",
    "loc_points_wRaster_extracts = loc_points_in_poly[:-4] + \"_extracts.shp\"\n",
    "\n",
    "# EXTRACT RASTER VALUES TO POINTS\n",
    "training_info = extractToPoints(loc_points_in_poly, loc_points_wRaster_extracts, base_datadir, base_landsatdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_points = training_info[\"training_points\"]\n",
    "band_names = training_info[\"band_names\"]\n",
    "logger.debug(\"Available raster variables: \\n\\t%s\" % band_names)\n",
    "print(\"Available raster variables: \\n\\t%s\" % band_names)\n",
    "\n",
    "class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "\n",
    "# Split the points data frame into train and test\n",
    "train, test = train_test_split(class_points, test_size=0.3)\n",
    "\n",
    "# INITIALIZE COLUMN FOR PREDICTED CLASSIFICATION VALUES\n",
    "predicted_column = \"CLASS_PREDICT\"\n",
    "test[predicted_column] = \"Null\"\n",
    "\n",
    "#remove_landsat = True\n",
    "\n",
    "#rasters values used in random forest\n",
    "#Allows removal of some rasters\n",
    "temp_rasters = band_names[:]\n",
    "rf_rasters = band_names[:]\n",
    "\n",
    "\"\"\"\n",
    "if remove_landsat:\n",
    "    for r in temp_rasters:\n",
    "        if \"Landsat\" in r:\n",
    "            rf_rasters.remove(r)\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Using raster variables: \\n%s\" % rf_rasters)\n",
    "\n",
    "# TRAIN RANDOM FORESTS\n",
    "rf_start = datetime.now()\n",
    "logger.info(\"Beginning Random Forest Train\")\n",
    "maxdepth = 400\n",
    "#maxdepth = 60\n",
    "n_est = 40\n",
    "n_job = 1\n",
    "min_per_leaf = 50\n",
    "crit = \"entropy\"  # gini or entropy\n",
    "\n",
    "rf = RandomForestClassifier(verbose=1, max_depth=maxdepth, n_estimators=n_est,\n",
    "                            n_jobs=n_job, min_samples_leaf=min_per_leaf,\n",
    "                            criterion=crit)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "X = Imputer().fit_transform(train[rf_rasters].dropna())\n",
    "\n",
    "rf.fit(train[rf_rasters].dropna(),\n",
    "       train[rf_rasters+[\"Class\"]].dropna()[\"Class\"])\n",
    "\n",
    "\n",
    "logger.info(\"Finished Fitting in\", datetime.now() - rf_start)\n",
    "#print('Out-of-bag score estimate:', {rf.oob_score_:.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"-------------------STARTING CREATION FOR SUB AREA-----------------------------\")\n",
    "single_comp_subset = []\n",
    "with open(os.path.join(base_datadir,\"NaipDone1.txt\")) as txt:\n",
    "    for l in txt:\n",
    "        #print(l)\n",
    "        fullname = getFullNAIPPath(l[:-1], naip_dir)\n",
    "        single_comp_subset.append(fullname)\n",
    "\n",
    "#Parallel(n_jobs=1)(delayed(createClassifiedFile)(naip_file, rf) for naip_file in single_comp_subset)\n",
    "for qquad_file in single_comp_subset:\n",
    "    qquad = os.path.basename(qquad_file)\n",
    "    #fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "    output_fname = \"RF\" + \"_D\" + str(maxdepth) + \"E\" + str(n_est) + \"MPL\" + str(min_per_leaf) + \"_\" + qquad # + \".tif\"\n",
    "    createClassifiedFile(qquad_file, output_fname, rf, overwrite=False)\n",
    "\n",
    "print(\"-------------------FINISHED CREATION FOR SUB AREA-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "# TESTING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createClassifiedFile(loc_NAIPFile, output_fname, rf_classifier, overwrite=False):\n",
    "    beg = datetime.now()\n",
    "    logger.debug(\"Starting on qquad: %s\" % loc_NAIPFile)\n",
    "\n",
    "    file = os.path.basename(loc_NAIPFile)\n",
    "    \n",
    "    qquad = getQQuadFromNAIP(file)\n",
    "    \n",
    "    #if mltype == \"RF\":\n",
    "    #\n",
    "    #else:\n",
    "    #    print(\"Unknown classifier type '{}' specified. Exiting...\")\n",
    "    #    raise(ValueError)\n",
    "        \n",
    "    loc_classified_file = os.path.join(loc_classifiedQuarterQuads, output_fname)\n",
    "    print(\"loc_classified_file; \", loc_classified_file)\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        cl_start = datetime.now()\n",
    "        logging.info(\"\\tClassifying landcover file at %s...\" % (loc_classified_file))\n",
    "        # loc_NAIPFile = os.path.join(root, file)\n",
    "\n",
    "        \"\"\"\n",
    "        vrt_naipvis = get_VegIndicies_VRT(loc_NAIPFile, qquad_vrt_dir) #  All float32\n",
    "        #vrt_stddev = get_STDDev_VRT(file)  # All 8 bit (\"byte\")\n",
    "        gaussf_path = get_GaussianFile(loc_NAIPFile)\n",
    "\n",
    "        # BEN!!!!!! YOU REMOVED LANDSAT VARIABLE FOR QUICK RUN\n",
    "        landsat_path = createSubSetLandsat(loc_NAIPFile, landsat_file, landsat_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        landsat_ndsi_path = createSubSetLandsat(loc_NAIPFile, ndsi_file, ndsi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "        landsat_ndwi_path = createSubSetLandsat(loc_NAIPFile, ndwi_file, ndwi_qquad_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        bands_data = []\n",
    "        #for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "        for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "            print(\"\\tIN RAS: {}\".format(inras))\n",
    "            try:\n",
    "                raster_dataset = gdal.Open(inras, gdal.GA_ReadOnly)\n",
    "            except RuntimeError as e:\n",
    "                report_and_exit(str(e))\n",
    "\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "\n",
    "            for b in range(1, raster_dataset.RasterCount + 1):\n",
    "                band = raster_dataset.GetRasterBand(b)\n",
    "                bands_data.append(band.ReadAsArray())\n",
    "        \"\"\"\n",
    "        # GET PROJECTION INFO FOR GDAL WRITE\n",
    "        try:\n",
    "            raster_dataset = gdal.Open(loc_NAIPFile, gdal.GA_ReadOnly)\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "        except RuntimeError as e:\n",
    "            report_and_exit(str(e))\n",
    "\n",
    "\n",
    "        training_raster = generateStack(loc_NAIPFile)\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        #bands_data = np.dstack(bands_data)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        print(\"training_raster\", training_raster)\n",
    "        with rio.open(training_raster) as tras:\n",
    "            bands_data = tras.read()\n",
    "            \n",
    "\n",
    "        print(bands_data.shape)  # (22, 7720, 6550)\n",
    "        n_bands, rows, cols, = bands_data.shape\n",
    "        n_samples = rows * cols\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "        \n",
    "        classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
    "\n",
    "        # A list of colors for each class\n",
    "        COLORS = [\n",
    "            \"#000000\",  # 0 EMPTY\n",
    "            \"#00af11\",  # 1 - Vegetation - Thick\n",
    "            \"#00e513\",  # 2 - Vegetation - Thin\n",
    "            \"#e9ff5a\",  # 3 - Herbaceous\n",
    "            \"#f1ac34\",  # 4 - Barren - Light\n",
    "            \"#a9852e\",  # 5 - Barren - Dark\n",
    "            \"#2759ff\",  # 6 - Water\n",
    "            \"#efefef\",  # 7 - Roof - White\n",
    "            \"#d65133\",  # 8 - Roof - Red\n",
    "            \"#cecece\",  # 9 - Roof - Grey\n",
    "            \"#a0a0a0\",  # 10 - Impervious - Light\n",
    "            \"#555555\",  # 11 - Impervious - Dark\n",
    "            \"#000000\",  # 12 - Shadows\n",
    "            \"#00734C\"   # 13 - Vegetation - Irrigated\n",
    "        ]\n",
    "\n",
    "        print(\"Classifing...\")\n",
    "        \"\"\"if not np.all(np.isfinite(flat_pixels)):\n",
    "            print(\"Not all value finite. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "        if np.any(np.isnan(flat_pixels)):\n",
    "            print(\"Some values are NaN. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels == np.NaN, np.finfo(np.float32).max, flat_pixels)\n",
    "        \"\"\"\n",
    "        #try:\n",
    "        result = rf_classifier.predict(flat_pixels)\n",
    "        print(\"Classified\")\n",
    "        # Reshape the result: split the labeled pixels into rows to create an image\n",
    "        classification = result.reshape((rows, cols))\n",
    "        print(\"Reshaped\")\n",
    "        # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "        # TODO - Rewrite this to use rasterio for consistency\n",
    "        write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "        logging.info(\"\\tCreated classified file in %s\" % (str(datetime.now() - cl_start)))\n",
    "        #except (ValueError) as e:\n",
    "        #logging.info(\"-----------BAD VALUES FOR PREDICTORS. SKIPPING FILE %s\\n%s\" % (file, str(e)))\n",
    "        #return None\n",
    "        \n",
    "        del bands_data\n",
    "        del flat_pixels\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"LandCover file %s exists and no overwrite.\" % loc_classified_file)\n",
    "\n",
    "    \"\"\"\n",
    "    o_veg_loc = r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\RiparianClass_VBs\"\n",
    "    utilities.useDirectory(o_veg_loc)\n",
    "        \n",
    "    quadrant_loc = os.path.join(o_veg_loc, qquad[:5])\n",
    "    utilities.useDirectory(quadrant_loc)\n",
    "    \n",
    "    # not writing out density calculations\n",
    "    #dq_path = os.path.join(degress_quadrant_loc, denseVeg_file)\n",
    "    #sq_path = os.path.join(degress_quadrant_loc, sparseVeg_file)\n",
    "    \n",
    "    riparian_class_qquad = os.path.join(quadrant_loc, \"RiparianClassification_\" + qquad + \".tif\")\n",
    "\n",
    "    if not os.path.exists(riparian_class_qquad) or overwrite:\n",
    "        createRiparianClass(loc_classified_file, riparian_class_qquad, qquad)\n",
    "\n",
    "    shutil.copy(riparian_class_qquad, r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\Pinal_AOI\")\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"COMPLETED riparian classification. Finished in %s\" % (str(datetime.now() - beg)))\n",
    "    logger.debug(\"_____________________________________________________________________________________\")\n",
    "\n",
    "    return loc_classified_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"flat_pixels = np.array([1,10,50,500, np.NaN])\n",
    "\n",
    "if np.isfinite(flat_pixels.any()) and not np.isnan(flat_pixels.any()):\n",
    "    flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "    #result = rf_classifier.predict(flat_pixels)\n",
    "    #np.where(x.values >= np.finfo(np.float32).max,)\n",
    "    print(\"MODING\")\n",
    "else:\n",
    "    print(\"TURRIBLE\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting projection information for M:/Data/ValleyBottoms/Watersheds/1505/HRNHDPlusRasters1505/elev_cm.tif\n"
     ]
    }
   ],
   "source": [
    "# TEMP\n",
    "geodatabase = r\"M:\\Data\\ValleyBottoms\\Watersheds\\1505\\NHDPlus_H_1505_GDB.gdb\"\n",
    "flowlines = gpd.GeoDataFrame.from_file(geodatabase, layer='NHDFlowline')\n",
    "dem_ras = r\"M:/Data/ValleyBottoms/Watersheds/1505/HRNHDPlusRasters1505/elev_cm.tif\"\n",
    "f_crs = flowlines.crs\n",
    "raster_crs = getRasterProj4(dem_ras)\n",
    "#print(\"Reprojecting flowlines dataframe to FAC raster projection...\")\n",
    "\n",
    "# GET VAA TABLE AND JOIN TO FEATURE CLASS FOR WATERSHED SIZE\n",
    "flowlines_vaa = gpd.GeoDataFrame.from_file(geodatabase, layer='NHDPlusFlowlineVAA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flowlines_merge = flowlines.merge(flowlines_vaa, on='NHDPlusID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flowlines_merge['geometry'] = flowlines_merge['geometry_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flowlines_merge = gpd.GeoDataFrame(flowlines_merge, crs=f_crs, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:4269'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (LINESTRING Z (-110.5056962722467 32.215984083...\n",
       "1    (LINESTRING Z (-110.2014410060523 32.306746683...\n",
       "2    (LINESTRING Z (-111.8079084035587 32.192767416...\n",
       "3    (LINESTRING Z (-111.5773974705832 31.730285017...\n",
       "4    (LINESTRING Z (-110.3114108725483 32.950074815...\n",
       "Name: geometry, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flowlines_merge.to_crs(raster_crs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (LINESTRING Z (-1355200.740856612 1119479.6717...\n",
       "1    (LINESTRING Z (-1325483.505384038 1125190.5103...\n",
       "2    (LINESTRING Z (-1476211.607364795 1136324.0812...\n",
       "3    (LINESTRING Z (-1463288.093667586 1081934.5893...\n",
       "4    (LINESTRING Z (-1324905.082208617 1197672.6546...\n",
       "Name: geometry, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowlines_merge[\"geometry\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watershedsize_col = \"TotDASqKm\"\n",
    "drop_columns = flowlines_merge.columns.tolist()\n",
    "drop_columns.remove('geometry')\n",
    "drop_columns.remove(watershedsize_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flowlines_merge.drop(drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get resolution\n",
    "with rio.open(dem_ras) as ras:\n",
    "    res_x, res_y = ras.res\n",
    "\n",
    "# pixel X number in km\n",
    "ratio_x = 1000 / res_x\n",
    "ratio_y = 1000 / res_y\n",
    "\n",
    "flowlines_merge[\"TotDASq_m\"] = flowlines_merge[watershedsize_col] * ratio_x * ratio_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRasterProj4(raster):\n",
    "    print(\"Getting projection information for %s\" % raster)\n",
    "    \"\"\" Function returns the projection of the input raster in proj4\"\"\"\n",
    "    fac = gdal.Open(raster)\n",
    "\n",
    "    ras_proj = fac.GetProjection()\n",
    "    print(ras_proj)\n",
    "    spatialRef = osr.SpatialReference()\n",
    "\n",
    "    osr.UseExceptions()\n",
    "    # Apparently osr has difficulties identifying albers projections\n",
    "    prjText = ras_proj.replace('\"Albers\"', '\"Albers_Conic_Equal_Area\"')\n",
    "    spatialRef.ImportFromWkt(prjText)\n",
    "    ras_proj_proj4 = spatialRef.ExportToProj4()\n",
    "    return ras_proj_proj4\n",
    "\n",
    "def calculateBufferSize(da):\n",
    "    if da <= 1:\n",
    "        buff_size = 1\n",
    "    else:\n",
    "        buff_size = math.sqrt(da) / (math.log(da, 10) * (4 / 3))\n",
    "\n",
    "    return buff_size\n",
    "\n",
    "def bufferLines(row):\n",
    "    geom = row.geometry\n",
    "    buffersize = row.BufferSize\n",
    "    # fac = row[watershedsize_col]\n",
    "\n",
    "    # log of 1 is 0, can't divide by zero. Also, a Flow accumulation value of 1 or zero is a misread, essentially minimum buffer size\n",
    "    # if fac <= 1:\n",
    "    #    fac = 2\n",
    "\n",
    "    \"\"\"Simple equation which correlates the flow accumulation values (fac), e.g. watershed size,\n",
    "    to the appropriate valley bottom buffer\"\"\"\n",
    "    # buffersize = math.sqrt(fac) / (math.log(fac, 10) * (4 / 3))\n",
    "\n",
    "    return geom.buffer(buffersize)\n",
    "\n",
    "\n",
    "def getResAndExtent(raster_file):\n",
    "    \"\"\" RETURN THE RESOLUTION AND EXTENT OF THE RASTER AS A LIST [resx, resy, xmin, ymin, xmax, ymax]\"\"\"\n",
    "    with rio.open(raster_file) as ras:\n",
    "        ymax = ras.profile['transform'][5]\n",
    "        xmin = ras.profile['transform'][2]\n",
    "        height = ras.profile['height']\n",
    "        width = ras.profile['width']\n",
    "        resx = ras.profile['transform'][0]\n",
    "        resy = ras.profile['transform'][4]\n",
    "        ymin = ymax + (height * resy)\n",
    "        xmax = xmin + (width * resx)\n",
    "\n",
    "        return [abs(resx), abs(resy), str(xmin), str(ymin), str(xmax), str(ymax)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROJCS[\"USA_Contiguous_Albers_Equal_Area_Conic_USGS_version\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers\"],PARAMETER[\"false_easting\",0.0],PARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-96.0],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_origin\",23.0],UNIT[\"Meter\",1.0],VERTCS[\"NAVD_1988\",VDATUM[\"North_American_Vertical_Datum_1988\"],PARAMETER[\"Vertical_Shift\",0.0],PARAMETER[\"Direction\",1.0],UNIT[\"Centimeter\",0.01]]]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raster_wkt = gdal.Open(dem_ras).GetProjection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flowlines_merge[\"BufferSize\"] = flowlines_merge[\"TotDASq_m\"].apply(calculateBufferSize)\n",
    "\n",
    "# Buffer each flowline by its watershed size\n",
    "flowlines_merge[\"geometry\"] = flowlines_merge.apply(bufferLines, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flowlines_vector = os.path.join(os.path.abspath(r\"M:/Data/ValleyBottoms/Watersheds/1505\"), \"NHD_Flowlines_buffered.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 118615 entries, 0 to 118614\n",
      "Data columns (total 4 columns):\n",
      "TotDASqKm     118615 non-null float64\n",
      "geometry      118615 non-null object\n",
      "TotDASq_m     118615 non-null float64\n",
      "BufferSize    118615 non-null float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "flowlines_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#flowlines_merge.to_file(flowlines_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outbuffer_ras = os.path.join(r\"M:\\Data\\ValleyBottoms\\Watersheds\\1505\\HRNHDPlusRasters1505\\Predictors\", \"WatershedSize_Buffer.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with rio.open(dem_ras) as ras:\n",
    "    #copy and update the metadata from the input raster for the output\n",
    "    meta = ras.meta.copy()\n",
    "    #meta.update(compress='lzw')\n",
    "    shape = ras.shape\n",
    "    inras_crs = ras.crs.from_string(raster_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRS({'lat_0': 23, 'ellps': 'GRS80', 'y_0': 0, 'lat_2': 45.5, 'units': 'm', 'x_0': 0, 'no_defs': True, 'lon_0': -96, 'lat_1': 29.5, 'proj': 'aea'})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inras_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta.update(\n",
    "    dtype=np.float32,\n",
    "    nodata=-9999,\n",
    "    crs=inras_crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_arr = np.full(shape, -9999.0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "feat_shapes = ((geom,value) for geom, value in zip(flowlines_merge.geometry, flowlines_merge.BufferSize))\n",
    "burned_array = rio.features.rasterize(shapes=feat_shapes, fill=-9999, out=out_arr, transform=meta['transform'], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "with rio.open(outbuffer_ras, 'w', **meta) as out_ras:\n",
    "    #out_ras.crs.from_wkt(raster_wkt)\n",
    "    out_ras.write_band(1, burned_array.astype(np.float32))\n",
    "    \n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writedToGPKG(filename, df):\n",
    "    \"\"\" Geopandas current implimentation is very slow to write data frame to file due to file, locking/unlock for\n",
    "    each feature. Get around this with fionas buffer.\n",
    "\n",
    "    This code pulled from https://github.com/geopandas/geopandas/issues/557\"\"\"\n",
    "\n",
    "    g = df.columns.to_series().groupby(df.dtypes).groups\n",
    "    properties = {}\n",
    "    for k, v in g.items():\n",
    "        for i in v:\n",
    "            if i != 'geometry':\n",
    "                # print(i)\n",
    "                properties[i] = \"float\" #k.name\n",
    "\n",
    "    file_schema = {'geometry': df.geom_type.tolist()[0],\n",
    "                    'properties': properties}\n",
    "    \n",
    "    print(\"HERE\")\n",
    "    with fiona.drivers():\n",
    "        with fiona.open(filename, 'w', driver=\"GPKG\", crs=df.crs, schema=file_schema) as colxn:\n",
    "            #colxn.writerecords(df.iterfeatures())\n",
    "            buf = []\n",
    "            for feature in df.iterfeatures():\n",
    "                buf.append(feature)\n",
    "            \n",
    "            print(len(buf))\n",
    "            #colxn.writerecords(buf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for ft in flowlinesBuffer.iterfeatures():\n",
    "    count += 1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writedToGPKG(r\"M:\\Data\\ValleyBottoms\\Watersheds\\1505\\NHD_Flowlines_buffered_python.gpkg\", flowlinesBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import osr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
