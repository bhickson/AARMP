{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "import rasterio as rio\n",
    "import os\n",
    "\n",
    "from skimage.filters import sobel, scharr, prewitt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import data, io, segmentation, color\n",
    "from skimage.future import graph\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "\n",
    "\n",
    "def _weight_mean_color(graph, src, dst, n):\n",
    "    \"\"\"Callback to handle merging nodes by recomputing mean color.\n",
    "\n",
    "    The method expects that the mean color of `dst` is already computed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    n : int\n",
    "        A neighbor of `src` or `dst` or both.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary with the `\"weight\"` attribute set as the absolute\n",
    "        difference of the mean color between node `dst` and `n`.\n",
    "    \"\"\"\n",
    "\n",
    "    diff = graph.node[dst]['mean color'] - graph.node[n]['mean color']\n",
    "    diff = np.linalg.norm(diff)\n",
    "    return {'weight': diff}\n",
    "\n",
    "\n",
    "def merge_mean_color(graph, src, dst):\n",
    "    \"\"\"Callback called before merging two nodes of a mean color distance graph.\n",
    "\n",
    "    This method computes the mean color of `dst`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    \"\"\"\n",
    "    graph.node[dst]['total color'] += graph.node[src]['total color']\n",
    "    graph.node[dst]['pixel count'] += graph.node[src]['pixel count']\n",
    "    graph.node[dst]['mean color'] = (graph.node[dst]['total color'] /\n",
    "                                     graph.node[dst]['pixel count'])\n",
    "\n",
    "\n",
    "nir_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub1.tif\"\n",
    "with rio.open(imanir_imagege) as nir_im:\n",
    "    nir_array = ras.read(1).astype(float)\n",
    "\n",
    "#img = data.coffee()\n",
    "io.imshow(img)\n",
    "io.show()\n",
    "\n",
    "labels = segmentation.slic(img, compactness=20, n_segments=1000)\n",
    "g = graph.rag_mean_color(img, labels)\n",
    "\n",
    "labels2 = graph.merge_hierarchical(labels, g, thresh=10, rag_copy=False,\n",
    "                                   in_place_merge=True,\n",
    "                                   merge_func=merge_mean_color,\n",
    "                                   weight_func=_weight_mean_color)\n",
    "\n",
    "out = color.label2rgb(labels2, img, kind='avg')\n",
    "#out = segmentation.mark_boundaries(out, labels2, (0, 0, 0))\n",
    "io.imshow(out)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CANNY EDGE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\rasterio\\__init__.py:160: FutureWarning: GDAL-style transforms are deprecated and will not be supported in Rasterio 1.0.\n",
      "  transform = guard_transform(transform)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nimgplot = plt.imshow(nir_array)\\nio.show()\\n\\n#io.imshow(nir_edge)\\n#io.show()\\n\\nio.imshow(ndvi_edge_s02)\\nio.show()\\n\\nio.imshow(nir_edge_s02)\\nio.show()\\n\\nio.imshow(red_edge_s02)\\nio.show()\\n\\nio.imshow(green_edge_s02)\\nio.show()\\n\\nio.imshow(blue_edge_s02)\\nio.show()\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import rasterio as rio\n",
    "\n",
    "\n",
    "from skimage import feature\n",
    "    \n",
    "s_val = 2\n",
    "\n",
    "outdir = r\"Q:\\Arid Riparian Project\\Data\\test\"\n",
    "\n",
    "image = r\"Q:/Arid Riparian Project/Data/NAIP_2015_Compressed/m_3311248_ne_12_1_20150529.tif\"\n",
    "#with rio.open(image) as im:\n",
    "#    full_array = im.read().astype(float)\n",
    "\n",
    "#nir_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub4.tif\"\n",
    "with rio.open(image) as nir_im:\n",
    "    kwargs = nir_im.profile\n",
    "    kwargs.update(count=1)\n",
    "    nir_array = nir_im.read(4).astype(float)\n",
    "    nir_edge_s02 = feature.canny(nir_array, sigma=s_val)\n",
    "    outfile = \"nir_canny_\" + str(s_val) + \".tif\"\n",
    "    opath = os.path.join(outdir,outfile)\n",
    "    with rio.open(opath, \"w\", **kwargs) as oras:\n",
    "        oras.write_band(1, nir_edge_s02.astype(rio.uint8))\n",
    "        \n",
    "#red_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub1.tif\"\n",
    "with rio.open(image) as red_im:\n",
    "    kwargs = red_im.profile\n",
    "    kwargs.update(count=1)\n",
    "    red_array = red_im.read(1).astype(float)\n",
    "    red_edge_s02 = feature.canny(red_array, sigma=s_val)\n",
    "    outfile = \"red_canny_\" + str(s_val) + \".tif\"\n",
    "    opath = os.path.join(outdir, outfile)\n",
    "    with rio.open(opath, \"w\", **kwargs) as oras:\n",
    "        oras.write_band(1, red_edge_s02.astype(rio.uint8))\n",
    "    \n",
    "#green_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub2.tif\"\n",
    "with rio.open(image) as green_im:\n",
    "    kwargs = green_im.profile\n",
    "    kwargs.update(count=1)\n",
    "    green_array = green_im.read(2).astype(float)\n",
    "    green_edge_s02 = feature.canny(green_array, sigma=s_val)\n",
    "    outfile = \"green_canny_\" + str(s_val) + \".tif\"\n",
    "    opath = os.path.join(outdir, outfile)\n",
    "    with rio.open(opath, \"w\", **kwargs) as oras:\n",
    "        oras.write_band(1, green_edge_s02.astype(rio.uint8))\n",
    "    \n",
    "#blue_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub3.tif\"\n",
    "with rio.open(image) as blue_im:\n",
    "    kwargs = blue_im.profile\n",
    "    kwargs.update(count=1)\n",
    "    blue_array = blue_im.read(3).astype(float)\n",
    "    blue_edge_s02 = feature.canny(blue_array, sigma=s_val)\n",
    "    outfile = \"blue_canny_\" + str(s_val) + \".tif\"\n",
    "    opath = os.path.join(outdir, outfile)\n",
    "    with rio.open(opath, \"w\", **kwargs) as oras:\n",
    "        oras.write_band(1, blue_edge_s02.astype(rio.uint8))\n",
    "    \n",
    "ndvi_array = (nir_array-red_array)/(nir_array+red_array)\n",
    "ndvi_edge_s02 = feature.canny(ndvi_array, sigma=s_val)\n",
    "outfile = \"ndvi_canny_\" + str(s_val) + \".tif\"\n",
    "opath = os.path.join(outdir, outfile)\n",
    "with rio.open(opath, 'w', **kwargs) as oras:\n",
    "    oras.write_band(1, ndvi_edge_s02.astype(rio.uint8))\n",
    "\n",
    "\n",
    "\n",
    "#all_ims = (nir_edge_s02 + red_edge_s02 + green_edge_s02 + blue_edge_s02 + nir_edge_s02)\n",
    "\"\"\"\n",
    "\n",
    "imgplot = plt.imshow(nir_array)\n",
    "io.show()\n",
    "\n",
    "#io.imshow(nir_edge)\n",
    "#io.show()\n",
    "\n",
    "io.imshow(ndvi_edge_s02)\n",
    "io.show()\n",
    "\n",
    "io.imshow(nir_edge_s02)\n",
    "io.show()\n",
    "\n",
    "io.imshow(red_edge_s02)\n",
    "io.show()\n",
    "\n",
    "io.imshow(green_edge_s02)\n",
    "io.show()\n",
    "\n",
    "io.imshow(blue_edge_s02)\n",
    "io.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function label in module skimage.measure._label:\n",
      "\n",
      "label(input, neighbors=None, background=None, return_num=False, connectivity=None)\n",
      "    Label connected regions of an integer array.\n",
      "    \n",
      "    Two pixels are connected when they are neighbors and have the same value.\n",
      "    In 2D, they can be neighbors either in a 1- or 2-connected sense.\n",
      "    The value refers to the maximum number of orthogonal hops to consider a\n",
      "    pixel/voxel a neighbor::\n",
      "    \n",
      "      1-connectivity      2-connectivity     diagonal connection close-up\n",
      "    \n",
      "           [ ]           [ ]  [ ]  [ ]         [ ]\n",
      "            |               \\  |  /             |  <- hop 2\n",
      "      [ ]--[x]--[ ]      [ ]--[x]--[ ]    [x]--[ ]\n",
      "            |               /  |  \\         hop 1\n",
      "           [ ]           [ ]  [ ]  [ ]\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    input : ndarray of dtype int\n",
      "        Image to label.\n",
      "    neighbors : {4, 8}, int, optional\n",
      "        Whether to use 4- or 8-\"connectivity\".\n",
      "        In 3D, 4-\"connectivity\" means connected pixels have to share face,\n",
      "        whereas with 8-\"connectivity\", they have to share only edge or vertex.\n",
      "        **Deprecated, use ``connectivity`` instead.**\n",
      "    background : int, optional\n",
      "        Consider all pixels with this value as background pixels, and label\n",
      "        them as 0. By default, 0-valued pixels are considered as background\n",
      "        pixels.\n",
      "    return_num : bool, optional\n",
      "        Whether to return the number of assigned labels.\n",
      "    connectivity : int, optional\n",
      "        Maximum number of orthogonal hops to consider a pixel/voxel\n",
      "        as a neighbor.\n",
      "        Accepted values are ranging from  1 to input.ndim. If ``None``, a full\n",
      "        connectivity of ``input.ndim`` is used.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    labels : ndarray of dtype int\n",
      "        Labeled array, where all connected regions are assigned the\n",
      "        same integer value.\n",
      "    num : int, optional\n",
      "        Number of labels, which equals the maximum label index and is only\n",
      "        returned if return_num is `True`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    regionprops\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Christophe Fiorio and Jens Gustedt, \"Two linear time Union-Find\n",
      "           strategies for image processing\", Theoretical Computer Science\n",
      "           154 (1996), pp. 165-181.\n",
      "    .. [2] Kensheng Wu, Ekow Otoo and Arie Shoshani, \"Optimizing connected\n",
      "           component labeling algorithms\", Paper LBNL-56864, 2005,\n",
      "           Lawrence Berkeley National Laboratory (University of California),\n",
      "           http://repositories.cdlib.org/lbnl/LBNL-56864\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> x = np.eye(3).astype(int)\n",
      "    >>> print(x)\n",
      "    [[1 0 0]\n",
      "     [0 1 0]\n",
      "     [0 0 1]]\n",
      "    >>> print(label(x, connectivity=1))\n",
      "    [[1 0 0]\n",
      "     [0 2 0]\n",
      "     [0 0 3]]\n",
      "    >>> print(label(x, connectivity=2))\n",
      "    [[1 0 0]\n",
      "     [0 1 0]\n",
      "     [0 0 1]]\n",
      "    >>> print(label(x, background=-1))\n",
      "    [[1 2 2]\n",
      "     [2 1 2]\n",
      "     [2 2 1]]\n",
      "    >>> x = np.array([[1, 0, 0],\n",
      "    ...               [1, 1, 5],\n",
      "    ...               [0, 0, 0]])\n",
      "    >>> print(label(x))\n",
      "    [[1 0 0]\n",
      "     [1 1 2]\n",
      "     [0 0 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skimage import measure\n",
    "help(measure.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(feature.canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import data, util, filters, color\n",
    "from skimage.morphology import watershed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nir_image = r\"Q:/Arid Riparian Project/Data/test/test_NAIP_sub4.tif\"\n",
    "with rio.open(nir_image) as nir_im:\n",
    "    nir_array = nir_im.read(1)\n",
    "\n",
    "coins = nir_array\n",
    "edges = filters.sobel(coins)\n",
    "\n",
    "grid = util.regular_grid(coins.shape, n_points=300)\n",
    "\n",
    "seeds = np.zeros(coins.shape, dtype=int)\n",
    "seeds[grid] = np.arange(seeds[grid].size).reshape(seeds[grid].shape) + 1\n",
    "\n",
    "w0 = watershed(edges, seeds)\n",
    "w1 = watershed(edges, seeds, compactness=0.05)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "ax0.imshow(color.label2rgb(w0, coins))\n",
    "ax0.set_title('Classical watershed')\n",
    "\n",
    "ax1.imshow(color.label2rgb(w1, coins))\n",
    "ax1.set_title('Compact watershed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "drainage = r\"Q:\\Arid Riparian Project\\Data\\test\\drainage_network_clip.tif\"\n",
    "\n",
    "Y = cdist(XA, XB, 'euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import gdal, osr\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import Utilities as utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhd_dir = r\"M:\\Data\\NHD\"\n",
    "\n",
    "watersheds_shp = os.path.join(nhd_dir, \"WBDHU4_Arizona.shp\")\n",
    "\n",
    "nhd_rasters_dir = os.path.join(nhd_dir, \"Rasters\")\n",
    "\n",
    "rsac_pred_dir = os.path.join(nhd_dir, \"RSAC_VB_Preds\")\n",
    "utilities.useDirectory(rsac_pred_dir)\n",
    "\n",
    "vb_classification_pnts = os.path.join(nhd_dir, \"VM_TrainingData_20180608.shp\")\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "extracts_shp = os.path.join(nhd_dir, \"VB_Predictors_Extracts.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_values(geom):\n",
    "\n",
    "    # print(row)\n",
    "    # geom = row['geometry']\n",
    "    x = geom.centroid.x\n",
    "    y = geom.centroid.y\n",
    "\n",
    "    values = []\n",
    "    # for raster in raster_objects:\n",
    "    # print(\"Starting Raster Extract for %s at x:%s y:%s\" % (os.path.basename(raster), str(x), str(y)))\n",
    "    # with rio.open(raster) as ras:\n",
    "\n",
    "    for val in elev_ras.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in float32_ras.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "\n",
    "    return pd.Series(values, index=raster_names)\n",
    "\n",
    "\n",
    "def getRasterNamesList(pdir):\n",
    "    raster_paths = []\n",
    "    raster_names = []\n",
    "    for root, dirs, files in os.walk(pdir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                fpath = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "                if \"elev_meters\" not in file.lower():\n",
    "                    raster_names.append(file[:-4])\n",
    "                    raster_paths.append(fpath)\n",
    "                else:\n",
    "                    raster_names.insert(0, file[:-4])\n",
    "                    raster_paths.insert(0, fpath)\n",
    "\n",
    "    # elev_raster = os.path.join(pdir, \"elev_meters.tif\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # raster_paths = [elev_raster] + float32_raster_paths\n",
    "    # raster_names = [\"elev_meters\"] + raster_names\n",
    "\n",
    "    return [raster_names, raster_paths]\n",
    "\n",
    "\n",
    "def createClassifiedFile(rasters, regmodel, regname, overwrite=False):\n",
    "    cl_start = datetime.now()\n",
    "\n",
    "    output_fname = \"VB_\" + watershed + \"_\" + subdir + \"_\" + regname + \".tif\"\n",
    "\n",
    "    outquad_loc = os.path.join(rsac_pred_dir, watershed)\n",
    "    if not os.path.exists(outquad_loc):\n",
    "        os.mkdir(outquad_loc)\n",
    "\n",
    "    loc_classified_file = os.path.join(outquad_loc, output_fname)\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        # bands_data_rio = []\n",
    "        bands_data = []\n",
    "        for inras in rasters:\n",
    "            print(\"Reading in raster file as array - %s\" % inras)\n",
    "\n",
    "            with rio.open(inras) as raster:\n",
    "                kwargs = raster.profile\n",
    "                b_array = raster.read(1).astype(rio.float32)\n",
    "\n",
    "            bands_data.append(b_array)\n",
    "\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        print(\"Creating numpy array stack...\")\n",
    "        bands_data = np.dstack(bands_data)\n",
    "\n",
    "        # print(\"BANDS_DATA.SHAPE: \", bands_data.shape)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        n_samples = rows * cols\n",
    "        # print(\"N_Samples: \", n_samples)\n",
    "        # print(\"n_bands: \", n_bands)\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        print(\"Reshaping numpy array to raster shape...\")\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "\n",
    "        print(\"Predicting Valley Bottoms...\")\n",
    "        result = regmodel.predict(flat_pixels)\n",
    "\n",
    "        # Reshape the result: split the labeled pixels into rows to create an image\n",
    "        classification = result.reshape((rows, cols))\n",
    "\n",
    "        # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "        # write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "\n",
    "        kwargs.update(\n",
    "            dtype=rio.float32\n",
    "        )\n",
    "        with rio.open(loc_classified_file, 'w', **kwargs) as outras:\n",
    "            outras.write_band(1, classification.astype(rio.float32))\n",
    "\n",
    "        print(\"Classification created:\\n\\t\", output_fname, \" in \", str(datetime.now() - cl_start))\n",
    "    else:\n",
    "        print(\"The file exists. Skipping %s\" % file)\n",
    "\n",
    "    return loc_classified_file\n",
    "\n",
    "\n",
    "def rasterSubDivide(preds_dir, overwrite=False):\n",
    "    parent_dir = os.path.abspath(os.path.join(preds_dir, os.pardir))\n",
    "    outdir = os.path.join(parent_dir, \"predictors_quads\")\n",
    "    utilities.useDirectory(outdir)\n",
    "\n",
    "    rasters = getRasterNamesList(preds_dir)[1]\n",
    "\n",
    "    for raster in rasters:\n",
    "        reference_f = gdal.Open(raster)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        proj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "        print(\"FULL EXTENT: \", minx, miny, maxx, maxy)\n",
    "\n",
    "        quads_extent_dict = {}\n",
    "\n",
    "        quad1_minx = str(minx)\n",
    "        quad1_maxx = str(minx + ((maxx - minx) / 2))\n",
    "        quad1_miny = str(miny + ((maxy - miny) / 2))\n",
    "        quad1_maxy = str(maxy)\n",
    "        quads_extent_dict[1] = \" \".join([quad1_minx, quad1_miny, quad1_maxx, quad1_maxy])\n",
    "\n",
    "        quad2_minx = str(quad1_maxx)\n",
    "        quad2_maxx = str(maxx)\n",
    "        quad2_miny = str(quad1_miny)\n",
    "        quad2_maxy = str(maxy)\n",
    "\n",
    "        quads_extent_dict[2] = \" \".join([quad2_minx, quad2_miny, quad2_maxx, quad2_maxy])\n",
    "\n",
    "        quad3_minx = str(minx)\n",
    "        quad3_maxx = str(quad1_maxx)\n",
    "        quad3_miny = str(miny)\n",
    "        quad3_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[3] = \" \".join([quad3_minx, quad3_miny, quad3_maxx, quad3_maxy])\n",
    "\n",
    "        quad4_minx = str(quad1_maxx)\n",
    "        quad4_maxx = str(maxx)\n",
    "        quad4_miny = str(miny)\n",
    "        quad4_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[4] = \" \".join([quad4_minx, quad4_miny, quad4_maxx, quad4_maxy])\n",
    "\n",
    "        print(\"Clipping Quads for %s\" % raster)\n",
    "        for i in range(1, 5):\n",
    "            print(\"Starting on quad %d\" % i)\n",
    "            quad_name = \"quad\" + str(i)\n",
    "            quad_dir = os.path.join(outdir, quad_name)\n",
    "            utilities.useDirectory(quad_dir)\n",
    "\n",
    "            oname = os.path.splitext(os.path.basename(raster))[0] + \"_\" + quad_name + \".tif\"\n",
    "            opath = os.path.join(quad_dir, oname)\n",
    "\n",
    "            if not os.path.exists(opath) or overwrite:\n",
    "                ouput_options = \"-overwrite -t_srs %s -tr %s %s -te_srs %s -te %s\" % (\n",
    "                    proj, resx, resy, proj, quads_extent_dict[i])\n",
    "\n",
    "                print(\"Executing gdal_warp operation on %s with extent %s\" % (raster, quads_extent_dict[i]))\n",
    "                gdal.Warp(opath, raster, options=ouput_options)\n",
    "\n",
    "    return outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(extracts_shp) or overwrite:\n",
    "    watersheds_df = gpd.read_file(watersheds_shp)\n",
    "    class_points_df = gpd.read_file(vb_classification_pnts)\n",
    "\n",
    "    if not watersheds_df.crs == class_points_df.crs:\n",
    "        class_points_df.to_crs(watersheds_df.crs, inplace=True)\n",
    "\n",
    "    vbpoints_ras_extract = gpd.sjoin(class_points_df, watersheds_df, op='within')\n",
    "    print(vbpoints_ras_extract.head())\n",
    "\n",
    "    with rio.open(r\"M:\\Data\\NHD\\Rasters\\HRNHDPlusRasters1505\\fac.tif\") as ras:\n",
    "        raster_crs = ras.crs\n",
    "\n",
    "    from rasterio import crs\n",
    "    raster_crs = \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs \"\n",
    "\n",
    "    vbpoints_ras_extract.to_crs(raster_crs, inplace=True)\n",
    "    print(vbpoints_ras_extract.geometry.head())\n",
    "\n",
    "    for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "        print(\"Starting extraction on points in watershed %s\" % watershed)\n",
    "\n",
    "        if watershed == \"1507\":\n",
    "            temp = vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed]\n",
    "            print(temp.shape)\n",
    "\n",
    "\n",
    "        # FIND RELEVANT WATERSHED DIRECTORY\n",
    "        for dir in os.listdir(nhd_rasters_dir):\n",
    "            if watershed in dir:\n",
    "                w_dir = os.path.join(nhd_rasters_dir, dir)\n",
    "                break\n",
    "\n",
    "        predictors_dir = os.path.join(w_dir, \"Predictors\")\n",
    "\n",
    "        float32_raster_paths = []\n",
    "        raster_names = []\n",
    "        for root, dirs, files in os.walk(predictors_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                    if file.lower() != \"elev_meters.tif\":\n",
    "                        raster_names.append(file[:-4])\n",
    "                        float32_raster_paths.append(os.path.join(root, file).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "        elev_raster = os.path.join(predictors_dir, \"elev_meters.tif\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "        vrt_of_rasters = os.path.join(predictors_dir, \"float32_predictors.vrt\")\n",
    "\n",
    "        if not os.path.exists(vrt_of_rasters):\n",
    "            print(\"Building VRT of FLOAT32 Rasters...\")\n",
    "            build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_of_rasters, \" \".join(float32_raster_paths))\n",
    "            os.system(build_vrt)\n",
    "\n",
    "        if not os.path.exists(os.path.join(predictors_dir, \"elev_meters.tif\")):\n",
    "            print(\"PROBLEM - %s doesn't exist in directory %s\" % (\"elev_meters.tif\", predictors_dir))\n",
    "            raise Exception\n",
    "\n",
    "        rasters = [elev_raster] + float32_raster_paths\n",
    "        raster_names = [\"elev_meters\"] + raster_names\n",
    "        print(\"Raster Names: \", raster_names)\n",
    "\n",
    "        if \"elev_meters\" not in vbpoints_ras_extract:\n",
    "            for name in raster_names:\n",
    "                #print(name)\n",
    "                vbpoints_ras_extract[name] = np.NaN\n",
    "\n",
    "        with rio.open(elev_raster) as elev_ras:\n",
    "            with rio.open(vrt_of_rasters) as float32_ras:\n",
    "                vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, raster_names] = \\\n",
    "                    vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, \"geometry\"].apply(get_values)\n",
    "\n",
    "    print(\"273\")\n",
    "    print(\"Finished value extracts. Saving to file: %s\" % extracts_shp)\n",
    "    vbpoints_ras_extract.to_file(driver=\"ESRI Shapefile\", filename=extracts_shp)\n",
    "else:\n",
    "    vbpoints_ras_extract = gpd.read_file(extracts_shp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raster_names = vbpoints_ras_extract.columns.tolist()[18:-1]\n",
    "print(\"RASTER NAMES: %s\" % raster_names)\n",
    "\n",
    "# TRAIN RANDOM FORESTS\n",
    "print(\"Beginning Regression Training\")\n",
    "n_job = 2\n",
    "maxiter = 100\n",
    "solv = 'liblinear'\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "regressorname = \"RandomForestsReg\"\n",
    "if regressorname == \"LogisticReg\":\n",
    "    regressor = LogisticRegression(C=1.0, solver=solv, max_iter=maxiter, verbose=1, n_jobs=n_job)\n",
    "elif regressorname == \"BaysianReg\":\n",
    "    regressor = BayesianRidge(verbose=True)\n",
    "elif regressorname == \"RandomForestsReg\":\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    regressor = RandomForestRegressor(n_jobs=n_job, verbose=True, min_samples_leaf=20)\n",
    "\n",
    "regressor.fit(vbpoints_ras_extract[raster_names].dropna(),\n",
    "              vbpoints_ras_extract[raster_names+[\"VB\"]].dropna()[\"VB\"])\n",
    "\n",
    "\n",
    "# CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "    # FIND RELEVANT WATERSHED DIRECTORY\n",
    "    for dir in os.listdir(nhd_rasters_dir):\n",
    "        if watershed in dir:\n",
    "            w_dir = os.path.join(nhd_rasters_dir, dir)\n",
    "\n",
    "    predictors_dir = os.path.join(w_dir, \"Predictors\")\n",
    "\n",
    "    if watershed == \"1505\":\n",
    "        quads_dir = rasterSubDivide(predictors_dir, overwrite=True)\n",
    "        print(quads_dir)\n",
    "        for subdir in os.listdir(quads_dir):\n",
    "            #print(subdir)\n",
    "            dirpath = os.path.join(quads_dir, subdir)\n",
    "            rasters = getRasterNamesList(dirpath)[1]\n",
    "            #print(\"Starting on directory : %s\" % dirpath)\n",
    "            classified_File = createClassifiedFile(rasters, regressor, regressorname, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
