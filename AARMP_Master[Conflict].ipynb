{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Initializer of the AARMP model in the following steps\n",
    "\n",
    "1. #### Download and of NHD Data for Valley Bottom Models and creation of the two valley bottom models\n",
    "\n",
    "\tThis checks for the existence of the data and then attempts download the NHD-HR data from Amazon S3 servers and create the remaining rasters variables from it. The some functions involved in this prep must be done within a python environment with access to the arcpy module.  \n",
    "\n",
    "1. #### Download and inital prep of Landsat Data\n",
    "\n",
    "\tThis checks for the existence of the final output and then, if it doesn't exist commences looking for the principal dataset and subsequent derivatives.\n",
    "\n",
    "1. #### Georeference Landsat Data\n",
    "\n",
    "\tBegins creating the Valley Bottom raster based on the inputs prepared in step 1. \n",
    "\n",
    "1. #### Calculate Landsat NDWI and NDSI Indicies\n",
    "    \n",
    "    Calculation of Indicies derived from landsat bands (soil and water)\n",
    "\n",
    "1. #### Initiation of the Landcover Analysis\n",
    "\n",
    "\tBeings creating landcover rasters for each QQuad of NAIP imagery. In the creation of each quarter quad, individual principals of vegetation indicies and gaussian blurs of NAIP bands, as well as resample and clipped landsats are prepared on demand. During this landcover classification, each quarter-quad the vegetation classes are isolated, density calculated, and a determination of potential riparian class created. Finally this qquad of potential riparian classes is clipped the valley bottom of that quarter quad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary necessary variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using main data directory M:\\Data for all non-NAIP files\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import linemerge\n",
    "import rasterio as rio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import gdal, osr\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import math\n",
    "from rasterio.merge import merge as merge_tool\n",
    "\n",
    "import Utilities as utils\n",
    "import ValleyBottomRastersPrep\n",
    "import VBET_ValleyBottomModel\n",
    "import RSAC_RegressionModel\n",
    "import getLandsatData\n",
    "import buildLandsat\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set to either INFO or DEBUG\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_dir = os.path.abspath(r\"M:\\Data\")\n",
    "logging.info(\"Using main data directory '%s' for all non-NAIP files\" % data_dir)\n",
    "\n",
    "# Input vector files directory and files\n",
    "vectors_dir = os.path.join(data_dir, \"initial_model_inputs\")\n",
    "\n",
    "vb_classification_pnts = os.path.join(vectors_dir, \"VM_TrainingData_20180619.shp\")\n",
    "watersheds_file = os.path.join(vectors_dir, \"WBDHU4_Arizona.shp\")\n",
    "# Specify the Area of inteurest file and landsat path/row file (wrs2). An intersect will find all necessary path/rows\n",
    "aoi_file = os.path.join(vectors_dir, \"NAIP_AZ_Boundary.gpkg\")\n",
    "landsat_wrs2_file = os.path.join(vectors_dir, \"wrs2_descending.shp\")\n",
    "naip_acqui_vector = os.path.join(vectors_dir, \"NAIP2015_AcquisitionDates_Dissolve.gpkg\")\n",
    "# Find and download the NAIP acquisition dates for year of naip imagery. USDA has this,\n",
    "# but it's hard to get. 2023 features in the layer with ~120 feature limit per download, \n",
    "# so had to chunk it into 100s for export then merge back together. Could also script. \n",
    "# Source: https://gis.apfo.usda.gov/arcgis/rest/services/NAIP_Status/NAIP_Image_Dates/MapServer/52\n",
    "naip_acqui_raster = os.path.join(data_dir, \"NAIP2015_AcquisitionDates_Dissolve.tif\")\n",
    "getLandsatData.rasterizeVector(naip_acqui_vector, naip_acqui_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and of NHD Data for Valley Bottom Models and creation of the two valley bottom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create NHD Data Directory - Hold Valley Bottom Data\n",
    "nhd_dir = os.path.join(data_dir, \"NHD\")\n",
    "watersheds_dir = os.path.join(nhd_dir, \"Watersheds\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Download of necessary NHD raster data\n",
    "getNHDData(nhd_dir)\n",
    "  \n",
    "# ------------------------------------------------\n",
    "# VBET-based Valley Bottom Model, Currently the model used (as of 06/30/2018\n",
    "large_slope_thresh = 2\n",
    "medium_slope_thresh = 5\n",
    "small_slope_thresh = 22\n",
    "\n",
    "VBET_ValleyBottomModel.createVBETValleyBottom(nhd_dir, watersheds_dir, large_slope_thresh, medium_slope_thresh, small_slope_thresh, overwrite=False, cleanup=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# RSAC Regression Model - better potential, currently poor in lower, flatter areas\n",
    "RSAC_RegressionModel.valleyBottomRegression(watersheds_file, vb_classification_pnts, nhd_dir, watersheds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download and inital prep of Landsat Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only DOY calc to determine which landsat scenes to download. Make sure it's the same as the NAIP year\n",
    "year = 2015\n",
    "# Maximum clouds over landsat scene for download on date\n",
    "cloud_lim = 40\n",
    "\n",
    "# Create landsat folder structure\n",
    "landsat_dir = os.path.join(data_dir, \"Landsat8\")\n",
    "utils.useDirectory(landsat_dir)\n",
    "toa_dir = os.path.join(landsat_dir, \"TOA_Corrected\")\n",
    "utils.useDirectory(toa_dir)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Download landsat and run TOA calculation and cloud mask\n",
    "getLandsatData.getLandsatTOA(landsat_dir, naip_acqui_raster, aoi_file, landsat_wrs2_file, cloud_lim)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Using the NAIP DOY acquisition try to match for the landsat values at that location \n",
    "buildLandsat.createLandatDOYCompositeForAOI(naip_acqui_raster, data_dir, toa_dir, landsat_dir,\n",
    "                                   landsat_toa_naipdate_merge, ls_nodata=landsat_nodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Georeference Landsat Data\n",
    "\n",
    "The Landsat data is spatially offset from the NAIP data and needs to be georeferenced to overlap the best we can. The Easiest way to do this is in Arc, but you can only georeference 3 bands at a time. Workflow used was to georeference bands 1-3, then 2-6, 7, and then 8 and then use composite bands tool to merge them all back together. There's no easy way to script this without arcpy.\n",
    "\n",
    "Provided is a sample set of control points in a text file that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "georef_cntrl_points = os.path.abspath(vectors_dir, \"ControlPoints_20180529.txt\")\n",
    "final_georef_landsat_composite = os.path.join(landsat_dir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Landsat NDWI and NDSI Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import RasterCalculations as rs\n",
    "\n",
    "ndwi_dir = os.path.join(data_dir, \"NDWI\")\n",
    "ndsi_dir = os.path.join(data_dir, \"NDSI\")\n",
    "rs.calculateLandsatIndicies(final_georef_landsat_composite, ndwi_outdir=ndwi_dir, ndsi_outdir=ndsi_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initiation of the Landcover Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import createRiparianClassifications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}