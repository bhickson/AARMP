{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Initializer of the AARMP model in the following steps\n",
    "\n",
    "1. #### Download and of NHD Data for Valley Bottom Models and creation of the two valley bottom models\n",
    "\n",
    "\tThis checks for the existence of the data and then attempts download the NHD-HR data from Amazon S3 servers and create the remaining rasters variables from it. The some functions involved in this prep must be done within a python environment with access to the arcpy module.  \n",
    "\n",
    "1. #### Download and inital prep of Landsat Data\n",
    "\n",
    "\tThis checks for the existence of the final output and then, if it doesn't exist commences looking for the principal dataset and subsequent derivatives.\n",
    "\n",
    "1. #### Georeference Landsat Data\n",
    "\n",
    "\tBegins creating the Valley Bottom raster based on the inputs prepared in step 1. \n",
    "\n",
    "1. #### Calculate Landsat NDWI and NDSI Indicies\n",
    "    \n",
    "    Calculation of Indicies derived from landsat bands (soil and water)\n",
    "\n",
    "1. #### Initiation of the Landcover Analysis\n",
    "\n",
    "\tBegins process of segmenting NAIP Images, creating raster stacks of training data, creating landcover classifications, and lastly creating the riparian classification based on the landcover classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary necessary variabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using main data directory M:\\Data for all non-NAIP files\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import linemerge\n",
    "import rasterio as rio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import gdal, osr\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import math\n",
    "from rasterio.merge import merge as merge_tool\n",
    "\n",
    "import Utilities as utils\n",
    "import ValleyBottomRastersPrep\n",
    "import VBET_ValleyBottomModel\n",
    "import RSAC_RegressionModel\n",
    "import getLandsatData\n",
    "import buildLandsat\n",
    "import RasterCalculations as rs\n",
    "from createRiparianClassifications import createClassification\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set to either INFO or DEBUG\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_dir = os.path.abspath(r\"M:\\Data\")\n",
    "logging.info(\"Using main data directory '%s' for all files\" % data_dir)\n",
    "\n",
    "\n",
    "# Initialize Data Directory Structure\n",
    "utils.initializeDirectoryStructure(data_dir)\n",
    "\n",
    "# Input vector files directory and files\n",
    "#vectors_dir = os.path.join(data_dir, \"initial_model_inputs\")\n",
    "\n",
    "vb_classification_pnts = os.path.join(utils.inputs_dir, \"VM_TrainingData_20180619.shp\")\n",
    "watersheds_file = os.path.join(utils.inputs_dir, \"WBDHU4_Arizona.shp\")\n",
    "# Specify the Area of interest file for landsat downloads and landsat path/row file (wrs2). An intersect will find all necessary path/rows\n",
    "aoi_file = os.path.join(utils.utils.inputs_dir, \"NAIP_AZ_Boundary.gpkg\")\n",
    "landsat_wrs2_file = os.path.join(utils.inputs_dir, \"wrs2_descending.shp\")\n",
    "naip_acqui_vector = os.path.join(utils.inputs_dir, \"NAIP2015_AcquisitionDates_Dissolve.gpkg\")\n",
    "# Find and download the NAIP acquisition dates for year of naip imagery. USDA has this,\n",
    "# but it's hard to get. 2023 features in the layer with ~120 feature limit per download, \n",
    "# so had to chunk it into 100s for export then merge back together. Could also script here. \n",
    "# Source: https://gis.apfo.usda.gov/arcgis/rest/services/NAIP_Status/NAIP_Image_Dates/MapServer/52\n",
    "naip_acqui_raster = os.path.join(utils.inputs_dir, \"NAIP2015_AcquisitionDates_Dissolve.tif\")\n",
    "getLandsatData.rasterizeVector(naip_acqui_vector, naip_acqui_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and of NHD Data for Valley Bottom Models and creation of the two valley bottom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Watersheds Data Directory\n",
    "watersheds_dir = os.path.join(utils.valley_bottom_dir, \"Watersheds\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Download of necessary NHD raster data\n",
    "utils.getNHDData(utils.valley_bottom_dir)\n",
    "  \n",
    "# ------------------------------------------------\n",
    "# VBET-based Valley Bottom Model, Currently the model used\n",
    "\"\"\"Large Slope Threshold: The value that represents the upper limit of slopes that will be included in the valley bottom\n",
    "for the 'large' portions of the network.\"\"\"\n",
    "large_slope_thresh = 3.2\n",
    "\"\"\"Medium Slope Threshold: The value that represents the upper limit of slopes that will be included in the valley bottom\n",
    " for the 'medium' portions of the network.\"\"\"\n",
    "medium_slope_thresh = 4.5\n",
    "\"\"\"Small Slope Threshold: The value that represents the upper limit of slopes that will be included in the valley bottoms\n",
    " for the \"small\" portions of the network.\"\"\"\n",
    "small_slope_thresh = 22\n",
    "\n",
    "slope_thresholds = {\"Small\": small_slope_thresh, \"Medium\": medium_slope_thresh, \"Large\": large_slope_thresh}\n",
    "\n",
    "\"\"\"High Drainage Area Threshold: The drainage area value in square meters. Streams whose upstream drainage area is greater\n",
    "than this value will be considered the \"large\" portion of the network, and whose maximum valley bottom width will be\n",
    "represented with the \"Large Buffer Size\" parameter.\"\"\"\n",
    "high_drainage_area_thresh = 1000000  # (m2)\n",
    "\"\"\"Low Drainage Area Threshold: The drainage area value in square meters. Streams whose upstream drainage area is less\n",
    "than this value will be considered the \"small\" portion of the network, and whose maximum valley bottom width will be\n",
    "represented with the \"Small Buffer Size\" parameter. Streams whose upstream drainage area is between the high and low\n",
    "drainage area thresholds will be considered the \"medium\" portion of the network and their maximum valley bottom width\n",
    "represented by the \"Medium Buffer Width\" parameter.\"\"\"\n",
    "low_drainage_area_thresh = 40000  # (m2)\n",
    "\n",
    "drainage_area_thresh = {\"High\": high_drainage_area_thresh, \"Low\": low_drainage_area_thresh}\n",
    "\n",
    "VBET_ValleyBottomModel.createVBETValleyBottom(utils.valley_bottom_dir, watersheds_dir, slope_thresh_dict, overwrite=False, cleanup=False)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# RSAC Regression Model - better potential, currently poor in lower, flatter areas\n",
    "#RSAC_RegressionModel.valleyBottomRegression(watersheds_file, vb_classification_pnts, nhd_dir, watersheds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download and inital preperationof Landsat Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only DOY calc to determine which landsat scenes to download. Make sure it's the same as the NAIP year\n",
    "year = 2015\n",
    "# Maximum clouds over landsat scene for download on date\n",
    "cloud_limit = 40\n",
    "\n",
    "# Create landsat folder for Top of Atmosphere Corrections\n",
    "toa_dir = os.path.join(utils.base_landsatdir, \"TOA_Corrected\")\n",
    "utils.useDirectory(toa_dir)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Download landsat and run TOA calculation and cloud mask\n",
    "getLandsatData.getLandsatTOA(utils.base_landsatdir, naip_acqui_raster, aoi_file, landsat_wrs2_file, cloud_limit)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Using the NAIP DOY acquisition try to match for the landsat values at that location \n",
    "buildLandsat.createLandatDOYCompositeForAOI(naip_acqui_raster, data_dir, toa_dir, utils.base_landsatdir,\n",
    "                                   landsat_toa_naipdate_merge, ls_nodata=landsat_nodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Georeference Landsat Data\n",
    "\n",
    "The Landsat data is spatially offset from the NAIP data and needs to be georeferenced to overlap the best we can. The Easiest way to do this is in Arc, but you can only georeference 3 bands at a time. Workflow used was create a single set of georeferencing points and then to georeference bands 1-3, then 2-6, 7, then 8, and then use Composite Bands tool to merge them all back together. There's no easy way to script this without arcpy.\n",
    "\n",
    "Provided is a sample set of control points in a text file that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "georef_cntrl_points = os.path.abspath(vectors_dir, \"ControlPoints_20180529.txt\")\n",
    "final_georef_landsat_composite = os.path.join(utils.base_landsatdir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Landsat NDWI and NDSI Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs.calculateLandsatIndicies(final_georef_landsat_composite, ndwi_outdir=utils.ndwi_qquad_dir, ndsi_outdir=utils.ndsi_qquad_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initiation of the Landcover Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In acres, the size of area which will be assessed for density of vegetation classes\n",
    "vegetation_assessment_area = 0.1\n",
    "\n",
    "rf_args = {\"maxdepth\": 250,\n",
    "           \"n_est\": 100,\n",
    "           \"n_job\": 2,\n",
    "           \"min_per_leaf\": 50,\n",
    "           \"crit\": \"entropy\"}  # gini or entropy}\n",
    "\n",
    "riplims = {\"xero_limit\": \"StdDev\",\n",
    "           \"meso_limit\": 0.7,\n",
    "           \"hydro_limit\": 0.9}\n",
    "\n",
    "\n",
    "base_data_dir = os.path.abspath(r\"../Data\")\n",
    "\n",
    "area_of_interest = gpd.read_file(os.path.abspath(r\"M:\\Data\\initial_model_inputs\\Ecoregions_AOI.gpkg\"))\n",
    "\n",
    "createClassification(area_of_interest,\n",
    "                     dataDir=base_data_dir,\n",
    "                     vaa=vegetation_assessment_area,\n",
    "                     classifier_args=rf_args,\n",
    "                     riparian_lims=riplims)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}