{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import gdal, osr\n",
    "    import rasterio as rio\n",
    "    from rasterio import features\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import logging as localLog\n",
    "    import math\n",
    "\n",
    "    import Utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "localLog.basicConfig(level=localLog.INFO)\n",
    "\n",
    "valley_bottom_dir = os.path.abspath(r\"M:\\Data\\ValleyBottoms\")\n",
    "watersheds_dir = os.path.join(valley_bottom_dir, \"Watersheds\")\n",
    "\n",
    "vb_classification_pnts = os.path.join(r\"M:\\Data\\inital_model_inputs\", \"VM_TrainingData_20180619.shp\")\n",
    "watersheds_shp = os.path.join(r\"M:\\Data\\inital_model_inputs\", \"WBDHU4_Arizona.shp\")\n",
    "\n",
    "rasters_not_used = [\"TPI_10\", \"TPI_20\", \"TPI_30\", \"Euc_times_Slope\", \"Slope_Degrees\"]\n",
    "\n",
    "#createFlowlineBufferRaster(watersheds_dir, overwrite=True)\n",
    "\n",
    "#valleyBottomRegression(watersheds_shp, vb_classification_pnts, valley_bottom_dir, watersheds_dir, rasters_not_used)\n",
    "\n",
    "#localLog.info(\"Finished creating valley bottoms from regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT TO POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def extractValuesToVBPoints(watersheds_shp, vb_classification_pnts, watershedsDir):\n",
    "    # Extract raster values to training points\n",
    "watersheds_df = gpd.read_file(watersheds_shp)\n",
    "class_points_df = gpd.read_file(vb_classification_pnts)\n",
    "\n",
    "print(class_points_df.crs)\n",
    "\n",
    "if not watersheds_df.crs == class_points_df.crs:\n",
    "    class_points_df.to_crs(watersheds_df.crs, inplace=True)\n",
    "\n",
    "vbpoints_ras_extract = gpd.sjoin(class_points_df, watersheds_df, op='within')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_raster = pickSampleRaster(watersheds_dir, \"elev_cm.tif\")\n",
    "raster_proj = getRasterProj4(sample_raster)\n",
    "\n",
    "localLog.debug(\"Reprojecting training points to coordinate system of rasters...\")\n",
    "vbpoints_ras_extract.to_crs(raster_proj, inplace=True)\n",
    "print(vbpoints_ras_extract.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "    print(watershed)\n",
    "    localLog.info(\"\\nStarting extraction on points in watershed %s\" % watershed)\n",
    "\n",
    "    # FIND RELEVANT WATERSHED DIRECTORY\n",
    "    # TODO This is a redundant workflow in this and the VBET Script\n",
    "    for wdir in os.listdir(watersheds_dir):\n",
    "        if watershed in wdir:\n",
    "            localLog.debug(\"--- BEGINNING ON WATERSHED %s ---\" % wdir)\n",
    "            w_dir = os.path.join(watersheds_dir, wdir)\n",
    "            for subdir in os.listdir(w_dir):\n",
    "                if \"Rasters\" in subdir:\n",
    "                    rasters_dir = os.path.join(w_dir, subdir)\n",
    "                    predictors_dir = os.path.join(rasters_dir, \"Predictors\")\n",
    "                    print(\"Using predictors_dir \", predictors_dir)\n",
    "                    break\n",
    "\n",
    "                if \"GDB\" in subdir:\n",
    "                    geodatabase = os.path.join(w_dir, subdir)\n",
    "\n",
    "    #createFlowlineBufferRaster(watersheds_dir)# overwrite=True)\n",
    "    # simple check to make sure that predictors have been made.\n",
    "    # TODO - initiate calculation if not\n",
    "    elev_raster = os.path.join(predictors_dir, \"elev_meters.tif\")\n",
    "    if not os.path.exists(elev_raster):\n",
    "        localLog.ERROR(\"PROBLEM - %s doesn't exist in directory %s\" % (\"elev_meters.tif\", predictors_dir))\n",
    "        raise Exception    \n",
    "\n",
    "    float32_raster_paths = []\n",
    "    raster_names = []\n",
    "    for root, dirs, files in os.walk(predictors_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                if file.lower() != \"elev_meters.tif\":\n",
    "                    raster_names.append(file[:-4])  # append to list without file extension\n",
    "                    float32_raster_paths.append(os.path.join(root, file))\n",
    "\n",
    "    rasters = [elev_raster] + float32_raster_paths\n",
    "    raster_names = [\"elev_meters\"] + raster_names\n",
    "\n",
    "    print(\"Extracting point values for rasters: \", raster_names)\n",
    "\n",
    "    for name in raster_names:\n",
    "        vbpoints_ras_extract[name] = np.NaN\n",
    "\n",
    "    # Build VRT to makes extraction easier/simpler. Can't include elev_meters because different data type\n",
    "    localLog.debug(\"Building VRT of FLOAT32 Rasters...\")\n",
    "    vrt_of_rasters = os.path.join(predictors_dir, \"float32_predictors.vrt\")\n",
    "    build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_of_rasters, '\"' + '\" \"'.join(float32_raster_paths) +'\"')\n",
    "    #print(\"Executing command to system:\\n %s\\n\" % build_vrt)\n",
    "    os.system(build_vrt)\n",
    "\n",
    "    def get_values(geom):\n",
    "        print(geom)\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "\n",
    "        values = []\n",
    "\n",
    "        for val in elev_ras.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "        for val in float32_ras.sample([(x, y)]):\n",
    "            values += np.ndarray.tolist(val)\n",
    "\n",
    "        return pd.Series(values, index=raster_names)\n",
    "\n",
    "\n",
    "    with rio.open(elev_raster) as elev_ras:\n",
    "        with rio.open(vrt_of_rasters) as float32_ras:\n",
    "            vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, raster_names] = \\\n",
    "                vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, \"geometry\"].apply(get_values)\n",
    "\n",
    "    #print(vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed,])\n",
    "    #break\n",
    "\n",
    "#return {\"points\": vbpoints_ras_extract, \"raster_paths\": rasters, \"raster_names\": raster_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def valleyBottomRegression(watersheds_shp, vb_classification_pnts, vbdir, watersheds_dir, rasters_to_not_regress_with):\n",
    "#extraction_variables = extractValuesToVBPoints(watersheds_shp, vb_classification_pnts, watersheds_dir)\n",
    "\n",
    "vbpoints_raster_values = extraction_variables[\"points\"]\n",
    "#vbpoints_raster_values.to_file(r\"M:\\Data\\ValleyBottoms\\Watersheds\\trainingPoints_extracts.shp\")\n",
    "#rasters = extraction_variables[\"raster_paths\"]\n",
    "raster_names = extraction_variables[\"raster_names\"]\n",
    "\n",
    "print(\"raster_names: \", raster_names)\n",
    "\n",
    "# remove the rasters that were reated in prep, but will not actually be used in regression\n",
    "#rasters_to_not_use = [\"TPI_10\", \"TPI_20\", \"TPI_30\", \"Euc_times_Slope\", \"Slope_Degrees\"]\n",
    "\n",
    "rasters_to_regress_with = specifyTrainingRasters(raster_names, rasters_to_not_regress_with)\n",
    "\n",
    "localLog.debug(\"Rasters to use in regression: \", rasters_to_regress_with)\n",
    "\n",
    "# Only some training data will be used. Those marked with a 1 in 'Use' column\n",
    "vbpoints_raster_values = vbpoints_raster_values[vbpoints_raster_values.Use == 1]\n",
    "\n",
    "#regressValleyBottoms(vbpoints_raster_values, rasters_to_regress_with, rasters_to_not_regress_with, valley_bottom_dir, watersheds_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forests Regression Model to predict valley bottoms. Based loosely on the USFS RSAC model\n",
    "def getRasterNamesList(pdir):\n",
    "    raster_paths = []\n",
    "    raster_names = []\n",
    "    for root, dirs, files in os.walk(pdir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                fpath = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "                if \"elev_meters\" not in file.lower():\n",
    "                    raster_names.append(file[:-4])\n",
    "                    raster_paths.append(fpath)\n",
    "                else:\n",
    "                    raster_names.insert(0, file[:-4])\n",
    "                    raster_paths.insert(0, fpath)\n",
    "\n",
    "    return [raster_names, raster_paths]\n",
    "\n",
    "\n",
    "def createClassifiedFile(rasters, regmodel, loc_classified_file, overwrite=False):\n",
    "    cl_start = datetime.now()\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        localLog.info(\"Creating classified file: %s...\" % loc_classified_file)\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        # bands_data_rio = []\n",
    "        bands_data = []\n",
    "        for inras in rasters:\n",
    "            localLog.info(\"Reading in raster file as array - %s\" % inras)\n",
    "\n",
    "            with rio.open(inras) as raster:\n",
    "                kwargs = raster.profile\n",
    "                b_array = raster.read(1).astype(rio.float32)\n",
    "\n",
    "            bands_data.append(b_array)\n",
    "\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        localLog.debug(\"Creating numpy array stack...\")\n",
    "        bands_data = np.dstack(bands_data)\n",
    "\n",
    "        # print(\"BANDS_DATA.SHAPE: \", bands_data.shape)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        n_samples = rows * cols\n",
    "        # print(\"N_Samples: \", n_samples)\n",
    "        # print(\"n_bands: \", n_bands)\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        localLog.debug(\"Reshaping numpy array to raster shape...\")\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "\n",
    "        localLog.debug(\"Predicting Valley Bottoms...\")\n",
    "        result = regmodel.predict(flat_pixels)\n",
    "\n",
    "        # Reshape the result: split the labeled pixels into rows to create an image\n",
    "        classification = result.reshape((rows, cols))\n",
    "\n",
    "        # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "        # write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "\n",
    "        kwargs.update(\n",
    "            dtype=rio.float32,\n",
    "            nodata=1\n",
    "        )\n",
    "          \n",
    "        with rio.open(loc_classified_file, 'w', **kwargs) as outras:\n",
    "            outras.write_band(1, classification.astype(rio.float32))\n",
    "\n",
    "        localLog.info(\"Classification created:\\n\\t %s in %s\" % (loc_classified_file, str(datetime.now() - cl_start)))\n",
    "    else:\n",
    "        localLog.info(\"The file exists and no overwrite set. Skipping creating %s\" % loc_classified_file)\n",
    "\n",
    "    return loc_classified_file\n",
    "\n",
    "\n",
    "def rasterSubDivide(preds_dir, overwrite=False):\n",
    "    parent_dir = utils.getParentDir(preds_dir)\n",
    "    outdir = os.path.join(parent_dir, \"predictors_quads\")\n",
    "    utils.useDirectory(outdir)\n",
    "\n",
    "    rasters = getRasterNamesList(preds_dir)[1]\n",
    "\n",
    "    for raster in rasters:\n",
    "        reference_f = gdal.Open(raster)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        proj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        quads_extent_dict = {}\n",
    "\n",
    "        quad1_minx = str(minx)\n",
    "        quad1_maxx = str(minx + ((maxx - minx) / 2))\n",
    "        quad1_miny = str(miny + ((maxy - miny) / 2))\n",
    "        quad1_maxy = str(maxy)\n",
    "        quads_extent_dict[1] = \" \".join([quad1_minx, quad1_miny, quad1_maxx, quad1_maxy])\n",
    "\n",
    "        quad2_minx = str(quad1_maxx)\n",
    "        quad2_maxx = str(maxx)\n",
    "        quad2_miny = str(quad1_miny)\n",
    "        quad2_maxy = str(maxy)\n",
    "\n",
    "        quads_extent_dict[2] = \" \".join([quad2_minx, quad2_miny, quad2_maxx, quad2_maxy])\n",
    "\n",
    "        quad3_minx = str(minx)\n",
    "        quad3_maxx = str(quad1_maxx)\n",
    "        quad3_miny = str(miny)\n",
    "        quad3_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[3] = \" \".join([quad3_minx, quad3_miny, quad3_maxx, quad3_maxy])\n",
    "\n",
    "        quad4_minx = str(quad1_maxx)\n",
    "        quad4_maxx = str(maxx)\n",
    "        quad4_miny = str(miny)\n",
    "        quad4_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[4] = \" \".join([quad4_minx, quad4_miny, quad4_maxx, quad4_maxy])\n",
    "\n",
    "        localLog.debug(\"Clipping Quads for %s\" % raster)\n",
    "        for i in range(1, 5):\n",
    "            #print(\"Starting on quad %d\" % i)\n",
    "            quad_name = \"quad\" + str(i)\n",
    "            quad_dir = os.path.join(outdir, quad_name)\n",
    "            \n",
    "            utils.useDirectory(quad_dir)\n",
    "\n",
    "            oname = os.path.splitext(os.path.basename(raster))[0] + \"_\" + quad_name + \".tif\"\n",
    "            opath = os.path.join(quad_dir, oname)\n",
    "\n",
    "            if not os.path.exists(opath) or overwrite:\n",
    "                ouput_options = \"-overwrite -t_srs %s -tr %s %s -te_srs %s -te %s\" % (\n",
    "                    proj, resx, resy, proj, quads_extent_dict[i])\n",
    "\n",
    "                localLog.info(\"Executing gdal_warp operation on %s with extent %s\" % (raster, quads_extent_dict[i]))\n",
    "                gdal.Warp(opath, raster, options=ouput_options)\n",
    "\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def specifyTrainingRasters(alist, to_remove):\n",
    "    keepers = alist[:]\n",
    "    for ras in alist:\n",
    "        for string in to_remove:\n",
    "            if string in ras:\n",
    "                keepers.remove(ras)\n",
    "\n",
    "    keepers = sorted(keepers)\n",
    "    return keepers\n",
    "\n",
    "\n",
    "def pickSampleRaster(directory, fileName):\n",
    "    \"\"\" given a directory returns the path of the first file matching the first file matching fileName \"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == fileName:\n",
    "                fpath = os.path.join(root, file)\n",
    "                print(\"Sample raster %s\" % fpath)\n",
    "                return fpath\n",
    "\n",
    "    if \"fpath\" not in locals():\n",
    "        localLog.error(\"Unable to find sample raster matching %s in directory %s. Aborting.\" % (fileName, directory))\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def regressValleyBottoms(vbpoints_ras_extract, rasters_to_use, rasters_to_not_use, vb_dir, watershedsDir, overwrite=False):\n",
    "    localLog.debug(\"Beginning Regression Training\")\n",
    "    n_job = 2\n",
    "    msl = 20\n",
    "    print(\"rasters_to_use\", rasters_to_use)\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    regressor = RandomForestRegressor(n_jobs=n_job, verbose=True, min_samples_leaf=msl)\n",
    "\n",
    "    regressor.fit(vbpoints_ras_extract[rasters_to_use].dropna(),\n",
    "                  vbpoints_ras_extract[rasters_to_use + [\"VB\"]].dropna()[\"VB\"])\n",
    "\n",
    "    # CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "    for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "\n",
    "        # FIND RELEVANT WATERSHED DIRECTORY\n",
    "        ## TODO, this is a redundant workflow in this file and in the VBET process. Create function to replace\n",
    "        for wdir in os.listdir(watershedsDir):\n",
    "            if watershed in wdir:\n",
    "                localLog.debug(\"--- BEGINNING ON WATERSHED %s ---\" % wdir)\n",
    "                watersheds_dir = os.path.join(watershedsDir, wdir)\n",
    "                for subdir in os.listdir(watershedDir):\n",
    "                    if \"Rasters\" in subdir:\n",
    "                        rasters_dir = os.path.join(watershedDir, subdir)\n",
    "                    # if \"GDB\" in subdir:\n",
    "                    #    geodatabase = os.path.join(watershedDir, subdir)\n",
    "\n",
    "                    predictors_dir = os.path.join(rasters_dir, \"Predictors\")\n",
    "\n",
    "                break\n",
    "\n",
    "        # This step divide the watershed predictors into 4 quadrants and return the location of the folder\n",
    "        quadsDir = rasterSubDivide(predictors_dir, overwrite=False)\n",
    "\n",
    "        # Set the output directory to write prediction rasters to\n",
    "        outquad_preds_dir = os.path.join(watershedDir, \"RSAC_temp\")\n",
    "        utils.useDirectory(outquad_preds_dir)\n",
    "\n",
    "        for subdir in os.listdir(quadsDir):\n",
    "            \n",
    "            dirpath = os.path.join(quadsDir, subdir)\n",
    "            raster_paths = sorted(getRasterNamesList(dirpath)[1])\n",
    "\n",
    "            qquad_rasters_to_use = sorted(specifyTrainingRasters(raster_paths, rasters_to_not_use))\n",
    "            print(\"qquad_rasters_to_use: \", qquad_rasters_to_use)\n",
    "            \n",
    "            for i in range(len(rasters_to_use)):\n",
    "                localLog.debug(i, rasters_to_use[i], os.path.basename(raster_paths[i]))\n",
    "            \n",
    "            if len(rasters_to_use) != len(rasters_to_use):\n",
    "                raise Exception\n",
    "\n",
    "            localLog.debug(\"Starting on directory : %s\" % dirpath)\n",
    "\n",
    "            modeltype = \"RandomForestsReg\"\n",
    "            output_fname = \"VB_\" + watershed + \"_\" + subdir + \"_\" + modeltype + \".tif\"\n",
    "            loc_classified_file = os.path.join(outquad_preds_dir, output_fname)\n",
    "\n",
    "            classified_File = createClassifiedFile(qquad_rasters_to_use, regressor, loc_classified_file, overwrite=True)\n",
    "\n",
    "        # NEED TO MERGE QUADS OF WATERSHED BACK TO ONE THE WATERSHED\n",
    "        watershed_rsac_name = \"VB_\" + watershed + \"_\" + modeltype + \".tif\"\n",
    "        watershed_rsac_path = os.path.join(watershedDir, watershed_rsac_name)\n",
    "\n",
    "        if not os.path.exists(watershed_rsac_path) or overwrite:\n",
    "            quadfiles = []\n",
    "            for file in os.listdir(outquad_preds_dir):\n",
    "                if modeltype in file and file.endswith(\".tif\"):\n",
    "                    fpath = os.path.join(outquad_preds_dir, file)\n",
    "                    quadfiles.append(fpath)\n",
    "\n",
    "            utils.mergeRasters(quadfiles, watershed_rsac_path)\n",
    "\n",
    "    # MERGE ALL WATERSHEDS TO ONE RASTER FOR WHOLE STATE\n",
    "    state_rsac_name = \"RSAC_ValleyBottoms.tif\"\n",
    "    state_rsac_path = os.path.join(vb_dir, state_rsac_name)\n",
    "\n",
    "    if not os.path.exists(state_rsac_path) or overwrite:\n",
    "        watershedfiles = []\n",
    "        for w_dir in os.listdir(watershedsDir):\n",
    "            watershedDir = os.path.join(watershedsDir, w_dir)\n",
    "            for file in os.listdir(watershedDir):\n",
    "                if modeltype in file and file.endswith(\".tif\"):\n",
    "                    fpath = os.path.join(watershedDir, file)\n",
    "                    watershedfiles.append(fpath)\n",
    "\n",
    "        utils.mergeRasters(watershedfiles, state_rsac_path)\n",
    "\n",
    "\n",
    "def getRasterProj4(raster):\n",
    "    print(\"Getting projection information for %s\" % raster)\n",
    "    \"\"\" Function returns the projection of the input raster in proj4\"\"\"\n",
    "    fac = gdal.Open(raster)\n",
    "\n",
    "    ras_proj = fac.GetProjection()\n",
    "    spatialRef = osr.SpatialReference()\n",
    "\n",
    "    osr.UseExceptions()\n",
    "    # Apparently osr has difficulties identifying albers projections\n",
    "    prjText = ras_proj.replace('\"Albers\"', '\"Albers_Conic_Equal_Area\"')\n",
    "    spatialRef.ImportFromWkt(prjText)\n",
    "    ras_proj_proj4 = spatialRef.ExportToProj4()\n",
    "    return ras_proj_proj4\n",
    "\n",
    "\n",
    "def getResAndExtent(raster_file):\n",
    "    \"\"\" RETURN THE RESOLUTION AND EXTENT OF THE RASTER AS A LIST [resx, resy, xmin, ymin, xmax, ymax]\"\"\"\n",
    "    with rio.open(raster_file) as ras:\n",
    "        ymax = ras.profile['transform'][5]\n",
    "        xmin = ras.profile['transform'][2]\n",
    "        height = ras.profile['height']\n",
    "        width = ras.profile['width']\n",
    "        resx = ras.profile['transform'][0]\n",
    "        resy = ras.profile['transform'][4]\n",
    "        ymin = ymax + (height * resy)\n",
    "        xmax = xmin + (width * resx)\n",
    "\n",
    "        return [abs(resx), abs(resy), str(xmin), str(ymin), str(xmax), str(ymax)]\n",
    "\n",
    "\n",
    "def createFlowlineBufferRaster(watershedsDir, overwrite=False, cleanup=False):\n",
    "    \"\"\" Creates a single output valley bottom using the VBET methodology. First iterates watersheds directory\n",
    "    for each HUC4 watersheds and \"\"\"\n",
    "\n",
    "    # The final output of the script\n",
    "    #vbet_allwatersheds = os.path.join(indir, \"VBET_ValleyBottoms.tif\")\n",
    "\n",
    "    # Watershed Size Column Name\n",
    "    watershedsize_col = \"TotDASqKm\"\n",
    "\n",
    "    for w_dir in os.listdir(watershedsDir):\n",
    "        localLog.info(\"--- BEGINNING ON WATERSHED %s ---\" % w_dir)\n",
    "        watershed_dir = os.path.join(watershedsDir, w_dir)\n",
    "        for subdir in os.listdir(watershed_dir):\n",
    "            if \"Rasters\" in subdir:\n",
    "                rasters_dir = os.path.join(watershed_dir, subdir)\n",
    "            if \"GDB\" in subdir:\n",
    "                geodatabase = os.path.join(watershed_dir, subdir)\n",
    "\n",
    "        fac_raster_loc = os.path.join(rasters_dir, \"fac.tif\")\n",
    "        preds_dir = os.path.join(rasters_dir, \"Predictors\")\n",
    "        intermediate_preds_dir = os.path.join(rasters_dir, \"RSAC_Intermediates\")\n",
    "        dem_ras = os.path.join(preds_dir, \"elev_meters.tif\")\n",
    "        slope_ras = os.path.join(intermediate_preds_dir, \"Slope.tif\")\n",
    "\n",
    "        outbuffer_ras = os.path.join(preds_dir, \"WatershedBufferSize.tif\")\n",
    "\n",
    "        raster_crs = getRasterProj4(dem_ras)\n",
    "\n",
    "        if not os.path.exists(slope_ras) or not os.path.exists(dem_ras):\n",
    "            localLog.error(\n",
    "                \"Slope raster does not exist. Run RSAC preprocessing script from ArcGIS python environment.\")\n",
    "            raise Exception\n",
    "\n",
    "        if not os.path.exists(outbuffer_ras) or overwrite:\n",
    "            localLog.info(\"%s doesn't exist. Beginning creation...\" % outbuffer_ras)\n",
    "            flowlines_vector = os.path.join(watershed_dir, \"NHD_Flowlines_buffered.shp\")\n",
    "            if not os.path.exists(flowlines_vector) or overwrite:\n",
    "                localLog.debug(\"Reading in NHD flowlines feature class from geodatabase...\")\n",
    "                flowlines = gpd.GeoDataFrame.from_file(geodatabase, layer='NHDFlowline')\n",
    "                f_crs = flowlines.crs\n",
    "\n",
    "                # GET VAA TABLE AND JOIN TO FEATURE CLASS FOR WATERSHED SIZE\n",
    "                flowlines_vaa = gpd.GeoDataFrame.from_file(geodatabase, layer='NHDPlusFlowlineVAA')\n",
    "\n",
    "                flowlines_merge = flowlines.merge(flowlines_vaa, on='NHDPlusID')\n",
    "\n",
    "                # Merge renames geometry to geometry_x. Fix\n",
    "                flowlines_merge['geometry'] = flowlines_merge['geometry_x']\n",
    "\n",
    "                # merge is a pandas operation, re-read in as a geodataframe\n",
    "                flowlines_merge = gpd.GeoDataFrame(flowlines_merge, crs=f_crs, geometry=\"geometry\")\n",
    "\n",
    "                flowlines_merge.to_crs(raster_crs, inplace=True)\n",
    "\n",
    "                localLog.debug(\"Reprojecting flowlines dataframe to FAC raster projection...\")\n",
    "\n",
    "                # TODO - select only flowlines which are true in-ground streams. Do not include canals, culverts, etc\n",
    "                # pipline underground  - FCODE 42803, 42804, 42807, 42808, 42812...\n",
    "\n",
    "                # Cleanup flowlines table by removing all columns not geometry\n",
    "                drop_columns = flowlines_merge.columns.tolist()\n",
    "                drop_columns.remove('geometry')\n",
    "                drop_columns.remove(watershedsize_col)\n",
    "                flowlines_merge.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "                # get resolution\n",
    "                with rio.open(fac_raster_loc) as ras:\n",
    "                    res_x, res_y = ras.res\n",
    "\n",
    "                # pixel X number in km\n",
    "                ratio_x = 1000 / res_x\n",
    "                ratio_y = 1000 / res_y\n",
    "\n",
    "                # convert watershed size from km to m\n",
    "                flowlines_merge[\"TotDASq_m\"] = flowlines_merge[watershedsize_col] * ratio_x * ratio_y\n",
    "\n",
    "                def calculateBufferSize(da):\n",
    "                    if da <= 1:\n",
    "                        buff_size = 1\n",
    "                    else:\n",
    "                        buff_size = math.sqrt(da) / (math.log(da, 10) * (4 / 3))\n",
    "\n",
    "                    return buff_size\n",
    "\n",
    "                def bufferLines(row):\n",
    "                    geom = row.geometry\n",
    "                    buffersize = row.BufferSize\n",
    "                    # fac = row[watershedsize_col]\n",
    "\n",
    "                    # log of 1 is 0, can't divide by zero. Also, a Flow accumulation value of 1 or zero is a misread, essentially minimum buffer size\n",
    "                    # if fac <= 1:\n",
    "                    #    fac = 2\n",
    "\n",
    "                    \"\"\"Simple equation which correlates the flow accumulation values (fac), e.g. watershed size,\n",
    "                    to the appropriate valley bottom buffer\"\"\"\n",
    "                    # buffersize = math.sqrt(fac) / (math.log(fac, 10) * (4 / 3))\n",
    "\n",
    "                    return geom.buffer(buffersize)\n",
    "\n",
    "                flowlines_merge[\"BufferSize\"] = flowlines_merge[\"TotDASq_m\"].apply(calculateBufferSize)\n",
    "\n",
    "                # Buffer each flowline by its watershed size\n",
    "                flowlines_merge[\"geometry\"] = flowlines_merge.apply(bufferLines, axis=1)\n",
    "\n",
    "                # Write out to shapefile\n",
    "                localLog.debug(\"Writing out flowline buffers to file: %s\" % flowlines_vector)\n",
    "                #print(\"Flowline dtypes:\\n {}\".format(flowlines.dtypes))\n",
    "\n",
    "                flowlines_merge.to_file(flowlines_vector)\n",
    "            else:\n",
    "                flowlines_merge = gpd.read_file(flowlines_vector)\n",
    "\n",
    "            with rio.open(dem_ras) as ras:\n",
    "                # copy and update the metadata from the input raster for the output\n",
    "                meta = ras.meta.copy()\n",
    "                # meta.update(compress='lzw')\n",
    "                shape = ras.shape\n",
    "                inras_crs = ras.crs.from_string(raster_crs)\n",
    "\n",
    "            meta.update(\n",
    "                dtype=np.float32,\n",
    "                nodata=-9999,\n",
    "                crs=inras_crs\n",
    "            )\n",
    "\n",
    "            # create empty blank array\n",
    "            out_arr = np.full(shape, -9999.0, dtype=np.float32)\n",
    "\n",
    "            # this is where we create a generator of geom, value pairs to use in rasterizing\n",
    "            feat_shapes = ((geom, value) for geom, value in zip(flowlines_merge.geometry, flowlines_merge.BufferSize))\n",
    "            # write generator to array\n",
    "            burned_array = features.rasterize(shapes=feat_shapes, fill=-9999, out=out_arr,\n",
    "                                                  transform=meta['transform'], dtype=np.float32)\n",
    "            # write array to file\n",
    "            print(\"Writing %s to file\" % outbuffer_ras)\n",
    "            with rio.open(outbuffer_ras, 'w', **meta) as out_ras:\n",
    "                out_ras.write_band(1, burned_array.astype(np.float32))\n",
    "\n",
    "            print(\"FINISHED\")\n",
    "\n",
    "            if cleanup:\n",
    "                os.remove(flowlines_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
