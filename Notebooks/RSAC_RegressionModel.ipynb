{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import gdal, osr\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "#from sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression, LarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# make sure that statsmodel (latest in conda forge) and pytest (conda forge) are installed\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import Utilities as utils\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRasterNamesList(pdir):\n",
    "    raster_paths = []\n",
    "    raster_names = []\n",
    "    for root, dirs, files in os.walk(pdir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                fpath = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
    "                if \"elev_meters\" not in file.lower():\n",
    "                    raster_names.append(file[:-4])\n",
    "                    raster_paths.append(fpath)\n",
    "                else:\n",
    "                    raster_names.insert(0, file[:-4])\n",
    "                    raster_paths.insert(0, fpath)\n",
    "\n",
    "    return [raster_names, raster_paths]\n",
    "\n",
    "\n",
    "def createClassifiedFile(rasters, regmodel, loc_classified_file, overwrite=False):\n",
    "    cl_start = datetime.now()\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        logging.info(\"Creating classified file: %s...\" % loc_classified_file)\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        # bands_data_rio = []\n",
    "        bands_data = []\n",
    "        for inras in rasters:\n",
    "            logging.debug(\"Reading in raster file as array - %s\" % inras)\n",
    "\n",
    "            with rio.open(inras) as raster:\n",
    "                kwargs = raster.profile\n",
    "                b_array = raster.read(1).astype(rio.float32)\n",
    "\n",
    "            bands_data.append(b_array)\n",
    "\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        logging.debug(\"Creating numpy array stack...\")\n",
    "        bands_data = np.dstack(bands_data)\n",
    "\n",
    "        # print(\"BANDS_DATA.SHAPE: \", bands_data.shape)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        n_samples = rows * cols\n",
    "        # print(\"N_Samples: \", n_samples)\n",
    "        # print(\"n_bands: \", n_bands)\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        logging.debug(\"Reshaping numpy array to raster shape...\")\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "\n",
    "        logging.debug(\"Predicting Valley Bottoms...\")\n",
    "        result = regmodel.predict(flat_pixels)\n",
    "\n",
    "        # Reshape the result: split the labeled pixels into rows to create an image\n",
    "        classification = result.reshape((rows, cols))\n",
    "\n",
    "        # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "        # write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "\n",
    "        kwargs.update(\n",
    "            dtype=rio.float32,\n",
    "            nodata=1\n",
    "        )\n",
    "          \n",
    "        with rio.open(loc_classified_file, 'w', **kwargs) as outras:\n",
    "            outras.write_band(1, classification.astype(rio.float32))\n",
    "\n",
    "        logging.info(\"Classification created:\\n\\t\", output_fname, \" in \", str(datetime.now() - cl_start))\n",
    "    else:\n",
    "        logging.info(\"The file exists and no overwrite set. Skipping creating %s\" % loc_classified_file)\n",
    "\n",
    "    return loc_classified_file\n",
    "\n",
    "\n",
    "def rasterSubDivide(preds_dir, overwrite=False):\n",
    "    parent_dir = utils.getParentDir(preds_dir)\n",
    "    outdir = os.path.join(parent_dir, \"predictors_quads\")\n",
    "    utils.useDirectory(outdir)\n",
    "\n",
    "    rasters = getRasterNamesList(preds_dir)[1]\n",
    "\n",
    "    for raster in rasters:\n",
    "        reference_f = gdal.Open(raster)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        proj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        quads_extent_dict = {}\n",
    "\n",
    "        quad1_minx = str(minx)\n",
    "        quad1_maxx = str(minx + ((maxx - minx) / 2))\n",
    "        quad1_miny = str(miny + ((maxy - miny) / 2))\n",
    "        quad1_maxy = str(maxy)\n",
    "        quads_extent_dict[1] = \" \".join([quad1_minx, quad1_miny, quad1_maxx, quad1_maxy])\n",
    "\n",
    "        quad2_minx = str(quad1_maxx)\n",
    "        quad2_maxx = str(maxx)\n",
    "        quad2_miny = str(quad1_miny)\n",
    "        quad2_maxy = str(maxy)\n",
    "\n",
    "        quads_extent_dict[2] = \" \".join([quad2_minx, quad2_miny, quad2_maxx, quad2_maxy])\n",
    "\n",
    "        quad3_minx = str(minx)\n",
    "        quad3_maxx = str(quad1_maxx)\n",
    "        quad3_miny = str(miny)\n",
    "        quad3_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[3] = \" \".join([quad3_minx, quad3_miny, quad3_maxx, quad3_maxy])\n",
    "\n",
    "        quad4_minx = str(quad1_maxx)\n",
    "        quad4_maxx = str(maxx)\n",
    "        quad4_miny = str(miny)\n",
    "        quad4_maxy = str(quad1_miny)\n",
    "\n",
    "        quads_extent_dict[4] = \" \".join([quad4_minx, quad4_miny, quad4_maxx, quad4_maxy])\n",
    "\n",
    "        logging.debug(\"Clipping Quads for %s\" % raster)\n",
    "        for i in range(1, 5):\n",
    "            #print(\"Starting on quad %d\" % i)\n",
    "            quad_name = \"quad\" + str(i)\n",
    "            quad_dir = os.path.join(outdir, quad_name)\n",
    "            \n",
    "            utils.useDirectory(quad_dir)\n",
    "\n",
    "            oname = os.path.splitext(os.path.basename(raster))[0] + \"_\" + quad_name + \".tif\"\n",
    "            opath = os.path.join(quad_dir, oname)\n",
    "\n",
    "            if not os.path.exists(opath) or overwrite:\n",
    "                ouput_options = \"-overwrite -t_srs %s -tr %s %s -te_srs %s -te %s\" % (\n",
    "                    proj, resx, resy, proj, quads_extent_dict[i])\n",
    "\n",
    "                logging.info(\"Executing gdal_warp operation on %s with extent %s\" % (raster, quads_extent_dict[i]))\n",
    "                gdal.Warp(opath, raster, options=ouput_options)\n",
    "\n",
    "    return outdir\n",
    "\n",
    "def specifyTrainingRasters(alist, to_remove):\n",
    "    keepers = alist[:]\n",
    "    for ras in alist:\n",
    "        for string in to_remove:\n",
    "            if string in ras:\n",
    "                keepers.remove(ras)\n",
    "\n",
    "    keepers = sorted(keepers)\n",
    "    return keepers\n",
    "\n",
    "def pickSampleRaster(directory, fileName):\n",
    "    \"\"\" given a directory returns the path of the first file matching the first file matching fileName \"\"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == fileName:\n",
    "                fpath = os.path.join(root, file)\n",
    "                return fpath\n",
    "\n",
    "def extractValuesToVBPoints(watersheds_shp, vb_classification_pnts, watershedsDir):\n",
    "    # Extract raster values to training points\n",
    "    watersheds_df = gpd.read_file(watersheds_shp)\n",
    "    class_points_df = gpd.read_file(vb_classification_pnts)\n",
    "\n",
    "    if not watersheds_df.crs == class_points_df.crs:\n",
    "        class_points_df.to_crs(watersheds_df.crs, inplace=True)\n",
    "\n",
    "    vbpoints_ras_extract = gpd.sjoin(class_points_df, watersheds_df, op='within')\n",
    "\n",
    "    sample_raster = pickSampleRaster(watershedsDir, \"elev_cm.tif\")\n",
    "    ras = gdal.Open(sample_raster)\n",
    "    ras_proj = ras.GetProjection()\n",
    "    spatialRef = osr.SpatialReference()\n",
    "    osr.UseExceptions()\n",
    "    # Apparently osr has difficulties identifying albers projections\n",
    "    prjText = ras_proj.replace('\"Albers\"', '\"Albers_Conic_Equal_Area\"')\n",
    "    spatialRef.ImportFromWkt(prjText)\n",
    "    ras_proj_proj4 = spatialRef.ExportToProj4()\n",
    "\n",
    "    logging.debug(\"Reprojecting training points to cooridinate system of rasters...\")\n",
    "    vbpoints_ras_extract.to_crs(ras_proj_proj4, inplace=True)\n",
    "\n",
    "    for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "        logging.info(\"Starting extraction on points in watershed %s\" % watershed)\n",
    "\n",
    "        # FIND RELEVANT WATERSHED DIRECTORY\n",
    "        # TODO This is a redunant workflow in this and the VBET Script\n",
    "        for wdir in os.listdir(watershedsDir):\n",
    "            if watershed in wdir:\n",
    "                logging.debug(\"--- BEGINNING ON WATERSHED %s ---\" % wdir)\n",
    "                w_dir = os.path.join(watershedsDir, wdir)\n",
    "                for subdir in os.listdir(w_dir):\n",
    "                    if \"Rasters\" in subdir:\n",
    "                        rasters_dir = os.path.join(w_dir, subdir)\n",
    "                    if \"GDB\" in subdir:\n",
    "                        geodatabase = os.path.join(w_dir, subdir)\n",
    "\n",
    "                    predictors_dir = os.path.join(rasters_dir, \"Predictors\")\n",
    "                \n",
    "                break\n",
    "                \n",
    "        # simple check to make sure that predictors have been made.\n",
    "        # TODO - initiate calculation if not\n",
    "        elev_raster = os.path.join(predictors_dir, \"elev_meters.tif\")\n",
    "        if not os.path.exists(elev_raster):\n",
    "            logging.ERROR(\"PROBLEM - %s doesn't exist in directory %s\" % (\"elev_meters.tif\", predictors_dir))\n",
    "            raise Exception    \n",
    "            \n",
    "        float32_raster_paths = []\n",
    "        raster_names = []\n",
    "        for root, dirs, files in os.walk(predictors_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".tif\") or file.endswith(\".img\"):\n",
    "                    if file.lower() != \"elev_meters.tif\":\n",
    "                        raster_names.append(file[:-4]) # append to list without file extension\n",
    "                        float32_raster_paths.append(os.path.join(root, file))\n",
    "\n",
    "        rasters = [elev_raster] + float32_raster_paths\n",
    "        raster_names = [\"elev_meters\"] + raster_names\n",
    "\n",
    "        for name in raster_names:\n",
    "            vbpoints_ras_extract[name] = np.NaN\n",
    "            \n",
    "        # Build VRT to makes extraction easier/simpler. Can't include elev_meters because different data type\n",
    "        logging.debug(\"Building VRT of FLOAT32 Rasters...\")\n",
    "        vrt_of_rasters = os.path.join(predictors_dir, \"float32_predictors.vrt\")\n",
    "        build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_of_rasters, '\"' + '\" \"'.join(float32_raster_paths) +'\"')\n",
    "        os.system(build_vrt)\n",
    "\n",
    "        def get_values(geom):\n",
    "            \n",
    "            x = geom.centroid.x\n",
    "            y = geom.centroid.y\n",
    "\n",
    "            values = []\n",
    "\n",
    "            for val in elev_ras.sample([(x, y)]):\n",
    "                values += np.ndarray.tolist(val)\n",
    "            for val in float32_ras.sample([(x, y)]):\n",
    "                values += np.ndarray.tolist(val)\n",
    "\n",
    "            return pd.Series(values, index=raster_names)\n",
    "            \n",
    "            \n",
    "        with rio.open(elev_raster) as elev_ras:\n",
    "            with rio.open(vrt_of_rasters) as float32_ras:\n",
    "                vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, raster_names] = \\\n",
    "                    vbpoints_ras_extract.loc[vbpoints_ras_extract.HUC4 == watershed, \"geometry\"].apply(get_values)\n",
    "    \n",
    "    return {\"points\":vbpoints_ras_extract, \"raster_paths\":rasters, \"raster_names\": raster_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regressValleyBottoms(vbpoints_ras_extract, values_to_train_on, nhdDir, watershedsDir, overwrite=False):\n",
    "    logging.debug(\"Beginning Regression Training\")\n",
    "    n_job = 2\n",
    "    msl = 20\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    regressor = RandomForestRegressor(n_jobs=n_job, verbose=True, min_samples_leaf=msl)\n",
    "\n",
    "    regressor.fit(vbpoints_ras_extract[values_to_train_on].dropna(),\n",
    "                  vbpoints_ras_extract[values_to_train_on + [\"VB\"]].dropna()[\"VB\"])\n",
    "\n",
    "    # CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "    for watershed, group in vbpoints_ras_extract.groupby(\"HUC4\"):\n",
    "\n",
    "        # FIND RELEVANT WATERSHED DIRECTORY\n",
    "        ## TODO, this is a redundant workflow in this file and in the VBET process. Create function to replace\n",
    "        for wdir in os.listdir(watershedsDir):\n",
    "            if watershed in wdir:\n",
    "                logging.debug(\"--- BEGINNING ON WATERSHED %s ---\" % wdir)\n",
    "                watershedDir = os.path.join(watershedsDir, wdir)\n",
    "                for subdir in os.listdir(watershedDir):\n",
    "                    if \"Rasters\" in subdir:\n",
    "                        rasters_dir = os.path.join(watershedDir, subdir)\n",
    "                    # if \"GDB\" in subdir:\n",
    "                    #    geodatabase = os.path.join(watershedDir, subdir)\n",
    "\n",
    "                    predictors_dir = os.path.join(rasters_dir, \"Predictors\")\n",
    "\n",
    "                break\n",
    "\n",
    "        # This step divide the watershed predictors into 4 quadrants and return the location of the folder\n",
    "        quadsDir = rasterSubDivide(predictors_dir, overwrite=False)\n",
    "\n",
    "        # Set the output directory to write prediction rasters to\n",
    "        outquad_preds_dir = os.path.join(watershedDir, \"RSAC_temp\")\n",
    "        utils.useDirectory(outquad_preds_dir)\n",
    "\n",
    "        for subdir in os.listdir(quadsDir):\n",
    "            \n",
    "            dirpath = os.path.join(quadsDir, subdir)\n",
    "            raster_paths = sorted(getRasterNamesList(dirpath)[1])\n",
    "            rasters_to_use = sorted(specifyTrainingRasters(raster_paths, [\"TPI_20\", \"TPI_30\"]))\n",
    "            \n",
    "            for i in range(len(values_to_train_on)):\n",
    "                logging.debug(i, values_to_train_on[i], os.path.basename(raster_paths[i]))\n",
    "            \n",
    "            if len(rasters_to_use) != len(values_to_train_on):\n",
    "                raise Exception\n",
    "                \n",
    "            logging.debug(\"Starting on directory : %s\" % dirpath)\n",
    "\n",
    "            modeltype = \"RandomForestsReg\"\n",
    "            output_fname = \"VB_\" + watershed + \"_\" + subdir + \"_\" + modeltype + \".tif\"\n",
    "            loc_classified_file = os.path.join(outquad_preds_dir, output_fname)\n",
    "\n",
    "            classified_File = createClassifiedFile(rasters_to_use, regressor, loc_classified_file, overwrite=True)\n",
    "\n",
    "        # NEED TO MERGE QUADS OF WATERSHED BACK TO ONE THE WATERSHED\n",
    "        watershed_rsac_name = \"VB_\" + watershed + \"_\" + modeltype + \".tif\"\n",
    "        watershed_rsac_path = os.path.join(watershedDir, watershed_rsac_name)\n",
    "\n",
    "        if not os.path.exists(watershed_rsac_path) or overwrite:\n",
    "            quadfiles = []\n",
    "            for file in os.listdir(outquad_preds_dir):\n",
    "                if modeltype in file and file.endswith(\".tif\"):\n",
    "                    fpath = os.path.join(outquad_preds_dir, file)\n",
    "                    quadfiles.append(fpath)\n",
    "\n",
    "            utils.mergeRasters(quadfiles, watershed_rsac_path)\n",
    "\n",
    "    # MERGE ALL WATERSHEDS TO ONE RASTER FOR WHOLE STATE\n",
    "    state_rsac_name = \"RSAC_ValleyBottoms.tif\"\n",
    "    state_rsac_path = os.path.join(nhdDir, state_rsac_name)\n",
    "\n",
    "    if not os.path.exists(state_rsac_path) or overwrite:\n",
    "        watershedfiles = []\n",
    "        for w_dir in os.listdir(watershedsDir):\n",
    "            watershedDir = os.path.join(watershedsDir, w_dir)\n",
    "            for file in os.listdir(watershedDir):\n",
    "                if modeltype in file and file.endswith(\".tif\"):\n",
    "                    fpath = os.path.join(watershedDir, file)\n",
    "                    watershedfiles.append(fpath)\n",
    "\n",
    "        utils.mergeRasters(watershedfiles, state_rsac_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valleyBottomRegression(watersheds_shp, vb_classification_pnts, nhd_dir, watersheds_dir):\n",
    "        extraction_variables = extractValuesToVBPoints(watersheds_shp, vb_classification_pnts, watersheds_dir)\n",
    "\n",
    "        vbpoints_raster_values = extraction_variables[\"points\"]\n",
    "        #rasters = extraction_variables[\"raster_paths\"]\n",
    "        raster_names = extraction_variables[\"raster_names\"]\n",
    "\n",
    "        # remove the large TPI rasters. Created in prep, but not useful\n",
    "        rasters_to_not_use = [\"TPI_20\", \"TPI_30\"]\n",
    "\n",
    "        rasters_to_regress_with = specifyTrainingRasters(raster_names, rasters_to_not_use) \n",
    "\n",
    "        logging.debug(\"Rasters to use in regression: \", rasters_to_regress_with)\n",
    "\n",
    "        # Only some training data will be used. Those marked with a 1 in 'Use' column\n",
    "        vbpoints_raster_values = vbpoints_raster_values[vbpoints_raster_values.Use == 1]\n",
    "\n",
    "        regressValleyBottoms(vbpoints_raster_values, rasters_to_regress_with, nhd_dir, watersheds_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    nhd_dir = os.path.abspath(r\"M:\\Data\\NHD\")\n",
    "    watersheds_dir = os.path.join(nhd_dir, \"Watersheds\")\n",
    "\n",
    "    vb_classification_pnts = os.path.join(nhd_dir, \"VM_TrainingData_20180619.shp\")\n",
    "    watersheds_shp = os.path.join(nhd_dir, \"WBDHU4_Arizona.shp\")\n",
    "    \n",
    "    valleyBottomRegression(watersheds_shp, vb_classification_pnts, nhd_dir, watersheds_dir)\n",
    "    \n",
    "    logging.info(\"Finished creating valley bottoms from regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experimental section\n",
    "\n",
    "### Classical RSAC calculation fitting distribution of training variables to curve\n",
    "\n",
    "formula = 'VB ~ ' + \"+\".join(values_to_train_on)\n",
    "\n",
    "print( formula)\n",
    "regressor = smf.glm(formula=formula, data=vbpoints_raster_values, family=sm.families.Binomial())\n",
    "coefficients = regressor.fit().params\n",
    "#coefficients\n",
    "\n",
    "intercept = coefficients.Intercept\n",
    "coeffs = coefficients[1:]\n",
    "\n",
    "predictors = removeTPIs(getRasterNamesList(r\"M:\\Data\\NHD\\Rasters\\HRNHDPlusRasters1504\\predictors_quads\\quad1\")[1])\n",
    "predictors\n",
    "\n",
    "calc = 0\n",
    "for i in range(len(coefficients)-1):\n",
    "    print(\"Beginning on %s...\" % predictors[i])\n",
    "    with rio.open(predictors[i]) as raster:\n",
    "        raster_array = raster.read(1).astype(float)\n",
    "        \n",
    "    print(\"\\tCalculating on %s...\" % predictors[i])\n",
    "    calc += (float(coeffs[i]) * raster_array)\n",
    "    \n",
    "    del raster_array\n",
    "    \n",
    "calc += intercept\n",
    "\n",
    "outfile = r\"M:\\Data\\NHD\\RSAC_VB_Preds\\glm_test_1505_quad1.tif\"\n",
    "with rio.open(predictors[0]) as ras:\n",
    "    kwargs = ras.profile\n",
    "\n",
    "print(\"Calculating Valley Bottoms...\\n\")\n",
    "#fp = eval(string)\n",
    "lp = 1.0/(1.0 + np.exp(-1.0 * calc))\n",
    "\n",
    "kwargs.update(dtype=np.float64)\n",
    "\n",
    "print(\"Writing calculation to outfile %s...\\n\" % outfile)\n",
    "with rio.open(outfile, 'w', **kwargs) as dst:\n",
    "    dst.write_band(1, lp.astype(np.float64))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
