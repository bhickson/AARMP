{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'StackGeneration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c0e1a7fcf49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m#import Create_Classification_Points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;31m#from RasterCalculations import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mStackGeneration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerateStack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'StackGeneration'"
     ]
    }
   ],
   "source": [
    "import logging, math\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import fiona\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import gdal\n",
    "from pyproj import transform, Proj\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import Utilities as utils\n",
    "import Create_Classification_Points\n",
    "from RasterCalculations import *\n",
    "from StackGeneration import generateStack\n",
    "\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "riolog = rio.logging.getLogger()\n",
    "riolog.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFullNAIPPath(naip_file, naipdir):\n",
    "    for root,dirs,files in os.walk(naipdir):\n",
    "        for file in files:\n",
    "            if naip_file in file:\n",
    "                return os.path.join(root,file)\n",
    "    \n",
    "    logging.error(\"Unable to find naip file %s in %s. Exiting\" % (naip_file, naipdir))\n",
    "    raise Exception\n",
    "\n",
    "\n",
    "def findSTDDevFile(dir, naip_file, band_num, windowsize):\n",
    "    #findFile(os.path.join(std3px_dir, bandnum), ffile)\n",
    "    \n",
    "    window_dir = os.path.join(dir, \"StdDev_\" + str(windowsize) + \"px\")\n",
    "    utilities.useDirectory(window_dir)\n",
    "    band_dir = os.path.join(window_dir, \"band\" + band_num)\n",
    "    utilities.useDirectory(band_dir)\n",
    "    \n",
    "    for root, dirs, files in os.walk(band_dir):\n",
    "        for file in files:\n",
    "            if f in file:\n",
    "                fpath = os.path.join(root, file)\n",
    "                return fpath\n",
    "                \n",
    "    if \"fpath\" not in locals():\n",
    "        standardDeviation(naip_file, dir, window_size=windowsize, overwrite=False)\n",
    "    logging.error(\"Unable to find file %s\" % f)\n",
    "    raise Exception\n",
    "\n",
    "\n",
    "def findVIFile(type, dir, f):\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if f in file:\n",
    "                fpath = os.path.join(root, file)\n",
    "                return fpath\n",
    "\n",
    "    if \"fpath\" not in locals():\n",
    "        vegIndexCalc(fpath, dir, [type])\n",
    "    return None\n",
    "\n",
    "\n",
    "def createSubSetLandsat(naip_path, landsat_file, opath, overwrite=False):\n",
    "    ssl_start = datetime.now()\n",
    "    ofile = \"Landsat8_\" + os.path.basename(naip_path)\n",
    "\n",
    "    landsat_opath = os.path.join(opath, ofile)\n",
    "    \n",
    "    if not os.path.exists(landsat_opath) or overwrite:\n",
    "        start = datetime.now()\n",
    "        reference_f = gdal.Open(naip_path)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        proj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        # build landsat tile from naip extent\n",
    "\n",
    "        if \"ndsi\" in opath.lower() or \"ndwi\" in opath.lower():\n",
    "            resampletype = \"bilinear\"\n",
    "        else:\n",
    "            resampletype = \"bilinear\"\n",
    "            #resampletype = \"near\"\n",
    "\n",
    "        gdal_warp = \"gdalwarp -overwrite -tap -r %s -t_srs %s -tr %s %s -te_srs %s -te %s %s %s %s %s %s\" % (\n",
    "            resampletype, proj, resx, resy, proj, str(minx), str(miny), str(maxx), str(maxy), landsat_file, landsat_opath)\n",
    "        logging.debug(\"Executing gdal_warp operation on %s for footprint of naip file %s\" % (landsat_file, naip_path))\n",
    "        os.system(gdal_warp)\n",
    "\n",
    "        logging.debug(\"\\tFinished qquad for %s landsat in %s\" % (landsat_file, str(datetime.now() - ssl_start)))\n",
    "    \n",
    "    return landsat_opath\n",
    "\n",
    "\n",
    "# FUNCTION TO WRITE OUT CLASSIFIED RASTER\n",
    "def write_geotiff(fname, data, geo_transform, projection, classes, COLORS, data_type=gdal.GDT_Byte):\n",
    "    \"\"\"\n",
    "    Create a GeoTIFF file with the given data.\n",
    "    :param fname: Path to a directory with shapefiles\n",
    "    :param data: Number of rows of the result\n",
    "    :param geo_transform: Returned value of gdal.Dataset.GetGeoTransform (coefficients for\n",
    "                          transforming between pixel/line (P,L) raster space, and projection\n",
    "                          coordinates (Xp,Yp) space.\n",
    "    :param projection: Projection definition string (Returned by gdal.Dataset.GetProjectionRef)\n",
    "    \"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    dataset = driver.Create(fname, cols, rows, 1, data_type)\n",
    "    dataset.SetGeoTransform(geo_transform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "\n",
    "    ct = gdal.ColorTable()\n",
    "    for pixel_value in range(len(classes)+1):\n",
    "        color_hex = COLORS[pixel_value]\n",
    "        r = int(color_hex[1:3], 16)\n",
    "        g = int(color_hex[3:5], 16)\n",
    "        b = int(color_hex[5:7], 16)\n",
    "        ct.SetColorEntry(pixel_value, (r, g, b, 255))\n",
    "    band.SetColorTable(ct)\n",
    "\n",
    "    metadata = {\n",
    "        'TIFFTAG_COPYRIGHT': 'CC BY 4.0, AND BEN HICKSON',\n",
    "        'TIFFTAG_DOCUMENTNAME': 'Land Cover Classification',\n",
    "        'TIFFTAG_IMAGEDESCRIPTION': 'Random Forests Supervised classification.',\n",
    "        'TIFFTAG_MAXSAMPLEVALUE': str(len(classes)),\n",
    "        'TIFFTAG_MINSAMPLEVALUE': '0',\n",
    "        'TIFFTAG_SOFTWARE': 'Python, GDAL, scikit-learn'\n",
    "    }\n",
    "    dataset.SetMetadata(metadata)\n",
    "\n",
    "    dataset = None  # Close the file\n",
    "    return\n",
    "\n",
    "\n",
    "def report_and_exit(txt, *args, **kwargs):\n",
    "    logger.error(txt, *args, **kwargs)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "def get_values(geom):\n",
    "    #print(row)\n",
    "    #geom = row['geometry']\n",
    "    x = geom.centroid.x\n",
    "    y = geom.centroid.y\n",
    "\n",
    "    values = []\n",
    "    # for raster in raster_objects:\n",
    "    # print(\"Starting Raster Extract for %s at x:%s y:%s\" % (os.path.basename(raster), str(x), str(y)))\n",
    "    # with rio.open(raster) as ras:\n",
    "    for val in rasnaip.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in rasnaipvis.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in rasgauss.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in raslandsat.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in rasNDSI.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    for val in rasNDWI.sample([(x, y)]):\n",
    "        values += np.ndarray.tolist(val)\n",
    "    \n",
    "    return pd.Series(values, index=rasters_names)\n",
    "\n",
    "\n",
    "def getQQuadFromNAIP(f):\n",
    "    qquad = f.split(\"_\")[1] + \"_\" + f.split(\"_\")[2]\n",
    "    return qquad\n",
    "\n",
    "\n",
    "def get_STDDev_VRT(naip_file):\n",
    "    naip_path = getFullNAIPPath(naip_file, naip_dir)\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    rasters_stddev = []\n",
    "    \n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=3, overwrite=False)\n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=5, overwrite=False)\n",
    "    rasters_stddev += standardDeviation(naip_path, base_datadir, window_size=10, overwrite=False)\n",
    "    \"\"\"\n",
    "    for bandnum in range(1, 5):\n",
    "        bandnum = \"band\" + str(bandnum)\n",
    "        ffile = \"stddev_\" + os.path.splitext(naip_file)[0] + bandnum + \".tif\"\n",
    "        \n",
    "        rasters_stddev.append(os.path.abspath(findFile(os.path.join(std3px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "        rasters_stddev.append(os.path.abspath(findFile(os.path.join(std5px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "        rasters_stddev.append(os.path.abspath(findFile(os.path.join(std10px_dir, bandnum), ffile)).replace(\"\\\\\", \"/\"))\n",
    "    \"\"\"\n",
    "    stddev_vrt_dir = os.path.join(qquad_vrt_dir, \"stddev\")\n",
    "    vrt_stddev = os.path.join(stddev_vrt_dir, qquad + \"_stddev.vrt\")\n",
    "    #print(vrt_stddev)\n",
    "\n",
    "    if not os.path.exists(vrt_stddev):\n",
    "        build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_stddev, \" \".join(rasters_stddev))\n",
    "        logging.debug(\"BUILDING VRT WITH: \\n\\t%s\" % build_vrt)\n",
    "        os.system(build_vrt)\n",
    "\n",
    "    return vrt_stddev\n",
    "\n",
    "\n",
    "def get_GaussianFile(naip_file):\n",
    "    naip_path = getFullNAIPPath(naip_file, naip_dir)\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    \n",
    "    gaussfile = gaussianCalc(naip_path, base_datadir, sigma=1, overwrite=False)\n",
    "    \n",
    "    return gaussfile\n",
    "    \n",
    "    \n",
    "def get_VegIndicies_VRT(naip_file):\n",
    "    qquad = getQQuadFromNAIP(naip_file)\n",
    "    rasters_float = []\n",
    "\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"NDVI\", ndvi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"SAVI\", savi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"OSAVI\", osavi_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"MSAVI2\", msavi2_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "    rasters_float.append(os.path.normpath(findVIFile(\"EVI2\", evi2_dir, naip_file)).replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    naipvis_vrt_dir = os.path.join(qquad_vrt_dir, \"naipvis\")\n",
    "    vrt_naipvis = os.path.join(naipvis_vrt_dir, qquad + \"_naipvis.vrt\")\n",
    "    \n",
    "    if not os.path.exists(vrt_naipvis):\n",
    "        build_vrt = \"gdalbuildvrt -overwrite -separate %s %s\" % (vrt_naipvis, \" \".join(rasters_float))\n",
    "        os.system(build_vrt)\n",
    "\n",
    "    return vrt_naipvis\n",
    "\n",
    "\n",
    "def createClassifiedFile(loc_NAIPFile, mltype, rf_classifier, overwrite=False):\n",
    "    file = os.path.basename(loc_NAIPFile)\n",
    "    \n",
    "    qquad = getQQuadFromNAIP(file)\n",
    "    \n",
    "    if mltype == \"RF\":\n",
    "        output_fname = mltype + \"_D\" + str(maxdepth) + \"E\" + str(n_est) + \"MPL\" + str(min_per_leaf) + \"_\" + qquad + \".tif\"\n",
    "    elif mltype == \"SVM\":\n",
    "        output_fname = mltype + \"_\" + maxi + \"_\" + qquad + \".tif\"\n",
    "        \n",
    "    loc_classified_file = os.path.join(loc_classifiedQuarterQuads, output_fname)\n",
    "    #print(loc_classified_file)\n",
    "\n",
    "    if not os.path.exists(loc_classified_file) or overwrite:\n",
    "        cl_start = datetime.now()\n",
    "        logging.info(\"\\tClassifying landcover file at %s...\" % (loc_classified_file))\n",
    "        # loc_NAIPFile = os.path.join(root, file)\n",
    "\n",
    "        file = os.path.basename(loc_NAIPFile)\n",
    "\n",
    "        vrt_naipvis = get_VegIndicies_VRT(file) #  All float32\n",
    "        #vrt_stddev = get_STDDev_VRT(file)  # All 8 bit (\"byte\")\n",
    "        gaussf_path = get_GaussianFile(file)\n",
    "\n",
    "        # BEN!!!!!! YOU REMOVED LANDSAT VARIABLE FOR QUICK RUN\n",
    "        #landsat_path = createSubSetLandsat(loc_NAIPFile, landsat_file, landsat_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        landsat_ndsi_path = createSubSetLandsat(loc_NAIPFile, ndsi_file, ndsi_dir).replace(\"\\\\\", \"/\")\n",
    "        landsat_ndwi_path = createSubSetLandsat(loc_NAIPFile, ndwi_file, ndwi_dir).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # GET RASTER INFO FROM INPUT\n",
    "        # NEED TO GET BANDS DATA INTO SINGLE ARRAY FOR OUTPUT CLASSIFICATION\n",
    "        bands_data = []\n",
    "        #for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "        for inras in [loc_NAIPFile, vrt_naipvis, gaussf_path, landsat_ndsi_path, landsat_ndwi_path]:\n",
    "            try:\n",
    "                raster_dataset = gdal.Open(inras, gdal.GA_ReadOnly)\n",
    "            except RuntimeError as e:\n",
    "                report_and_exit(str(e))\n",
    "\n",
    "            geo_transform = raster_dataset.GetGeoTransform()\n",
    "            proj = raster_dataset.GetProjectionRef()\n",
    "\n",
    "            for b in range(1, raster_dataset.RasterCount + 1):\n",
    "                band = raster_dataset.GetRasterBand(b)\n",
    "                bands_data.append(band.ReadAsArray())\n",
    "\n",
    "        # CREATE NP DATASTACK FROM ALL RASTERS\n",
    "        bands_data = np.dstack(bands_data)\n",
    "        # CREATE VARIABLES OF ROWS, COLUMNS, AND NUMBER OF BANDS\n",
    "        rows, cols, n_bands = bands_data.shape\n",
    "        n_samples = rows * cols\n",
    "\n",
    "        # CREATE EMPTY ARRAY WITH SAME SIZE AS RASTER\n",
    "        flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "        \n",
    "        classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "\n",
    "        # A list of colors for each class\n",
    "        COLORS = [\n",
    "            \"#000000\",  # 0 EMPTY\n",
    "            \"#00af11\",  # 1 - Veg - Thick\n",
    "            \"#00e513\",  # 2 - Veg - Sparse\n",
    "            \"#e9ff5a\",  # 3 - Herbaceous\n",
    "            \"#f1ac34\",  # 4 - Barren - Light\n",
    "            \"#a9852e\",  # 5 - Barren - Dark\n",
    "            \"#2759ff\",  # 6 - Water\n",
    "            \"#efefef\",  # 7 - Roof - White\n",
    "            \"#d65133\",  # 8 - Roof - Red\n",
    "            \"#cecece\",  # 9 - Roof - Grey\n",
    "            \"#a0a0a0\",  # 10 - Impervious - Light\n",
    "            \"#555555\",  # 11 - Impervious - Dark\n",
    "            \"#000000\"  # 12 - Shadows\n",
    "        ]\n",
    "\n",
    "        #print(\"Classifing...\")\n",
    "        \n",
    "        \"\"\"if not np.all(np.isfinite(flat_pixels)):\n",
    "            print(\"Not all value finite. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "        if np.any(np.isnan(flat_pixels)):\n",
    "            print(\"Some values are NaN. Fixing...\")\n",
    "            flat_pixels = np.where(flat_pixels == np.NaN, np.finfo(np.float32).max, flat_pixels)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = rf_classifier.predict(flat_pixels)\n",
    "            # Reshape the result: split the labeled pixels into rows to create an image\n",
    "            classification = result.reshape((rows, cols))\n",
    "\n",
    "            # WRITE OUT THE CLASSIFIED ARRAY TO RASTER BASED ON PROPERTIES OF TRAINING RASTERS\n",
    "            # TODO - Rewrite this to use rasterio for consistency\n",
    "            write_geotiff(loc_classified_file, classification, geo_transform, proj, classes, COLORS)\n",
    "            logging.info(\"\\tCreated classified file in %s\" % (str(datetime.now() - cl_start)))\n",
    "        except (ValueError) as e:\n",
    "            logging.info(\"-----------BAD VALUES FOR PREDICTORS. SKIPPING FILE %s\\n%s\" % (file, str(e)))\n",
    "            return None\n",
    "        \n",
    "        del bands_data\n",
    "        del flat_pixels\n",
    "        \n",
    "    else:\n",
    "        logging.info(\"LandCover file %s exists and no overwrite.\" % loc_classified_file)\n",
    "    \n",
    "    o_veg_loc = r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\RiparianClass_VBs\"\n",
    "    utilities.useDirectory(o_veg_loc)\n",
    "        \n",
    "    quadrant_loc = os.path.join(o_veg_loc, qquad[:5])\n",
    "    utilities.useDirectory(quadrant_loc)\n",
    "    \n",
    "    # not writing out density calculations\n",
    "    #dq_path = os.path.join(degress_quadrant_loc, denseVeg_file)\n",
    "    #sq_path = os.path.join(degress_quadrant_loc, sparseVeg_file)\n",
    "    \n",
    "    riparian_class_qquad = os.path.join(quadrant_loc, \"RiparianClassification_\" + qquad + \".tif\")\n",
    "        \n",
    "    if not os.path.exists(riparian_class_qquad) or overwrite:\n",
    "        createRiparianClass(loc_classified_file, riparian_class_qquad, qquad)\n",
    "    \n",
    "    return loc_classified_file\n",
    "\n",
    "def createRiparianClass(lc_raster, o_file, qquad):\n",
    "    rc_start = datetime.now()\n",
    "    logging.info(\"\\tClassifying riparian zones for %s\" % lc_raster)\n",
    "    with rio.open(lc_raster) as class_file:\n",
    "        class_array = class_file.read(1)#_band(1)\n",
    "        kwargs = class_file.profile\n",
    "    \n",
    "    # Get average densities of each class across the whole raster.\n",
    "    # TO DO - Updatet this to be evaulate on on something more specific than the qquad area\n",
    "    dense_veg_array = np.where(class_array == 1, 1, 0)\n",
    "    dense_file_avg = np.mean(dense_veg_array)\n",
    "    sparse_veg_array = np.where(class_array == 2, 1, 0)\n",
    "    sparse_file_avg = np.mean(sparse_veg_array)\n",
    "    \n",
    "    sparse_veg_array_localmean = ndimage.uniform_filter(sparse_veg_array.astype(np.float32), size=vaa_diameter, mode='constant')\n",
    "    dense_veg_array_localmean = ndimage.uniform_filter(dense_veg_array.astype(np.float32), size=vaa_diameter, mode='constant')\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # CRITICAL : Identify the splits where xero, meso, and hydro will be identified \n",
    "    # based on density ofsparse and thick vegetation\n",
    "    sparse_xero_lowlimit = sparse_file_avg + np.std(sparse_veg_array_localmean)\n",
    "    sparse_meso_lowlimit = sparse_file_avg + (1-sparse_file_avg)*0.7\n",
    "    sparse_hydro_lowlimit = sparse_file_avg + (1-sparse_file_avg)*0.9\n",
    "    \n",
    "    dense_xero_lowlimit = dense_file_avg + np.std(dense_veg_array_localmean)\n",
    "    dense_meso_lowlimit = dense_file_avg + (1-dense_file_avg)*0.7\n",
    "    dense_hydro_lowlimit = dense_file_avg + (1-dense_file_avg)*0.9\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Reassign pixel values based on density assessment\n",
    "    sparse_local_xero  = np.where(sparse_veg_array_localmean > sparse_xero_lowlimit,  1, 0) # xero (1) if true, upland (0) if false\n",
    "    sparse_local_meso  = np.where(sparse_veg_array_localmean > sparse_meso_lowlimit,  2, 0) # meso (2) if true, upland (0) if false\n",
    "    sparse_local_hydro = np.where(sparse_veg_array_localmean > sparse_hydro_lowlimit, 3, 0) # hydro (3) if true, upland (0) if false\n",
    "    # For some reason can't take numpy.maximum from more than two arrays at once\n",
    "    sparse_combine = np.maximum(sparse_local_xero, sparse_local_meso)#, sparse_local_hydro)\n",
    "    sparse_combine = np.maximum(sparse_combine, sparse_local_hydro)\n",
    "        \n",
    "    dense_local_xero  = np.where(dense_veg_array_localmean > dense_xero_lowlimit,  1, 0) # xero (1) if true, upland (0) if false\n",
    "    dense_local_meso  = np.where(dense_veg_array_localmean > dense_meso_lowlimit,  2, 0) # meso (2) if true, upland (0) if false\n",
    "    dense_local_hydro = np.where(dense_veg_array_localmean > dense_hydro_lowlimit, 3, 0) # hydro (3) if true, upland (0) if false\n",
    "    # For some reason can't take numpy.maximum from more than two arrays at once\n",
    "    dense_combine = np.maximum(dense_local_xero, dense_local_meso)#, sparse_local_hydro)\n",
    "    dense_combine = np.maximum(dense_combine, dense_local_hydro)\n",
    "    \n",
    "    # COMPARISON OF DENSITY VALUES OF BOTH RASTERS AT EACH PIXEL FOR \n",
    "    # DETERMINATION. ESSENTAILLY A DECISION TREE\n",
    "    p = np.where(dense_combine == 0, np.where(sparse_combine == 0, 0, 0), 0)\n",
    "    o = np.where(dense_combine == 0, np.where(sparse_combine == 1, 1, p), p)\n",
    "    n = np.where(dense_combine == 0, np.where(sparse_combine == 2, 2, o), o)\n",
    "    m = np.where(dense_combine == 0, np.where(sparse_combine == 3, 3, n), n)\n",
    "    l = np.where(dense_combine == 1, np.where(sparse_combine == 0, 1, m), m)\n",
    "    k = np.where(dense_combine == 1, np.where(sparse_combine == 1, 1, l), l)\n",
    "    j = np.where(dense_combine == 1, np.where(sparse_combine == 2, 2, k), k)\n",
    "    i = np.where(dense_combine == 1, np.where(sparse_combine == 3, 3, j), j)\n",
    "    h = np.where(dense_combine == 2, np.where(sparse_combine == 0, 1, i), i)\n",
    "    g = np.where(dense_combine == 2, np.where(sparse_combine == 1, 2, h), h)\n",
    "    f = np.where(dense_combine == 2, np.where(sparse_combine == 2, 2, g), g)\n",
    "    e = np.where(dense_combine == 2, np.where(sparse_combine == 3, 3, f), f)\n",
    "    d = np.where(dense_combine == 3, np.where(sparse_combine == 0, 2, e), e)\n",
    "    c = np.where(dense_combine == 3, np.where(sparse_combine == 1, 2, d), d)\n",
    "    b = np.where(dense_combine == 3, np.where(sparse_combine == 2, 3, c), c)\n",
    "    riparian = np.where(dense_combine == 3, np.where(sparse_combine == 3, 3, b), b)\n",
    "    \n",
    "    kwargs.update(\n",
    "        dtype=np.uint8,\n",
    "        nodata=0,\n",
    "        compress='lzw'\n",
    "    )\n",
    "    \n",
    "    valleybottom_ras = findVBRaster(qquad)\n",
    "    \n",
    "    with rio.open(valleybottom_ras) as vb_raster:\n",
    "        vb_array = vb_raster.read(1).astype(np.float32)\n",
    "    \n",
    "    #print(\"Clipping to Valley Bottoms\")\n",
    "    clipped_riparian = np.where(vb_array > 1, riparian, 0)\n",
    "\n",
    "    with rio.open(o_file, 'w', **kwargs) as dst:\n",
    "        dst.write_band(1, clipped_riparian.astype(np.uint8))\n",
    "\n",
    "        dst.write_colormap(\n",
    "            1, {\n",
    "                0: (255, 255, 255),\n",
    "                1: (186,228,179),\n",
    "                2: (116,196,118),\n",
    "                3: (35,139,69)})\n",
    "        cmap = dst.colormap(1)\n",
    "        \n",
    "    logging.info(\"\\tFinished riparian classification in %s\" % (str(datetime.now()-rc_start)))\n",
    "\n",
    "\n",
    "def findVBRaster(qquad, overwrite=False):\n",
    "    vb_start = datetime.now()\n",
    "    logging.debug(\"Starting creation of subset of valley bottom...\")\n",
    "    naip_path = getFullNAIPPath(qquad, naip_dir)\n",
    "    ofile = \"ValleyBottom_\" + qquad + \".tif\"\n",
    "\n",
    "    o_path = os.path.join(loc_valleybottoms, ofile)\n",
    "    \n",
    "    reference_f = gdal.Open(VBET_VB_loc)\n",
    "    geo_transform = reference_f.GetGeoTransform()\n",
    "    sproj = reference_f.GetProjectionRef()\n",
    "    \n",
    "    # TODO - Duplicative scripting. Exisits twice in this file and also in the VBET classification\n",
    "    if not os.path.exists(o_path) or overwrite:\n",
    "        reference_f = gdal.Open(naip_path)\n",
    "        geo_transform = reference_f.GetGeoTransform()\n",
    "        resx = geo_transform[1]\n",
    "        resy = geo_transform[5]\n",
    "        tproj = reference_f.GetProjectionRef()\n",
    "        minx = geo_transform[0]\n",
    "        maxy = geo_transform[3]\n",
    "        maxx = minx + (resx * reference_f.RasterXSize)\n",
    "        miny = maxy + (resy * reference_f.RasterYSize)\n",
    "\n",
    "        resampletype = \"bilinear\"\n",
    "        \n",
    "        gdal_warp = \"gdalwarp -overwrite -tap -r %s -s_srs %s -t_srs %s -tr %s %s -te_srs %s -te %s %s %s %s %s %s\" % (\n",
    "            resampletype, sproj, tproj, resx, resy, tproj, str(minx), str(miny), str(maxx), str(maxy), VBET_VB_loc, o_path)\n",
    "        #print(\"Executing gdal_warp operation on %s for footprint of naip file %s\" % (o_path, naip_path))\n",
    "        os.system(gdal_warp)\n",
    "\n",
    "        logging.debug(\"\\tFinished VB subset in %s\" % (str(datetime.now() - vb_start)))\n",
    "    \n",
    "    return o_path\n",
    "        \n",
    "def get_class_value(geom):\n",
    "    \"\"\" TAKES A VARIABLE OF GEOMETRY TYPE AND RETURNS THE VALUE AT X,Y\n",
    "    AS A PANDAS SERIES FOR FOR LOCAL RASTER 'CLASSRAS' \"\"\"\n",
    "    #print(row)\n",
    "    #geom = row.geometry\n",
    "    x = geom.centroid.x\n",
    "    y = geom.centroid.y\n",
    "    for val in classras.sample([(x, y)]):\n",
    "        # print(np.ndarray.tolist(val))\n",
    "        return pd.Series(val, index=[predicted_column])\n",
    "\n",
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n",
    "\n",
    "\n",
    "def calculateGeom(row):\n",
    "    #print(row)\n",
    "    geom = row[\"geometry\"]\n",
    "    if row['PROJ'] == \"NAD83 / UTM zone 11N\":\n",
    "        x = geom.centroid.x\n",
    "        y = geom.centroid.y\n",
    "        point = Point(transform(utm11, utm12, x, y))\n",
    "        return point\n",
    "    else:\n",
    "        return geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "veg_assessment_area = 0.1 # in acres\n",
    "vaa_meters = veg_assessment_area * 4046.86\n",
    "vaa_radius = math.sqrt(vaa_meters/math.pi)\n",
    "vaa_diameter = vaa_radius*2\n",
    "\n",
    "# LOCATIONS OF FOLDERS HOLDING ALL INPUT RASTER DATA\n",
    "naip_dir = os.path.abspath(r\"Q:\\Arid Riparian Project\\Data\\NAIP_2015_Compressed\")\n",
    "\n",
    "base_datadir = os.path.abspath(r\"M:\\Data\")\n",
    "ndvi_dir = os.path.join(base_datadir, \"NDVI\")\n",
    "savi_dir = os.path.join(base_datadir, \"SAVI\")\n",
    "osavi_dir = os.path.join(base_datadir, \"OSAVI\")\n",
    "msavi2_dir = os.path.join(base_datadir, \"MSAVI2\")\n",
    "evi2_dir = os.path.join(base_datadir, \"EVI2\")\n",
    "ndwi_dir = os.path.join(base_datadir, \"NDWI\")\n",
    "ndsi_dir = os.path.join(base_datadir, \"NDSI\")\n",
    "\n",
    "std3px_dir = os.path.join(base_datadir, \"StdDev_3px\")\n",
    "std5px_dir = os.path.join(base_datadir, \"StdDev_5px\")\n",
    "std10px_dir = os.path.join(base_datadir, \"StdDev_10px\")\n",
    "\n",
    "base_landsatdir = os.path.join(base_datadir, \"Landsat8\")\n",
    "landsat_dir = os.path.join(base_landsatdir, \"byNAIPDOY_QQuads\")\n",
    "\n",
    "# LOCATION OF LANDSAT RASTER\n",
    "landsat_file = os.path.os.path.join(base_landsatdir, \"Landsat1to8_TOA_NAIPAcquiDate_merge_rectified.tif\")\n",
    "\n",
    "# LOCATION OF THE NDSI FILE\n",
    "ndsi_file = os.path.join(ndsi_dir, \"LandsatOLI_NDSI_30m.tif\")\n",
    "\n",
    "# LOCATION OF THE NDWI FILE\n",
    "ndwi_file = os.path.join(ndwi_dir, \"LandsatOLI_NDWI_30m.tif\")\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POINTS PRE-EXTRACT\n",
    "loc_class_points = os.path.abspath(r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\classificationPoints_join.shp\")\n",
    "\n",
    "# LOCATION OF FILE CONTAINING CLASSIFICATION POINTS POST EXTRACT\n",
    "loc_points_wRaster_extracts = loc_class_points[:-4] + \"_extracts.shp\"\n",
    "\n",
    "# DIRECTORY HOLDING VRTS BY QUARTER QUAD FOR VEGETATION INDICIES (FLOAT32) AND STD_DEV (UINT8))\n",
    "qquad_vrt_dir = os.path.join(base_datadir, \"QQuad_VRTs\")\n",
    "\n",
    "loc_classifiedQuarterQuads = os.path.join(base_datadir, \"classifiedQuarterQuads\")\n",
    "\n",
    "nhd_dir = os.path.join(base_datadir, \"NHD\")\n",
    "loc_valleybottoms = os.path.join(nhd_dir, \"VBET_ValleyBottoms\")\n",
    "utilities.useDirectory(loc_valleybottoms)\n",
    "VBET_VB_loc = os.path.abspath(r\"M:\\Data\\NHD\\VBET_ValleyBottoms_20180624.tif\")\n",
    "\n",
    "# DEFINE THE PROJECTION USED OVER ARIZONA. USED FOR TRANSLATING POINT GEOMETRY LATER ON\n",
    "utm11 = Proj(init=\"epsg:26911\")\n",
    "utm12 = Proj(init=\"epsg:26912\")\n",
    "\n",
    "# IDENTIFY RASTER VARIABLES\n",
    "# THESE ORDER OF THESE RASTER VARIABLES MUST COINCIDE WITH THE CONSTRUCTED ARRAY OF EXTRACTS in the get_values FUNCTION\n",
    "naip = [\"NAIP1\", \"NAIP2\", \"NAIP3\", \"NAIP4\"]\n",
    "landsat = [\"Landsat1\", \"Landsat2\", \"Landsat3\", \"Landsat4\", \"Landsat5\", \"Landsat6\", \"Landsat7\", \"Landsat8\"]\n",
    "landsat_vis = [\"NDSI\", \"NDWI\"]\n",
    "naip_vis = [\"NDVI\", \"EVI2\", \"SAVI\", \"OSAVI\", 'MSAVI2']\n",
    "# NOT using texture in this iteration\n",
    "textures = [\"StdDev_3px_band1\", \"StdDev_3px_band2\", \"StdDev_3px_band3\", \"StdDev_3px_band4\",\n",
    "            \"StdDev_5px_band1\", \"StdDev_5px_band2\", \"StdDev_5px_band3\", \"StdDev_5px_band4\",\n",
    "            \"StdDev_10px_band1\", \"StdDev_10px_band2\", \"StdDev_10px_band3\", \"StdDev_10px_band4\"]\n",
    "filters = [\"Gauss1_band1\", \"Gauss1_band2\", \"Gauss1_band3\", \"Gauss1_band4\",]\n",
    "            \n",
    "rasters_names = naip + naip_vis + filters + landsat + landsat_vis\n",
    "print(rasters_names)\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF VECTOR FILE OF POINTS WITH RASTER EXTRACTS DOESN'T EXIST, BUILD IT\n",
    "if not os.path.exists(loc_points_wRaster_extracts):\n",
    "    if \"class_points\" not in locals():\n",
    "        logging.debug(\"READING IN %s as class_points\" % loc_class_points)\n",
    "        class_points = gpd.read_file(loc_class_points, crs={'init': 'epsg:26912'})\n",
    "    \n",
    "    if \"utm_geom\" not in class_points:\n",
    "        logging.debug(\"ADDING COLUMN 'utm_geom' WITH CORRECT UTM COORDINATES FOR EACH QUARTER QUAD\")\n",
    "        # CREATE TRUE RASTER GEOMETRY COLUMN (BASED ON UTM)\n",
    "        class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "        \n",
    "    # NDSI is only used because its the last raster column\n",
    "    if \"NDSI\" not in class_points:\n",
    "        logging.debug(\"CREATING COLUMNS...\")\n",
    "        # CREATE EMPTY COLUMNS IN DATA FRAME FOR EACH RASTER VARIABLE\n",
    "        for column in rasters_names:\n",
    "            class_points[column] = np.NaN\n",
    "    \n",
    "    net_percentage = 0.0\n",
    "    # ITERATE THROUGH DATAFRAME IN GROUPS BY NAIP_FILE. KEEPS US FROM OPENING/CLOSING RASTERS FOR EACH POINT - INSTEAD FOR EACH GROUP\n",
    "    for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "        logger.debug(\"\\nStarting raster value extraction for points in qquad %s\" % loc_NAIPFile)\n",
    "        loc_NAIPFile.replace(\"\\\\\", \"/\")\n",
    "    \n",
    "        # LOOK FOR RASTERS FROM WHICH VALUES WILL BE EXTRACTED\n",
    "        file = os.path.basename(loc_NAIPFile)\n",
    "    \n",
    "        vrt_naipvis = get_VegIndicies_VRT(file)\n",
    "        #vrt_stddev = get_STDDev_VRT(file)\n",
    "        gaussf_path = get_GaussianFile(file)\n",
    "    \n",
    "        landsat_path = createSubSetLandsat(loc_NAIPFile, landsat_file, landsat_dir).replace(\"\\\\\",\"/\")\n",
    "    \n",
    "        landsat_ndsi_path = createSubSetLandsat(loc_NAIPFile, ndsi_file, ndsi_dir).replace(\"\\\\\", \"/\")\n",
    "        landsat_ndwi_path = createSubSetLandsat(loc_NAIPFile, ndwi_file, ndwi_dir).replace(\"\\\\\", \"/\")\n",
    "    \n",
    "        net_percentage += 100 * len(class_points.loc[class_points[\"NAIP_FILE\"] == loc_NAIPFile])/len(class_points)\n",
    "        logger.debug(\"Percentage of total: %d\" % net_percentage)\n",
    "        # SELECT POINTS WHICH HAVE NAIP PATH VALUE\n",
    "        \n",
    "        # Only if group hasn't had values assigned (Jupyter and Rodeo iterations)\n",
    "        if group[\"NDSI\"].isnull().values.any():\n",
    "            with rio.open(loc_NAIPFile) as rasnaip:\n",
    "                with rio.open(vrt_naipvis) as rasnaipvis:\n",
    "                    with rio.open(gaussf_path) as rasgauss:\n",
    "                        with rio.open(landsat_path) as raslandsat:\n",
    "                            with rio.open(landsat_ndsi_path) as rasNDSI:\n",
    "                                with rio.open(landsat_ndwi_path) as rasNDWI:\n",
    "                                    count = 0\n",
    "                                    class_points.loc[class_points.NAIP_FILE == loc_NAIPFile, rasters_names] = \\\n",
    "                                        class_points.loc[class_points.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_values)\n",
    "        \n",
    "        logger.debug(\"Finished with group %s at %s\" % (loc_NAIPFile, str(datetime.now())))\n",
    "    \n",
    "    # REMOVE ALL ROWS WHICH EXTRACTED NO DATA VALUES FROM LANDSAT\n",
    "    #for column in landsat:\n",
    "    #    class_points = class_points[class_points.loc[column] != 32766]\n",
    "    \n",
    "    logger.info(\"Finished raster value extraction of %s points in %s\" % (str(len(class_points)), str(datetime.now() - start)))\n",
    "    \n",
    "    # GEOPANDAS WON\"T ALLOW MORE THAN ONE COLUMN WITH GEOMETRY TYPE. REMOVE THE utm_geom COLUMN CREATED PREVIOUSLY\n",
    "    del class_points['utm_geom']\n",
    "    #print(\"COLUMNS:\\n\", class_points.columns)\n",
    "    logger.debug(\"WRITING DATAFRAME TO OUTPUT...\")\n",
    "    class_points.to_file(loc_points_wRaster_extracts)\n",
    "\n",
    "else:\n",
    "    if \"class_points\" not in \"locals\":\n",
    "        logger.info(\"Reading in point file %s\" % loc_points_wRaster_extracts)\n",
    "        class_points = gpd.read_file(loc_points_wRaster_extracts)\n",
    "        # Had to delete utm_geom when writing file (can't have two geometry columns). Recreate...\n",
    "        rasters_names = class_points.columns.tolist()[18:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_points[\"utm_geom\"] = class_points.apply(calculateGeom, axis=1)\n",
    "\n",
    "# Split the points data frame into train and test\n",
    "train, test = train_test_split(class_points, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE COLUMN FOR PREDICTED CLASSIFICATION VALUES\n",
    "predicted_column = \"CLASS_PREDICT\"\n",
    "test[predicted_column] = \"Null\"\n",
    "\n",
    "#rasters_names = class_points.columns.tolist()[20:-2]\n",
    "logger.info(\"Available raster variables: \\n\\t%s\" % rasters_names)\n",
    "\n",
    "\"\"\"\n",
    "Allows removal of some rasters\n",
    "#rasters values used in random forest\n",
    "temp_rasters = rasters_names[:]\n",
    "rf_rasters = rasters_names[:]\n",
    "for r in temp_rasters:\n",
    "    if \"Landsat\" in r:\n",
    "#    if \"StdDev_\" in r:\n",
    "#        print(r)\n",
    "        rf_rasters.remove(r)\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Using raster variables: \\n%s\" % rf_rasters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN RANDOM FORESTS\n",
    "rf_start = datetime.now()\n",
    "logger.info(\"Beginning Random Forest Train\")\n",
    "#maxdepth = -1\n",
    "maxdepth = 60\n",
    "n_est = 40\n",
    "n_job = 6\n",
    "min_per_leaf = 50\n",
    "crit = \"entropy\" # gini or entropy\n",
    "\n",
    "rf = RandomForestClassifier(verbose=1, max_depth=maxdepth, n_estimators=n_est,\n",
    "                            n_jobs=n_job, min_samples_leaf=min_per_leaf,\n",
    "                            criterion=crit)\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "X = Imputer().fit_transform(train[rf_rasters].dropna())\n",
    "\n",
    "rf.fit(train[rf_rasters].dropna(),\n",
    "       train[rf_rasters+[\"Class\"]].dropna()[\"Class\"])\n",
    "\n",
    "\n",
    "logger.info(\"Finished Fitting in\", datetime.now() - rf_start)\n",
    "#print('Out-of-bag score estimate:', {rf.oob_score_:.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LINEAR SVM\n",
    "datetimestart = datetime.now()\n",
    "print(\"- Beginning Linear SVM Train -\")\n",
    "maxi=1000\n",
    "\n",
    "svm = LinearSVC(verbose=1, max_iter=maxi)\n",
    "\n",
    "svm.fit(train[rf_rasters].dropna(),\n",
    "       train[rf_rasters+[\"Class\"]].dropna()[\"Class\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE CLASSIFIED RASTERS FOR QUARTER QUADS USED IN TRAINING DATA FIRST\n",
    "for loc_NAIPFile, group in class_points.groupby(\"NAIP_FILE\"):\n",
    "    #if \"3210911_sw\" in loc_NAIPFile:\n",
    "        #print(\"FOUND\")\n",
    "    classified_File_rf = createClassifiedFile(loc_NAIPFile, \"RF\", rf, overwrite=False)\n",
    "    #classified_File_svm = createClassifiedFile(loc_NAIPFile, \"SVM\", svm, overwrite=True)\n",
    "    \"\"\"\n",
    "    # EXTRACT PREDICTED PIXEL CLASSIFICATION TO TESTING DATAFRAME\n",
    "    print(\"Extracting predicted classified values...\")\n",
    "    with rio.open(classified_File_rf) as classras:\n",
    "        # print(classras.indexes)\n",
    "        test.loc[test.NAIP_FILE == loc_NAIPFile, [predicted_column]] = \\\n",
    "            test.loc[test.NAIP_FILE == loc_NAIPFile, \"utm_geom\"].apply(get_class_value)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"THIS CODE BLOCK USES A SHAPEFILE OF THE NAIP FOOTPRINTS AND THE AOI ECOREGIONS FEATURE \n",
    "CLASS TO FIND ONLY THE NAMES OF THE QQUADS WHICH WE WAND TO CLASSIFY. THEN IDENTIFIES THE \n",
    "ACTUAL PATH OF THE NAIP FILE AND PASSES IT TO THE CLASSIFIER\"\"\"\n",
    "\n",
    "\n",
    "aoi = gpd.GeoDataFrame.from_file(r\"Q:\\Arid Riparian Project\\AridRiparianProject\\AridRiparianProject.gdb\", layer='TargetEcoregions')\n",
    "aoi.crs = fiona.crs.from_epsg(2163)\n",
    "\n",
    "naip_footprints = gpd.read_file(r\"Q:\\Arid Riparian Project\\AridRiparianProject\")\n",
    "aoi.to_crs(naip_footprints.crs, inplace=True)\n",
    "\n",
    "aoi_qquads = []\n",
    "for i, row in naip_footprints.iterrows():\n",
    "    for j, arow in aoi.iterrows():\n",
    "        if row.geometry.within(arow.geometry):\n",
    "            aoi_qquads.append(row.Name)\n",
    "#print(len(aoi_qquads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_already_created = []\n",
    "for root, dirs, files in os.walk(r\"Q:\\GoogleDrive\\AridRiparianProject\\WorkingDirectory\\Data\\RiparianClass_VBs\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\"):\n",
    "            files_already_created.append(file)\n",
    "\n",
    "for qquad_name in aoi_qquads:\n",
    "    fpath = getFullNAIPPath(qquad_name, naip_dir)\n",
    "    already_created = False\n",
    "    for file in files_already_created:\n",
    "        if qquad_name[2:12] in file:\n",
    "            already_created = True\n",
    "            break\n",
    "            \n",
    "    if not already_created:\n",
    "        beg = datetime.now()\n",
    "        logger.debug(\"Starting on qquad: %s\" % qquad_name)\n",
    "        createClassifiedFile(fpath, \"RF\", rf, overwrite=False)\n",
    "        logger.debug(\"COMPLETED riparian classification. Finished in %s\" % (str(datetime.now()-beg)))\n",
    "        logger.debug(\"_____________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THEN CREATE CLASSIFIED RASTER FROM ALL OTHER QUARTER QUADS\n",
    "for root, dirs, files in os.walk(naip_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\"):\n",
    "            fpath = os.path.join(root,file)\n",
    "            #createClassifiedFile(fpath, \"RF\", rf, overwrite=False)\n",
    "            \n",
    "logger.info(\"\\n\\t\\t-- DONE WITH ALL FILES --\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "# TESTING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"flat_pixels = np.array([1,10,50,500, np.NaN])\n",
    "\n",
    "if np.isfinite(flat_pixels.any()) and not np.isnan(flat_pixels.any()):\n",
    "    flat_pixels = np.where(flat_pixels > np.finfo(np.float32).max, np.finfo(np.float32).max, flat_pixels)\n",
    "    #result = rf_classifier.predict(flat_pixels)\n",
    "    #np.where(x.values >= np.finfo(np.float32).max,)\n",
    "    print(\"MODING\")\n",
    "else:\n",
    "    print(\"TURRIBLE\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
